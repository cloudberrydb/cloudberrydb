The Greenplum pg_upgrade code is based on pg_upgrade from 9.0.23 and has
been extended to work with Greenplum. This README is intended to document the
changes that have been done.

Operation of pg_upgrade remains unchanged as are the available command line
parameters. Upgrading a Greenplum cluster with pg_upgrade is done by first
upgrading the QD and then the QEs.


Extensions to upstream pg_upgrade
---------------------------------

pg_upgrade is in PostgreSQL only maintaning the Oids of types, Greenplum
however requires all Oids to be maintained such that they are synchronized
between QD and QEs after upgrade. The binary-upgrade option in pg_dump has
been extended to inject Oid preservation calls for all required object types
in the dump. Technically, the Oids needn't be preserved during upgrade as
long as they are synchronized between QD/QEs after the upgrade. The current
implementation preserves the Oids rather than allocate new ones.

Functionality for restoring the append-only catalog relations is in
aotable.c, Greenplum specific checks for upgrading from GPDB 4 are added to
version_old_gpdb4.c.


Adding a new object type
------------------------

When a new object type is added to Greenplum Database, it may need to be added
to pg_upgrade as well. Below are the bottom-up steps for adding a new object to
pg_upgrade. This assumes that the object already has Oid dispatch handling and
is supported in pg_dump.

* In contrib/pg_upgrade_support/pg_upgrade_support.c, add a new function for
  calling the oid dispatch code with the details required for the new object.
  To ensure the all required details are included, make sure it matches the
  object entry in CreateKeyFromCatalogTuple() in the oid dispatcher code
  (src/backend/catalog/oid_dispatch.c).

* Ensure that the Oid dumping in contrib/pg_upgrade/oid_dump.c is covering the
  new object, and extend it if not.

* In case any special handling is required during the upgrade, or any special
  checks are warranted, add these to the appropriate places in pg_upgrade.
  The variety of what may be needed is too big to do into detail here.


Upgrading from 4.3 to 5.0
-------------------------

When upgrading from 4.3 to 5.0 the QD must be upgraded before any QEs are
upgraded. This is because new Oids are allocated during the QD upgrade and
these needs to be distributed to the QEs in order to preserve the QD/QE Oid
synchronization.

* Each base relation type is in 5.0 defined as an array type, something which
  is not supported in 4.3. The arraytypes for base relations will be created
  during upgrade and thier Oids recorded. To be able to distribute the Oids to
  the QEs, the upgrade will leave a dump file in the working directory named
  pg_upgrade_dump_arraytypes.sql. If this file is placed in the working
  directory of upgrading a QE, pg_upgrade will use it to ensure that the Oids
  are synchronized.

* The on-disk format of the NUMERIC data type was changed in 5.0 due to the
  merge of PostgreSQL 8.3, both heap and AO tables are affected by this. There
  were also changes to the flags in the heap page header. PostgreSQL doesn't
  support upgrading with pg_upgrade from 8.2 to 8.3, thus handling this format
  change was added to pg_upgrade for Greenplum:

  - Heap tables are rewritten as part of the upgrade, any pages containing
    numeric attributes will be converted when the segment files are copied
	to the new data directory.

  - AO table segment files will be rewritten when a segment containing numeric
    attributes are written to from database operations.

* External partitions are not supported since management of external partitions
  is not allowed in utility mode. All external partitions must be either moved
  out of the partition hierarchy with ALTER TABLE EXCHANGE, or dropped, prior
  to the upgrade.


Upgrading from 5.0 to 6.0
-------------------------

The name_pattern_ops and pattern equality operators were removed in 6.0 and
text equality is treated like bitwise equality. This will cause internal sort
order in bpchar_pattern_ops which is now trailing-blank-insensitive. This means
that indexes must be dropped and recreated as part of the upgrade to ensure
correct results.


Testing Greenplum pg_upgrade
----------------------------

There is a testrunner script in contrib/pg_upgrade which runs a full upgrade of
the gpdemo cluster and it's current contents. The idea is that it can be used
to test upgrades by running as the final step of ICW. The test can be invoked
by "make installcheck" in the contrib/pg_upgrade directory.

The testscript also has a "smoketest" option which only tests upgrading a
single node.  This is *not* a test of an actual Greenplum cluster upgrade,
but a test of the Oid synchronization required to handle a node upgrade. The
intention is for this to be a quick test that all objects are handled by
pg_upgrade; upgrade testing still requires the full run across all
segments/mirrors. The smoketest can be invoked by "make check" in the
contrib/pg_upgrade directory.
