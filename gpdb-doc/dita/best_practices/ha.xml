<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_q5g_pmj_s4">
  <title>High Availability</title>
  <shortdesc>Greenplum Database supports highly available, fault-tolerant database services when you
    enable and properly configure Greenplum high availability features. To guarantee a required
    level of service, each component must have a standby ready to take its place if it should
    fail.</shortdesc>
  <topic id="topic_vrb_sxc_54">
    <title>Disk Storage</title>
    <body>
      <p>With the Greenplum Database "shared-nothing" MPP architecture, the master host and segment
        hosts each have their own dedicated memory and disk storage, and each master or segment
        instance has its own independent data directory. For both reliability and high performance,
        Pivotal recommends a hardware RAID storage solution with from 8 to 24 disks. A larger number
        of disks improves I/O throughput when using RAID 5 (or 6) because striping increases
        parallel disk I/O. The RAID controller can continue to function with a failed disk because
        it saves parity data on each disk in a way that it can reconstruct the data on any failed
        member of the array. If a hot spare is configured (or an operator replaces the failed disk
        with a new one) the controller rebuilds the failed disk automatically. </p>
      <p>RAID 1 exactly mirrors disks, so if a disk fails, a replacement is immediately available
        with performance equivalent to that before the failure. With RAID 5 each I/O for data on the
        failed array member must be reconstructed from data on the remaining active drives until the
        replacement disk is rebuilt, so there is a temporary performance degradation. If the
        Greenplum master and segments are mirrored, you can switch any affected Greenplum instances
        to their mirrors during the rebuild to maintain acceptable performance. </p>
      <p>A RAID disk array can still be a single point of failure, for example, if the entire RAID
        volume fails. At the hardware level, you can protect against a disk array failure by
        mirroring the array, using either host operating system mirroring or RAID controller
        mirroring, if supported.</p>
      <p>It is important to regularly monitor available disk space on each segment host. Query the
          <codeph>gp_disk_free</codeph> external table in the <codeph>gptoolkit</codeph> schema to
        view disk space available on the segments. This view runs the Linux <codeph>df</codeph>
        command. Be sure to check that there is sufficient disk space before performing operations
        that consume large amounts of disk, such as copying a large table. </p>
      <p>See <codeph>gp_toolkit.gp_disk_free</codeph> in the <i>Greenplum Database Reference
          Guide</i>.</p>
      <section>
        <title>Best Practices</title>
        <ul id="ul_ry5_syr_s4">
          <li>Use a hardware RAID storage solution with 8 to 24 disks.</li>
          <li>Use RAID 1, 5, or 6 so that the disk array can tolerate a failed disk.</li>
          <li>Configure a hot spare in the disk array to allow rebuild to begin automatically when
            disk failure is detected.</li>
          <li>Protect against failure of the entire disk array and degradation during rebuilds by
            mirroring the RAID volume.</li>
          <li>Monitor disk utilization regularly and add additional space when needed. </li>
          <li>Monitor segment skew to ensure that data is distributed evenly and storage is consumed
            evenly at all segments.</li>
        </ul>
      </section>
    </body>
  </topic>
  <topic id="topic_b5s_txc_54">
    <title>Master Mirroring</title>
    <body>
      <p>The Greenplum Database master instance is clients' single point of access to the system.
        The master instance stores the global system catalog, the set of system tables that store
        metadata about the database instance, but no user data. If an unmirrored master instance
        fails or becomes inaccessible, the Greenplum instance is effectively off-line, since the
        entry point to the system has been lost. For this reason, a standby master must be ready to
        take over if the primary master fails. </p>
      <p>Master mirroring uses two processes, a sender on the active master host and a receiver on
        the mirror host, to synchronize the mirror with the master. As changes are applied to the
        master system catalogs, the active master streams its write-ahead log (WAL) to the mirror so
        that each transaction applied on the master is applied on the mirror. </p>
      <p>The mirror is a <i>warm standby</i>. If the primary master fails, switching to the standby
        requires an administrative user to run the <codeph>gpactivatestandby</codeph> utility on the
        standby host so that it begins to accept client connections. Clients must reconnect to the
        new master and will lose any work that was not committed when the primary failed.</p>
      <p>See "Enabling High Availability Features" in the <i>Greenplum Database Administrator
          Guide</i> for more information. </p>
      <section>
        <title>Best Practices</title>
        <ul id="ul_gpj_lzr_s4">
          <li>Set up a standby master instance&#8212;a <i>mirror</i>&#8212;to take over if the
            primary master fails. </li>
          <li>The standby can be on the same host or on a different host, but it is best practice to
            place it on a different host from the primary master to protect against host
            failure.</li>
          <li>Plan how to switch clients to the new master instance when a failure occurs, for
            example, by updating the master address in DNS.</li>
          <li>Set up monitoring to send notifications in a system monitoring application or by email
            when the primary fails.</li>
        </ul>
      </section>
    </body>
  </topic>
  <topic id="topic_cnm_vxc_54">
    <title>Segment Mirroring</title>
    <body>
      <p>Greenplum Database segment instances each store and manage a portion of the database data,
        with coordination from the master instance. If any unmirrored segment fails, the database
        may have to be shutdown and recovered, and transactions occurring after the most recent
        backup could be lost. Mirroring segments is, therefore, an essential element of a high
        availability solution.</p>
      <p>A segment mirror is a hot standby for a primary segment. Greenplum Database detects when a
        segment is unavailable and automatically activates the mirror. During normal operation, when
        the primary segment instance is active, data is replicated from the primary to the mirror in
        two ways:<ul id="ul_aqq_fhv_t4">
          <li>
            <p>The transaction commit log is replicated from the primary to the mirror before the
              transaction is committed. This ensures that if the mirror is activated, the changes
              made by the last successful transaction at the primary are present at the mirror. When
              the mirror is activated, transactions in the log are applied to tables in the mirror.
            </p>
          </li>
          <li>
            <p>Second, segment mirroring uses physical file replication to update heap tables.
              Greenplum Server stores table data on disk as fixed-size blocks packed with tuples. To
              optimize disk I/O, blocks are cached in memory until the cache fills and some blocks
              must be evicted to make room for newly updated blocks. When a block is evicted from
              the cache it is written to disk and replicated over the network to the mirror. Because
              of the caching mechanism, table updates at the mirror can lag behind the primary.
              However, because the transaction log is also replicated, the mirror remains consistent
              with the primary. If the mirror is activated, the activation process updates the
              tables with any unapplied changes in the transaction commit log.</p>
          </li>
        </ul></p>
      <p>When the acting primary is unable to access its mirror, replication stops and state of the
        primary changes to "Change Tracking." The primary saves changes that have not been
        replicated to the mirror in a system table to be replicated to the mirror when it is back
        on-line.</p>
      <p>The master automatically detects segment failures and activates the mirror. Transactions in
        progress at the time of failure are restarted using the new primary. Depending on how
        mirrors are deployed on the hosts, the database system may be unbalanced until the original
        primary segment is recovered. For example, if each segment host has four primary segments
        and four mirror segments, and a mirror is activated on one host, that host will have five
        active primary segments. Queries are not complete until the last segment has finished its
        work, so performance can be degraded until the balance is restored by recovering the
        original primary. </p>
      <p>Administrators perform the recovery while Greenplum Database is up and running by running
        the <codeph>gprecoverseg</codeph> utility.This utility locates the failed segments, verifies
        they are valid, and compares the transactional state with the currently active segment to
        determine changes made while the segment was offline. <codeph>gprecoverseg</codeph>
        synchronizes the changed database files with the active segment and brings the segment back
        online.</p>
      <p>It is important to reserve enough memory and CPU resources on segment hosts to allow for
        increased activity from mirrors that assume the primary role during a failure. The formulas
        provided in <xref href="workloads.xml#topic_hhc_z5w_r4/section_r52_rbl_zt"/> for configuring
        segment host memory include a factor for the maximum number of primary hosts on any one
        segment during a failure. The arrangement of mirrors on the segment hosts affects this
        factor and how the system will respond during a failure. See <xref href="#topic_ngz_qf4_tt"
          format="dita"/> for a discussion of segment mirroring options. </p>
      <section>
        <title>Best Practices</title>
        <ul id="ul_e5w_3ds_s4">
          <li>Set up mirrors for all segments.</li>
          <li>Locate primary segments and their mirrors on different hosts to protect against host
            failure.</li>
          <li>Mirrors can be on a separate set of hosts or co-located on hosts with primary
            segments.</li>
          <li>Set up monitoring to send notifications in a system monitoring application or by email
            when a primary segment fails.</li>
          <li>Recover failed segments promptly, using the <codeph>gprecoverseg</codeph> utility, to
            restore redundancy and return the system to optimal balance.</li>
        </ul>
      </section>
    </body>
  </topic>
  <topic id="topic_tlk_zw4_1s">
    <title>Dual Clusters</title>
    <body>
      <p>For some use cases, an additional level of redundancy can be provided by maintaining two
        Greenplum Database clusters that store the same data. The decision to implement dual
        clusters should be made with business requirements in mind.</p>
      <p>There are two recommended methods for keeping the data synchronized in a dual cluster
        configuration. The first method is called Dual ETL. ETL (extract, transform, and load) is
        the common data warehousing process of cleansing, transforming, validating, and loading data
        into a data warehouse. With Dual ETL, the ETL processes are performed twice, in parallel on
        each cluster, and validated each time. Dual ETL provides for a complete standby cluster with
        the same data. It also provides the capability to query the data on both clusters, doubling
        the processing throughput. The application can take advantage of both clusters as needed and
        also ensure that the ETL is successful and validated on both sides. </p>
      <p>The second mechanism for maintaining dual clusters is backup and restore. The data is
        backed­up on the primary cluster, then the backup is replicated to and restored on the
        second cluster. The backup and restore mechanism has higher latency than Dual ETL, but
        requires less application logic to be developed. Backup and restore is ideal for use cases
        where data modifications and ETL are done daily or less frequently.</p>
      <section>
        <title>Best Practices</title>
        <ul id="ul_m41_y1p_1s">
          <li>Consider a Dual Cluster configuration to provide an additional level of redundancy and
            additional query processing throughput. </li>
        </ul>
      </section>
    </body>
  </topic>
  <topic id="topic_mc5_k1p_1s">
    <title>Backup and Restore</title>
    <body>
      <p>Backups are recommended for Greenplum Database databases unless the data in the database
        can be easily and cleanly regenerated from source data. Backups protect from operational,
        software, or hardware errors.</p>
      <p>The <codeph>gpcrondump</codeph> utility makes backups in parallel across the segments, so
        that backups scale as the cluster grows in hardware size. </p>
      <p>Incremental backups can significantly reduce backup sizes in some cases. An incremental
        backup saves all heap tables, but only the append-optimized and column-oriented partitions
        that have changed since the previous backup. When the database has large fact tables with
        many partitions and modifications are confined to one or a few partitions in a time period,
        incremental backups can save a large amount of disk space. Conversely, a database with
        large, unpartitioned fact tables is a poor application for incremental backups. </p>
      <p>A backup strategy must consider where the backups will be written and where they will be
        stored. Backups can be taken to the local cluster disks, but they should not be stored there
        permanently. If the database and its backup are on the same storage, they can be lost
        simultaneously. The backup also occupies space that could be used for database storage or
        operations. After performing a local backup, the files should be copied to a safe,
        off-cluster location. </p>
      <p>An alternative is to back up directly to an NFS mount. If each host in the cluster has an
        NFS mount, ghe backups can be written directly to NFS storage. A scale-out NFS solution is
        recommended to ensure that backups do not bottleneck on the IO throughput of the NFS device.
        Dell EMC Isilon is an example of this type of solution and can scale alongside the Greenplum
        cluster. </p>
      <p>Finally, through native API integration, Greenplum Database can stream backups directly to
        Dell EMC Data Domain or Veritas NetBackup enterprise backup platforms. </p>
      <section>
        <title>Best Practices</title>
        <p>
          <ul id="ul_nry_scl_b1b">
            <li dir="ltr">
              <p dir="ltr">Back up Greenplum databases regularly unless the data is easily restored
                from sources.</p>
            </li>
            <li dir="ltr">
              <p dir="ltr">Use the <codeph>gpcrondump</codeph>
                <codeph>-s</codeph>, <codeph>-S</codeph>, <codeph>-t</codeph>, or
                  <codeph>-T</codeph> options to specify only the schema and tables that need to be
                backed up. See the <xref
                  href="https://gpdb.docs.pivotal.io/latest/utility_guide/admin_utilities/gpcrondump.html"
                  format="html" scope="external">gpcrondump</xref> reference in the <cite>Greenplum
                  Database Utility Reference Guide</cite> for more information.</p>
            </li>
            <li dir="ltr">
              <p dir="ltr">Back up one schema at a time to reduce the length of time the
                  <codeph>pg_class</codeph> table is locked.</p>
              <p dir="ltr">At the start of the backup, <codeph>gpcrondump</codeph> places an
                EXCLUSIVE lock on the <codeph>pg_class</codeph> table, which prevents creating,
                altering, or dropping tables. This lock is held while <codeph>gpcrondump</codeph>
                computes the set of tables to back up and then places SHARED ACCESS locks on the
                tables. Once the tables are locked and backup processes started on the segments, the
                EXCLUSIVE lock on <codeph>pg_class</codeph> is released. In a database with many
                objects the duration of the EXCLUSIVE lock may be disruptive. Backing up one schema
                at a time can reduce the duration of the EXCLUSIVE lock. Backups with fewer tables
                are also more efficient for selectively restoring schemas and tables, since
                  <codeph>gpdbrestore</codeph> does not have to search through the entire database.
                See <xref
                  href="http://gpdb.docs.pivotal.io/latest/admin_guide/managing/backup-locks.html"
                  format="html" scope="external">Backup Process and Locks</xref> for more
                information. </p>
            </li>
            <li dir="ltr">Use incremental backups when heap tables are relatively small and few
              append-optimized or column-oriented partitions are modified between backups. </li>
            <li dir="ltr">If backups are saved to local cluster storage, move the files to a safe,
              off-cluster location when the backup is complete. Backup files and database files that
              reside on the same storage can be lost simultaneously.</li>
            <li dir="ltr">
              <p dir="ltr">If your operating system supports direct I/O, set the
                  <codeph>gp_backup_directIO</codeph> configuration parameter to reduce CPU usage
                during backups. See <xref
                  href="http://gpdb.docs.pivotal.io/latest/admin_guide/managing/backup-direct-io.html"
                  format="html" scope="external">Using Direct I/O</xref> for more information. </p>
            </li>
            <li dir="ltr">
              <p dir="ltr">If backups are saved to NFS mounts, use a scale-out NFS solution such as
                Dell EMC Isilon to prevent IO bottlenecks.</p>
            </li>
            <li dir="ltr" otherprops="pivotal">Consider using Pivotal Greenplum Database integration
              to stream backups to the Dell EMC Data Domain or Veritas NetBackup enterprise backup
              platforms.</li>
          </ul>
        </p>
      </section>
    </body>
  </topic>
  <topic id="topic_wlw_wxc_54">
    <title>Detecting Failed Master and Segment Instances</title>
    <body>
      <p>Recovering from system failures requires intervention from a system administrator, even
        when the system detects a failure and activates a standby for the failed component. In each
        case, the failed component must be replaced or recovered to restore full redundancy. Until
        the failed component is recovered, the active component lacks a standby, and the system may
        not be executing optimally. For these reasons, it is important to perform recovery
        operations promptly. Constant system monitoring and automated fault notifications through
        SNMP and email ensure that administrators are aware of failures that demand their attention. </p>
      <p>The Greenplum Database server <codeph>ftsprobe</codeph> subprocess handles fault detection.
          <codeph>ftsprobe</codeph> connects to and scans all segments and database processes at
        intervals that you can configure with the <codeph>gp_fts_probe_interval</codeph>
        configuration parameter. If <codeph>ftsprobe</codeph> cannot connect to a segment, it marks
        the segment “down” in the Greenplum Database system catalog. The segment remains down until
        an administrator runs the <codeph>gprecoverseg</codeph> recovery utility. </p>
      <p>You can configure a Greenplum Database system to trigger SNMP (Simple Network Management
        Protocol) alerts or send email notifications to system administrators if certain database
        events occur. See "Using SNMP with a Greenplum Database System" in the <cite>Pivotal
          Greenplum Database Administrator Guide</cite> for instructions to set up SNMP.</p>
      <section>
        <title>Best Practices</title>
        <ul id="ul_b4h_rxl_v4">
          <li>Run the <codeph>gpstate</codeph> utility to see the overall state of the Greenplum
            system.</li>
          <li>Configure Greenplum Database to send SNMP notifications to your network monitor. </li>
          <li>Set up email notification in the
              <codeph>$MASTER_DATA_DIRECTORY/postgresql.conf</codeph> configuration file so that the
            Greenplum system can email administrators when a critical issue is detected.</li>
        </ul>
      </section>
      <section><title>Additional Information</title><i>Greenplum Database Administrator Guide</i>:
          <ul id="ul_wff_k1m_v4">
          <li>Monitoring a Greenplum System</li>
          <li>Recovering a Failed Segment</li>
          <li>Using SNMP with a Greenplum System</li>
          <li>Enabling Email Notifications</li>
        </ul><p><i>Greenplum Database Utility Guide</i>:<ul id="ul_fk2_hyl_v4">
            <li><codeph>gpstate</codeph>&#8212;view state of the Greenplum system</li>
            <li><codeph>gprecoverseg</codeph>&#8212;recover a failed segment</li>
            <li><codeph>gpactivatestandby</codeph>&#8212;make the standby master the active
              master</li>
          </ul></p><p><xref href="http://tools.ietf.org/html/rfc1697" format="html" scope="external"
            >RDBMS MIB Specification</xref></p></section>
    </body>
  </topic>
  <topic id="topic_ngz_qf4_tt">
    <title>Segment Mirroring Configuration</title>
    <body>
      <p>Segment mirroring allows database queries to fail over to a backup segment if the primary
        segment fails or becomes unavailable. Pivotal requires mirroring for supported production
        Greenplum Database systems.</p>
      <p>A primary segment and its mirror must be on different hosts to ensure high availability.
        Each host in a Greenplum Database system has the same number of primary segments and mirror
        segments. Multi-homed hosts should have the same numbers of primary and mirror segments on
        each interface. This ensures that segment hosts and network resources are equally loaded
        when all primary segments are operational and brings the most resources to bear on query
        processing. </p>
      <p>When a segment becomes unavailable, its mirror segment on another host becomes the active
        primary and processing continues. The additional load on the host creates skew and degrades
        performance, but should allow the system to continue. A database query is not complete until
        all segments return results, so a single host with an additional active primary segment has
        the same effect as adding an additional primary segment to every host in the cluster. </p>
      <p>The least amount of performance degradation in a failover scenario occurs when no host has
        more than one mirror assuming the primary role. If multiple segments or hosts fail, the
        amount of degradation is determined by the host or hosts with the largest number of mirrors
        assuming the primary role. Spreading a host's mirrors across the remaining hosts minimizes
        degradation when any single host fails.</p>
      <p>It is important, too, to consider the cluster's tolerance for multiple host failures and
        how to maintain a mirror configuration when expanding the cluster by adding hosts. There is
        no mirror configuration that is ideal for every situation. </p>
      <p>You can allow Greenplum Database to arrange mirrors on the hosts in the cluster using one
        of two standard configurations, or you can design your own mirroring configuration. </p>
      <p>The two standard mirroring arrangements are <i>group mirroring</i> and <i>spread
          mirroring</i>:<ul id="ul_uwr_gkv_tt">
          <li><b>Group mirroring</b> — Each host mirrors another host's primary segments. This is
            the default for <codeph>gpinitsystem</codeph> and <codeph>gpaddmirrors</codeph>.</li>
          <li><b>Spread mirroring</b> — Mirrors are spread across the available hosts. This requires
            that the number of hosts in the cluster is greater than the number of segments per
            host.</li>
        </ul></p>
      <p>You can design a custom mirroring configuration and use the Greenplum
          <codeph>gpaddmirrors</codeph> or <codeph>gpmovemirrors</codeph> utilities to set up the
        configuration.</p>
      <p><i>Block mirroring</i> is a custom mirror configuration that divides hosts in the cluster
        into equally sized blocks and distributes mirrors evenly to hosts within the block. If a
        primary segment fails, its mirror on another host within the same block becomes the active
        primary. If a segment host fails, mirror segments on each of the other hosts in the block
        become active.</p>
      <p>The following sections compare the group, spread, and block mirroring configurations. </p>
      <section>
        <title>Group Mirroring</title>
        <p>Group mirroring is easiest to set up and is the default Greenplum mirroring
          configuration. It is least expensive to expand, since it can be done by adding as few as
          two hosts. There is no need to move mirrors after expansion to maintain a consistent
          mirror configuration. </p>
        <p>The following diagram shows a group mirroring configuration with eight primary segments
          on four hosts. </p>
        <p>
          <image href="./graphics/group-mirrors.png" id="image_igx_wds_wt"/>
        </p>
        <p>Unless both the primary and mirror of the same segment instance fail, up to half of your
          hosts can fail and the cluster will continue to run as long as resources (CPU, memory, and
          IO) are sufficient to meet the needs. </p>
        <p>Any host failure will degrade performance by half or more because the host with the
          mirrors will have twice the number of active primaries. If your resource utilization is
          normally greater than 50%, you will have to adjust your workload until the failed host is
          recovered or replaced. If you normally run at less than 50% resource utilization the
          cluster can continue to operate at a degraded level of performance until the failure is
          corrected.</p>
      </section>
      <section>
        <title>Spread Mirroring</title>
        <p>With spread mirroring, mirrors for each host's primary segments are spread across as many
          hosts as there are segments per host. Spread mirroring is easy to set up when the cluster
          is initialized, but requires that the cluster have at least one more host than there are
          segments per host.</p>
        <p>The following diagram shows the spread mirroring configuration for a cluster with three
          primaries on four hosts. </p>
        <p>
          <image href="./graphics/spread-mirrors.png" id="image_src_r2s_wt"/>
        </p>
        <p>Expanding a cluster with spread mirroring requires more planning and may take more time.
          You must either add a set of hosts equal to the number of primaries per host plus one, or
          you can add two nodes in a group mirroring configuration and, when the expansion is
          complete, move mirrors to recreate the spread mirror configuration.</p>
        <p>Spread mirroring has the least performance impact for a single failed host because each
          host's mirrors are spread across the maximum number of hosts. Load is increased by
            1/<i>Nth</i>, where <i>N</i> is the number of primaries per host. Spread mirroring is,
          however, the most likely configuration to suffer catastrophic failure if two or more hosts
          fail simultaneously. </p>
      </section>
      <section>
        <title>Block Mirroring</title>
        <p>With block mirroring, nodes are divided into blocks, for example a block of four or eight
          hosts, and the mirrors for segments on each host are placed on other hosts within the
          block. Depending on the number of hosts in the block and the number of primary segments
          per host, each host maintains more than one mirror for each other host's segments. </p>
        <p>The following diagram shows a single block mirroring configuration for a block of four
          hosts, each with eight primary segments:</p>
        <p>
          <image href="./graphics/block-mirrors-4x8.png" id="image_uy3_dfs_wt"/>
        </p>
        <p>If there are eight hosts, an additional four-host block is added with the mirrors for
          primary segments 32 through 63 set up in the same pattern. </p>
        <p>A cluster with block mirroring is easy to expand because each block is a self-contained
          primary mirror group. The cluster is expanded by adding one or more blocks. There is no
          need to move mirrors after expansion to maintain a consistent mirror setup. This
          configuration is able to survive multiple host failures as long as the failed hosts are in
          different blocks.</p>
        <p>Because each host in a block has multiple mirror instances for each other host in the
          block, block mirroring has a higher performance impact for host failures than spread
          mirroring, but a lower impact than group mirroring. The expected performance impact varies
          by block size and primary segments per node. As with group mirroring, if the resources are
          available, performance will be negatively impacted but the cluster will remain available.
          If resources are insufficient to accommodate the added load you must reduce the workload
          until the failed node is replaced. </p>
      </section>
      <section>
        <title>Implementing Block Mirroring</title>
        <p>Block mirroring is not one of the automatic options Greenplum Database offers when you
          set up or expand a cluster. To use it, you must create your own configuration. </p>
        <p>For a new Greenplum system, you can initialize the cluster without mirrors, and then run
            <codeph>gpaddmirrors -i <varname>mirror_config_file</varname></codeph> with a custom
          mirror configuration file to create the mirrors for each block. You must create the file
          system locations for the mirror segments before you run <codeph>gpaddmirrors</codeph>. See
          the <codeph>gpaddmirrors</codeph> reference page in the <cite>Greenplum Database Management
            Utility Guide</cite> for details. </p>
        <p>If you expand a system that has block mirroring or you want to implement block mirroring
          at the same time you expand a cluster, it is recommended that you complete the expansion
          first, using the default grouping mirror configuration, and then use the
            <codeph>gpmovemirrors</codeph> utility to move mirrors into the block configuration.</p>
        <p>To implement block mirroring with an existing system that has a different mirroring
          scheme, you must first determine the desired location for each mirror according to your
          block configuration, and then determine which of the existing mirrors must be relocated.
          Follow these steps: </p>
        <ol id="ol_qqn_nly_5t">
          <li>Run the following query to find the current locations of the primary and mirror
              segments:<codeblock>SELECT dbid, content, address, port, 
   replication_port, fselocation as datadir 
   FROM gp_segment_configuration, pg_filespace_entry 
   WHERE dbid=fsedbid AND content > -1
   ORDER BY dbid;</codeblock><p>The
                <codeph>gp_segment_configuration</codeph>, <codeph>pg_filespace</codeph>, and
                <codeph>pg_filespace_entry</codeph> system catalog tables contain the current
              segment configuration.</p></li>
          <li>Create a list with the current mirror location and the desired block mirroring
            location, then remove any mirrors from the list that are already on the correct
            host.</li>
          <li>Create an input file for the <codeph>gpmovemirrors</codeph> utility with an entry for
            each mirror that must be moved.<p>The <codeph>gpmovemirrors</codeph> input file has the
              following
              format:<codeblock>filespaceOrder=[filespace1_fsname[:filespace2_fsname:...]
old_address:port:fselocation new_address:port:replication_port:fselocation[:fselocation:...]</codeblock></p><p>The
              first non-comment line must be a line beginning with <codeph>filespaceOrder=</codeph>.
              Do not include the default <codeph>pg_system</codeph> filespace in the list of
              filespaces. Leave the list empty if you are using only the <codeph>pg_system</codeph>
              filespace.</p><p>The following example <codeph>gpmovemirrors</codeph> input file
              specifies three mirror segments to move.
              <codeblock>filespaceOrder=
sdw2:50001:/data2/mirror/gpseg1 sdw3:50000:51000:/data/mirror/gpseg1
sdw2:50001:/data2/mirror/gpseg2 sdw4:50000:51000:/data/mirror/gpseg2
sdw3:50001:/data2/mirror/gpseg3 sdw1:50000:51000:/data/mirror/gpseg3
</codeblock></p></li>
          <li>Run <codeph>gpmovemirrors</codeph> with a command like the
            following:<codeblock>gpmovemirrors -i <varname>mirror_config_file</varname></codeblock></li>
        </ol>
        <p>The <codeph>gpmovemirrors</codeph> utility validates the input file, calls
            <codeph>gp_recoverseg</codeph> to relocate each specified mirror, and removes the
          original mirror. It creates a backout configuration file which can be used as input to
            <codeph>gpmovemirrors</codeph> to undo the changes that were made. The backout file has
          the same name as the input file, with the suffix
              <codeph>_backout_<varname>timestamp</varname></codeph> added.</p>
        <p>See the <cite>Greenplum Database Management Utility Reference</cite> for complete
          information about the <codeph>gpmovemirrors</codeph> utility.</p>
      </section>
    </body>
  </topic>
</topic>
