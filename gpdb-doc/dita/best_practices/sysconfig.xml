<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_dt3_fkv_r4">
  <title>System Configuration</title>
  <shortdesc>Requirements and best practices for system administrators who are configuring Greenplum
    Database cluster hosts.</shortdesc>
  <body>
    <p>Configuration of the Greenplum Database cluster is usually performed as root.</p>
    <section id="file_system">
      <title>File System</title>
      <p>XFS is the file system used for Greenplum Database data directories. XFS should be mounted
        with the following mount options:
        <codeblock>rw,noatime,nobarrier,nodev,inode64,allocsize=16m</codeblock></p>
    </section>
    <section id="port_config">
      <title>Port Configuration</title>
      <p>Set up <codeph>ip_local_port_range</codeph> so it does not conflict with the Greenplum
        Database port ranges. For example:
        <codeblock>net.ipv4.ip_local_port_range = 3000         65535
PORT_BASE=2000
MIRROR_PORT_BASE=2100
REPLICATION_PORT_BASE=2200
MIRROR_REPLICATION_PORT_BASE=2300</codeblock></p>
    </section>
    <section id="io_config">
      <title>I/O Configuration</title>
      <p>Set the blockdev read-ahead size to 16384 on the devices that contain data directories.</p>
      <codeblock># /sbin/blockdev --getra /dev/sdb
16384</codeblock>
      <p>The deadline IO scheduler should be set for all data directory devices. </p>
      <codeblock> # cat /sys/block/sdb/queue/scheduler
 noop anticipatory [deadline] cfq </codeblock>
      <p>The maximum number of OS files and processes should be increased in the
          <codeph>/etc/security/limits.conf</codeph> file.
        <codeblock>* soft  nofile 65536
* hard  nofile 65536
* soft  nproc 131072
* hard  nproc 131072</codeblock></p>
      <p>Enable core files output to a known location and make sure <codeph>limits.conf</codeph>
        allows core files.
        <codeblock>kernel.core_pattern = /var/core/core.%h.%t
# grep core /etc/security/limits.conf  
* soft  core unlimited</codeblock></p>
    </section>
    <section id="os_mem_config">
      <title>OS Memory Configuration</title>
      <p>The Linux sysctl <codeph>vm.overcommit_memory</codeph> and
          <codeph>vm.overcommit_ratio</codeph> variables affect how the operating system manages
        memory allocation. These variables should be set as follows: </p>
      <p><codeph>vm.overcommit_memory</codeph> determines the method the OS uses for determining how
        much memory can be allocated to processes. This should be always set to 2, which is the only
        safe setting for the database. <note>For information on configuration of overcommit memory,
          refer to: <ul id="ul_pgp_4hz_m1b">
            <li><xref
                href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Memory_overcommitment&amp;sa=D&amp;ust=1499719618717000&amp;usg=AFQjCNErcHO7vErv4pn9fIhCxrR0XRiknA"
                format="html" scope="external"
                >https://en.wikipedia.org/wiki/Memory_overcommitment</xref></li>
            <li><xref
                href="https://www.google.com/url?q=https://www.kernel.org/doc/Documentation/vm/overcommit-accounting&amp;sa=D&amp;ust=1499719618717000&amp;usg=AFQjCNEmu5tZutAaN1KCSlIwz4hwqihkOQ"
                format="html" scope="external"
                >https://www.kernel.org/doc/Documentation/vm/overcommit-accounting</xref></li>
          </ul></note></p>
      <p><codeph>vm.overcommit_ratio</codeph> is the percent of RAM that is used for application
        processes. The default is 50 on Red Hat Enterprise Linux. See <xref
          href="#topic_dt3_fkv_r4/segment_mem_config" format="dita"/> for a formula to calculate an
        optimal value.</p>
      <p>Do not enable huge pages in the operating system.</p>
      <p>See also <xref href="workloads.xml#topic_hhc_z5w_r4"/>.</p>
    </section>
    <section id="shared_mem_config">
      <title>Shared Memory Settings </title>
      <p>Greenplum Database uses shared memory to communicate between <codeph>postgres</codeph>
        processes that are part of the same <codeph>postgres</codeph> instance. The following shared
        memory settings should be set in <codeph>sysctl</codeph> and are rarely modified.</p>
      <codeblock>kernel.shmmax = 500000000
kernel.shmmni = 4096
kernel.shmall = 4000000000</codeblock>
    </section>
    <section id="gpcheck">
      <title>Validate the Operating System </title>
      <p>Run <codeph>gpcheck</codeph> to validate the operating system configuration. See
          <codeph>gpcheck</codeph> in the <i>Greenplum Database Utility Guide</i>.</p>
    </section>
    <section id="host_segs">
      <title>Number of Segments per Host</title>
      <p>Determining the number of segments to execute on each segment host has immense impact on
        overall system performance. The segments share the host's CPU cores, memory, and NICs with
        each other and with other processes running on the host. Over-estimating the number of
        segments a server can accommodate is a common cause of suboptimal performance.</p>
      <p>The factors that must be considered when choosing how many segments to run per host include
        the following:<ul id="ul_z44_qfp_y4">
          <li>Number of cores</li>
          <li>Amount of physical RAM installed in the server</li>
          <li>Number of NICs</li>
          <li>Amount of storage attached to server</li>
          <li>Mixture of primary and mirror segments</li>
          <li>ETL processes that will run on the hosts</li>
          <li>Non-Greenplum processes running on the hosts</li>
        </ul></p>
    </section>
    <section id="segment_mem_config">
      <title>Resource Queue Segment Memory Configuration</title>
      <p>The <codeph>gp_vmem_protect_limit</codeph> server configuration parameter specifies the
        amount of memory that all active postgres processes for a single segment can consume at any
        given time. Queries that exceed this amount will fail. Use the following calculations to
        estimate a safe value for <codeph>gp_vmem_protect_limit</codeph>.<ol id="ol_osx_srq_kv">
          <li>Calculate <codeph>gp_vmem</codeph>, the host memory available to Greenplum Database,
            using this
            formula:<codeblock>gp_vmem = ((SWAP + RAM) – (7.5GB + 0.05 * RAM)) / 1.7</codeblock>where
              <codeph>SWAP</codeph> is the host's swap space in GB and <codeph>RAM</codeph> is the
            RAM installed on the host in GB.</li>
          <li>Calculate <codeph>max_acting_primary_segments</codeph>. This is the maximum number of
            primary segments that can be running on a host when mirror segments are activated due to
            a segment or host failure on another host in the cluster. With mirrors arranged in a
            4-host block with 8 primary segments per host, for example, a single segment host
            failure would activate two or three mirror segments on each remaining host in the failed
            host's block. The <codeph>max_acting_primary_segments</codeph> value for this
            configuration is 11 (8 primary segments plus 3 mirrors activated on failure). </li>
          <li>Calculate <codeph>gp_vmem_protect_limit</codeph> by dividing the total Greenplum
            Database memory by the maximum number of acting
            primaries:<codeblock>gp_vmem_protect_limit = gp_vmem / max_acting_primary_segments</codeblock>Convert
            to megabytes to find the value to set for the <codeph>gp_vmem_protect_limit</codeph>
            system configuration parameter. </li>
        </ol>
      </p>
      <p dir="ltr">For scenarios where a large number of workfiles are generated, adjust the
        calculation for <codeph>gp_vmem</codeph> to account for the
        workfiles:<codeblock>gp_vmem = ((SWAP + RAM) – (7.5GB + 0.05 * RAM - (300KB * <varname>total_#_workfiles</varname>))) / 1.7</codeblock></p>
      <p dir="ltr">For information about monitoring and managing workfile usage, see the
          <i>Greenplum Database Administrator Guide</i>.</p>
      <p dir="ltr">You can calculate the value of the <codeph>vm.overcommit_ratio</codeph> operating
        system parameter from the value of <codeph>gp_vmem</codeph>:</p>
      <codeblock>vm.overcommit_ratio = (RAM - 0.026 * gp_vmem) / RAM</codeblock>
      <p>See <xref href="#topic_dt3_fkv_r4/os_mem_config" format="dita"/> for more about about
          <codeph>vm.overcommit_ratio</codeph>.</p>
      <p>See also <xref href="workloads.xml#topic_hhc_z5w_r4"/>.</p>
    </section>
    <section id="statement_mem_config">
      <title>Resource Queue Statement Memory Configuration</title>
      <p>The <codeph>statement_mem</codeph> server configuration parameter is the amount of
        memory to be allocated to any single query in a segment database. If a statement requires
        additional memory it will spill to disk. Calculate the value for
          <codeph>statement_mem</codeph> with the following formula: </p>
      <p>
        <codeph>(gp_vmem_protect_limit * .9) / max_expected_concurrent_queries </codeph>
      </p>
      <p>For example, for 40 concurrent queries with <codeph>gp_vmem_protect_limit</codeph> set to
        8GB (8192MB), the calculation for <codeph>statement_mem</codeph> would be: </p>
      <p>
        <codeph>(8192MB * .9) / 40 = 184MB</codeph>
      </p>
      <p>Each query would be allowed 184MB of memory before it must spill to disk. </p>
      <p>To increase <codeph>statement_mem</codeph> safely you must either increase
          <codeph>gp_vmem_protect_limit</codeph> or reduce the number of concurrent queries. To
        increase <codeph>gp_vmem_protect_limit</codeph>, you must add physical RAM and/or swap
        space, or reduce the number of segments per host.</p>
      <p>Note that adding segment hosts to the cluster cannot help out-of-memory errors unless you
        use the additional hosts to decrease the number of segments per host.</p>
      <p>Spill files are created when there is not enough memory to fit all the mapper output,
        usually when 80% of the buffer space is occupied. </p>
      <p>Also, see <xref href="workloads.xml#topic_hhc_z5w_r4">Resource Management</xref> for best
        practices for managing query memory using resource queues.</p>
    </section>
    <section id="spill_files">
      <title>Resource Queue Spill File Configuration</title>
      <p>Greenplum Database creates <i>spill files</i> (also called <i>workfiles</i>) on disk if a
        query is allocated insufficient memory to execute in memory. A single query can create no
        more than 100,000 spill files, by default, which is sufficient for the majority of queries. </p>
      <p>You can control the maximum number of spill files created per query and per segment with
        the configuration parameter <codeph>gp_workfile_limit_files_per_query</codeph>. Set the
        parameter to 0 to allow queries to create an unlimited number of spill files. Limiting the
        number of spill files permitted prevents run-away queries from disrupting the system. </p>
      <p>A query could generate a large number of spill files if not enough memory is allocated to
        it or if data skew is present in the queried data. If a query creates more than the
        specified number of spill files, Greenplum Database returns this error: </p>
      <p>
        <codeph>ERROR: number of workfiles per query limit exceeded</codeph>
      </p>
      <p>Before raising the <codeph>gp_workfile_limit_files_per_query</codeph>, try reducing the
        number of spill files by changing the query, changing the data distribution, or changing the
        memory configuration.</p>
      <p>The <codeph>gp_toolkit</codeph> schema includes views that allow you to see information
        about all the queries that are currently using spill files. This information can be used for
        troubleshooting and for tuning queries:</p>
      <ul id="ul_dyd_1qq_bp">
        <li>The <codeph>gp_workfile_entries</codeph> view contains one row for each operator using
          disk space for workfiles on a segment at the current time. See <xref
            href="tuning_queries.xml#reading_explain_plan"/>for information about operators.</li>
        <li>The <codeph>gp_workfile_usage_per_query</codeph> view contains one row for each query
          using disk space for workfiles on a segment at the current time.</li>
        <li>The <codeph>gp_workfile_usage_per_segment</codeph> view contains one row for each
          segment. Each row displays the total amount of disk space used for workfiles on the
          segment at the current time.</li>
      </ul>
      <p>See the <i>Greenplum Database Reference Guide</i> for descriptions of the columns in these
        views.</p>
      <p>The <codeph>gp_workfile_compress_algorithm</codeph> configuration parameter specifies a
        compression algorithm to apply to spill files. It can have the value <codeph>none</codeph>
        (the default) or <codeph>zlib</codeph>. Setting this parameter to <codeph>zlib</codeph> can
        improve performance when spill files are used.</p>
      <p>A query could generate a large number of spill files if not enough memory is allocated to
        it or if data skew is present in the queried data. If a query creates more than the
        specified number of spill files, Greenplum Database returns this error: </p>
      <p>
        <codeph>ERROR: number of workfiles per query limit exceeded</codeph>
      </p>
      <p>Before raising the <codeph>gp_workfile_limit_files_per_query</codeph>, try reducing the
        number of spill files by changing the query, changing the data distribution, or changing the
        memory configuration. </p>
    </section>
  </body>
</topic>
