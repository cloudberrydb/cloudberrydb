<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="topic1" xml:lang="en">
  <title id="ii20941">Greenplum MapReduce Specification</title>
  <body>
    <p>This specification describes the document format and schema for defining Greenplum MapReduce
      jobs. <note>Both the MapReduce tool (<codeph>gpmapreduce</codeph>) and its associated YAML
        specification are deprecated and will be removed in the next major release (6.x) of
        Greenplum Database. Use other query formats, such as SQL or a procedural language extension,
        instead.</note></p>
    <p><xref href="http://en.wikipedia.org/wiki/MapReduce" scope="external" format="html"
        >MapReduce</xref> is a programming model developed by Google for processing and generating
      large data sets on an array of commodity servers. Greenplum MapReduce allows programmers who
      are familiar with the MapReduce model to write map and reduce functions and submit them to the
      Greenplum Database parallel engine for processing. </p>
    <p>To enable Greenplum to process MapReduce functions, define the functions in a document, then
      pass the document to the Greenplum MapReduce program, <codeph>gpmapreduce</codeph>, for
      execution by the Greenplum Database parallel engine. The Greenplum Database system distributes
      the input data, executes the program across a set of machines, handles machine failures, and
      manages the required inter-machine communication.</p>
    <p>See the <i>Greenplum Database Utility Guide</i> for information about
        <codeph>gpmapreduce</codeph>.</p>
  </body>
  <topic id="topic2" xml:lang="en">
    <title id="ii133181">Greenplum MapReduce Document Format</title>
    <body>
      <p>This section explains some basics of the Greenplum MapReduce document format to help you
        get started creating your own Greenplum MapReduce documents. Greenplum uses the <xref
          href="http://yaml.org/spec/1.1/" scope="external" format="html">YAML 1.1</xref> document
        format and then implements its own schema for defining the various steps of a MapReduce
        job.</p>
      <p>All Greenplum MapReduce files must first declare the version of the YAML specification they
        are using. After that, three dashes (<codeph>---</codeph>) denote the start of a document,
        and three dots (<codeph>...</codeph>) indicate the end of a document without starting a new
        one. Comment lines are prefixed with a pound symbol (<codeph>#</codeph>). It is possible to
        declare multiple Greenplum MapReduce documents in the same file:</p>
      <codeblock>%YAML 1.1
---
# Begin Document 1
# ...
---
# Begin Document 2
# ...</codeblock>
      <p>Within a Greenplum MapReduce document, there are three basic types of data structures or
          <i>nodes</i>: <i>scalars</i>, <i>sequences</i> and <i>mappings</i>.</p>
      <p>A <i>scalar</i> is a basic string of text indented by a space. If you have a scalar input
        that spans multiple lines, a preceding pipe ( <codeph>|</codeph> ) denotes a <i>literal</i>
        style, where all line breaks are significant. Alternatively, a preceding angle bracket (
          <codeph>&gt;</codeph> ) folds a single line break to a space for subsequent lines that
        have the same indentation level. If a string contains characters that have reserved meaning,
        the string must be quoted or the special character must be escaped with a backslash (
          <codeph>\</codeph> ).</p>
      <codeblock># Read each new line literally
somekey: <i>|</i>   this value contains two lines
   and each line is read literally
# Treat each new line as a space
anotherkey: <i>&gt;
</i>   this value contains two lines
   but is treated as one continuous line
# This quoted string contains a special character
ThirdKey: "This is a string: not a mapping"</codeblock>
      <p>A <i>sequence</i> is a list with each entry in the list on its own line denoted by a dash
        and a space (<codeph>- </codeph>). Alternatively, you can specify an inline sequence as a
        comma-separated list within square brackets. A sequence provides a set of data and gives it
        an order. When you load a list into the Greenplum MapReduce program, the order is kept.</p>
      <codeblock># list sequence
- this
- is
- a list
- with
- five scalar values
# inline sequence
[this, is, a list, with, five scalar values]</codeblock>
      <p>A <i>mapping</i> is used to pair up data values with indentifiers called <i>keys</i>.
        Mappings use a colon and space (<codeph>: </codeph>) for each <codeph>key: value</codeph>
        pair, or can also be specified inline as a comma-separated list within curly braces. The
          <i>key</i> is used as an index for retrieving data from a mapping. </p>
      <codeblock># a mapping of items
title: War and Peace
author: Leo Tolstoy
date: 1865
# same mapping written inline
{title: War and Peace, author: Leo Tolstoy, date: 1865}</codeblock>
      <p>Keys are used to associate meta information with each node and specify the expected node
        type (<i>scalar</i>, <i>sequence</i> or <i>mapping</i>). See <xref href="#topic3"
          type="topic" format="dita"/> for the keys expected by the Greenplum MapReduce program.</p>
      <p>The Greenplum MapReduce program processes the nodes of a document in order and uses
        indentation (spaces) to determine the document hierarchy and the relationships of the nodes
        to one another. The use of white space is significant. White space should not be used simply
        for formatting purposes, and tabs should not be used at all.</p>
    </body>
  </topic>
  <topic id="topic3" xml:lang="en">
    <title id="ii143911">Greenplum MapReduce Document Schema</title>
    <body>
      <p>Greenplum MapReduce uses the YAML document framework and implements its own YAML schema.
        The basic structure of a Greenplum MapReduce document is:</p>
      <codeblock>%YAML 1.1
---
<xref href="#topic3/VERSION" format="dita">VERSION</xref>: 1.0.0.2
<xref href="#topic3/DATABASE" format="dita">DATABASE</xref>: <i>dbname</i>
<xref href="#topic3/USER" format="dita">USER</xref>: <i>db_username</i>
<xref href="#topic3/HOST" format="dita">HOST</xref>: <i>master_hostname</i>
<xref href="#topic3/PORT" format="dita">PORT</xref>: <i>master_port</i></codeblock>
      <codeblock><xref href="#topic3/DEFINE" format="dita">DEFINE</xref>: 
  - <xref href="#topic3/INPUT" format="dita">INPUT</xref>:
     <xref href="#topic3/NAME" format="dita">NAME</xref>: <i>input_name</i>
     <xref href="#topic3/FILE" format="dita">FILE</xref>: 
      - <i>hostname</i>:<i>/path/to/file</i>
     <xref href="#topic3/GPFDIST" format="dita">GPFDIST</xref>:
       - <i>hostname</i>:port<i>/file_pattern</i>
     <xref href="#topic3/TABLE" format="dita">TABLE</xref>: <i>table_name</i>
     <xref href="#topic3/QUERY" format="dita">QUERY</xref>: <i>SELECT_statement</i>
     <xref href="#topic3/EXEC" format="dita">EXEC</xref>: <i>command_string</i>
     <xref href="#topic3/COLUMNS" format="dita">COLUMNS</xref>:
       - <i>field_name data_type</i>
     <xref href="#topic3/FORMAT" format="dita">FORMAT</xref>: TEXT | CSV
     <xref href="#topic3/DELIMITER" format="dita">DELIMITER</xref>: <i>delimiter_character</i>
     <xref href="#topic3/ESCAPE" format="dita">ESCAPE</xref>: <i>escape_character</i>
     <xref href="#topic3/NULL" format="dita">NULL</xref>: <i>null_string</i>
     <xref href="#topic3/QUOTE" format="dita">QUOTE</xref>: <i>csv_quote_character</i>
     <xref href="#topic3/ERROR_LIMIT" format="dita">ERROR_LIMIT</xref>: <i>integer</i>
     <xref href="#topic3/ENCODING" format="dita">ENCODING</xref>: <i>database_encoding</i></codeblock>
      <codeblock>  - <xref href="#topic3/OUTPUT" format="dita">OUTPUT</xref>:
     <xref href="#topic3/OUTPUTNAME" format="dita">NAME</xref>: <i>output_name</i>
     <xref href="#topic3/OUTPUTFILE" format="dita">FILE</xref>: <i>file_path_on_client</i>
     <xref href="#topic3/OUTPUTTABLE" format="dita">TABLE</xref>: <i>table_name</i>
     <xref href="#topic3/KEYS" format="dita">KEYS</xref>: 
<i>       - column_name</i>
     <xref href="#topic3/MODE" format="dita">MODE</xref>: REPLACE | APPEND</codeblock>
      <codeblock>  - <xref href="#topic3/MAP" format="dita">MAP</xref>:
     <xref href="#topic3/NAME" format="dita">NAME</xref>: <i>function_name</i>
     <xref href="#topic3/FUNCTION" format="dita">FUNCTION</xref>: <i>function_definition</i>
     <xref href="#topic3/LANGUAGE" format="dita">LANGUAGE</xref>: perl | python | c
     <xref href="#topic3/LIBRARY" format="dita">LIBRARY</xref>: <i>/path/filename.so</i>
     <xref href="#topic3/PARAMETERS" format="dita">PARAMETERS</xref>: 
<i>       - name</i><i>type</i>
     <xref href="#topic3/RETURNS" format="dita">RETURNS</xref>: 
<i>       - name</i><i>type</i>
     <xref href="#topic3/OPTIMIZE" format="dita">OPTIMIZE</xref>: STRICT IMMUTABLE
     <xref href="#topic3/MODE" format="dita">MODE</xref>: SINGLE | MULTI</codeblock>
      <codeblock>  - <xref href="#topic3/TCF" format="dita">TRANSITION | CONSOLIDATE | FINALIZE</xref>:
     <xref href="#topic3/TCFNAME" format="dita">NAME</xref>: <i>function_name</i>
     <xref href="#topic3/FUNCTION" format="dita">FUNCTION</xref>: <i>function_definition</i>
     <xref href="#topic3/LANGUAGE" format="dita">LANGUAGE</xref>: perl | python | c
     <xref href="#topic3/LIBRARY" format="dita">LIBRARY</xref>: <i>/path/filename.so</i>
     <xref href="#topic3/PARAMETERS" format="dita">PARAMETERS</xref>: 
<i>       - name</i><i>type</i>
     <xref href="#topic3/RETURNS" format="dita">RETURNS</xref>: 
<i>       - name</i><i>type</i>
     <xref href="#topic3/OPTIMIZE" format="dita">OPTIMIZE</xref>: STRICT IMMUTABLE
     <xref href="#topic3/TCFMODE" format="dita">MODE</xref>: SINGLE | MULTI</codeblock>
      <codeblock>  - <xref href="#topic3/REDUCE" format="dita">REDUCE</xref>:
     <xref href="#topic3/REDUCENAME" format="dita">NAME</xref>: <i>reduce_job_name</i>
     <xref href="#topic3/TRANSITION" format="dita">TRANSITION</xref>: <i>transition_function_name</i>
     <xref href="#topic3/CONSOLIDATE" format="dita">CONSOLIDATE</xref>: <i>consolidate_function_name</i>
     <xref href="#topic3/FINALIZE" format="dita">FINALIZE</xref>: <i>finalize_function_name</i>
     <xref href="#topic3/INITIALIZE" format="dita">INITIALIZE</xref>: <i>value</i>
     <xref href="#topic3/REDUCEKEYS" format="dita">KEYS</xref>:
<i>       - key_name</i></codeblock>
      <codeblock>  - <xref href="#topic3/TASK" format="dita">TASK</xref>:
     <xref href="#topic3/TASKNAME" format="dita">NAME</xref>: <i>task_name</i>
     <xref href="#topic3/SOURCE" format="dita">SOURCE</xref>: <i>input_name</i>
     <xref href="#topic3/TASKMAP" format="dita">MAP</xref>: <i>map_function_name</i>
     <xref href="#topic3/REDUCE" format="dita">REDUCE</xref>: <i>reduce_function_name</i>
<xref href="#topic3/EXECUTE" format="dita">EXECUTE</xref></codeblock>
      <codeblock>  - <xref href="#topic3/RUN" format="dita">RUN</xref>:
     <xref href="#topic3/EXECUTESOURCE" format="dita">SOURCE</xref>: <i>input_or_task_name</i>
     <xref href="#topic3/TARGET" format="dita">TARGET</xref>: <i>output_name</i>
     <xref href="#topic3/EXECUTEMAP" format="dita">MAP</xref>: <i>map_function_name</i>
     <xref href="#topic3/EXECUTEREDUCE" format="dita">REDUCE</xref>: <i>reduce_function_name</i>...</codeblock>
      <parml>
        <plentry>
          <pt id="VERSION">VERSION</pt>
          <pd>
            <p>Required. The version of the Greenplum MapReduce YAML specification. Current versions
              are 1.0.0.1.</p>
          </pd>
        </plentry>
        <plentry>
          <pt id="DATABASE">DATABASE</pt>
          <pd>Optional. Specifies which database in Greenplum to connect to. If not specified,
            defaults to the default database or <codeph>$PGDATABASE</codeph> if set.</pd>
        </plentry>
        <plentry>
          <pt id="USER">USER</pt>
          <pd>Optional. Specifies which database role to use to connect. If not specified, defaults
            to the current user or <codeph>$PGUSER</codeph> if set. You must be a Greenplum
            superuser to run functions written in untrusted Python and Perl. Regular database users
            can run functions written in trusted Perl. You also must be a database superuser to run
            MapReduce jobs that contain <xref href="#topic3/FILE" format="dita">FILE</xref>, <xref
              href="#topic3/GPFDIST" format="dita">GPFDIST</xref> and <xref href="#topic3/EXEC"
              format="dita">EXEC</xref> input types.</pd>
        </plentry>
        <plentry>
          <pt id="HOST">HOST</pt>
          <pd>Optional. Specifies Greenplum master host name. If not specified, defaults to
            localhost or <codeph>$PGHOST</codeph> if set.</pd>
        </plentry>
        <plentry>
          <pt id="PORT">PORT</pt>
          <pd>Optional. Specifies Greenplum master port. If not specified, defaults to 5432 or
              <codeph>$PGPORT</codeph> if set.</pd>
        </plentry>
        <plentry>
          <pt id="DEFINE">DEFINE</pt>
          <pd>Required. A sequence of definitions for this MapReduce document. The
              <codeph>DEFINE</codeph> section must have at least one <codeph>INPUT</codeph>
            definition. <parml>
              <plentry>
                <pt id="INPUT">INPUT</pt>
                <pd>Required. Defines the input data. Every MapReduce document must have at least
                  one input defined. Multiple input definitions are allowed in a document, but each
                  input definition can specify only one of these access types:a file, a
                    <codeph>gpfdist</codeph> file distribution program, a table in the database, an
                  SQL command, or an operating system command. See the <i>Greenplum Database Utility
                    Guide</i> for information about <codeph>gpfdist</codeph>.<parml>
                    <plentry>
                      <pt id="NAME">NAME</pt>
                      <pd>
                        <p>A name for this input. Names must be unique with regards to the names of
                          other objects in this MapReduce job (such as map function, task, reduce
                          function and output names). Also, names cannot conflict with existing
                          objects in the database (such as tables, functions or views).</p>
                      </pd>
                    </plentry>
                    <plentry>
                      <pt id="FILE">FILE</pt>
                      <pd>A sequence of one or more input files in the format:
                          <codeph>seghostname:/path/to/filename</codeph>. You must be a Greenplum
                        Database superuser to run MapReduce jobs with <codeph>FILE</codeph> input.
                        The file must reside on a Greenplum segment host.</pd>
                    </plentry>
                  </parml><parml>
                    <plentry>
                      <pt id="GPFDIST">GPFDIST</pt>
                      <pd>A sequence of one or more running <codeph>gpfdist</codeph> file
                        distribution programs in the format:
                          <codeph>hostname[:port]/file_pattern</codeph>. You must be a Greenplum
                        Database superuser to run MapReduce jobs with <codeph>GPFDIST</codeph>
                        input, unless the server configuration parameter <xref
                          href="config_params/guc_config.xml#topic1" type="topic" format="dita"/> is
                        set to <codeph>on</codeph>.</pd>
                    </plentry>
                    <plentry>
                      <pt id="TABLE">TABLE</pt>
                      <pd>The name of an existing table in the database.</pd>
                    </plentry>
                    <plentry>
                      <pt id="QUERY">QUERY</pt>
                      <pd>A SQL <codeph>SELECT</codeph> command to run within the database.</pd>
                    </plentry>
                    <plentry>
                      <pt id="EXEC">EXEC</pt>
                      <pd>An operating system command to run on the Greenplum segment hosts. The
                        command is run by all segment instances in the system by default. For
                        example, if you have four segment instances per segment host, the command
                        will be run four times on each host. You must be a Greenplum Database
                        superuser to run MapReduce jobs with <codeph>EXEC</codeph> input and the
                        server configuration parameter <xref
                          href="config_params/guc_config.xml#topic1" type="topic" format="dita"/> is
                        set to <codeph>on</codeph>.</pd>
                    </plentry>
                    <plentry>
                      <pt id="COLUMNS">COLUMNS</pt>
                      <pd>Optional. Columns are specified as: <codeph>column_name
                          </codeph><codeph>[</codeph><codeph>data_type</codeph><codeph>]</codeph>.
                        If not specified, the default is <codeph>value text</codeph>. The <xref
                          href="#topic3/DELIMITER" format="dita">DELIMITER</xref> character is what
                        separates two data value fields (columns). A row is determined by a line
                        feed character (<codeph>0x0a</codeph>).</pd>
                    </plentry>
                    <plentry>
                      <pt id="FORMAT">FORMAT</pt>
                      <pd>
                        <p>Optional. Specifies the format of the data - either delimited text
                            (<codeph>TEXT</codeph>) or comma separated values (<codeph>CSV</codeph>)
                          format. If the data format is not specified, defaults to
                            <codeph>TEXT</codeph>.</p>
                      </pd>
                    </plentry>
                    <plentry>
                      <pt id="DELIMITER">DELIMITER</pt>
                      <pd>
                        <p>Optional for <xref href="#topic3/FILE" format="dita">FILE</xref>, <xref
                            href="#topic3/GPFDIST" format="dita">GPFDIST</xref> and <xref
                            href="#topic3/EXEC" format="dita">EXEC</xref> inputs. Specifies a single
                          character that separates data values. The default is a tab character in
                            <codeph>TEXT</codeph> mode, a comma in <codeph>CSV</codeph> mode.The
                          delimiter character must only appear between any two data value fields. Do
                          not place a delimiter at the beginning or end of a row.</p>
                      </pd>
                    </plentry>
                    <plentry>
                      <pt id="ESCAPE">ESCAPE</pt>
                      <pd>Optional for <xref href="#topic3/FILE" format="dita">FILE</xref>, <xref
                          href="#topic3/GPFDIST" format="dita">GPFDIST</xref> and <xref
                          href="#topic3/EXEC" format="dita">EXEC</xref> inputs. Specifies the single
                        character that is used for C escape sequences (such as
                          <codeph>\n</codeph>,<codeph>\t</codeph>,<codeph>\100</codeph>, and so on)
                        and for escaping data characters that might otherwise be taken as row or
                        column delimiters. Make sure to choose an escape character that is not used
                        anywhere in your actual column data. The default escape character is a \
                        (backslash) for text-formatted files and a <codeph>"</codeph> (double quote)
                        for csv-formatted files, however it is possible to specify another character
                        to represent an escape. It is also possible to disable escaping by
                        specifying the value <codeph>'OFF'</codeph> as the escape value. This is
                        very useful for data such as text-formatted web log data that has many
                        embedded backslashes that are not intended to be escapes.</pd>
                    </plentry>
                    <plentry>
                      <pt id="NULL">NULL</pt>
                      <pd>Optional for <xref href="#topic3/FILE" format="dita">FILE</xref>, <xref
                          href="#topic3/GPFDIST" format="dita">GPFDIST</xref> and <xref
                          href="#topic3/EXEC" format="dita">EXEC</xref> inputs. Specifies the string
                        that represents a null value. The default is <codeph>\N</codeph> in
                          <codeph>TEXT</codeph> format, and an empty value with no quotations in
                          <codeph>CSV</codeph> format. You might prefer an empty string even in
                          <codeph>TEXT</codeph> mode for cases where you do not want to distinguish
                        nulls from empty strings. Any input data item that matches this string will
                        be considered a null value.</pd>
                    </plentry>
                    <plentry>
                      <pt id="QUOTE">QUOTE</pt>
                      <pd>Optional for <xref href="#topic3/FILE" format="dita">FILE</xref>, <xref
                          href="#topic3/GPFDIST" format="dita">GPFDIST</xref> and <xref
                          href="#topic3/EXEC" format="dita">EXEC</xref> inputs. Specifies the
                        quotation character for <codeph>CSV</codeph> formatted files. The default is
                        a double quote (<codeph>"</codeph>). In <codeph>CSV</codeph> formatted
                        files, data value fields must be enclosed in double quotes if they contain
                        any commas or embedded new lines. Fields that contain double quote
                        characters must be surrounded by double quotes, and the embedded double
                        quotes must each be represented by a pair of consecutive double quotes. It
                        is important to always open and close quotes correctly in order for data
                        rows to be parsed correctly.</pd>
                    </plentry>
                    <plentry>
                      <pt id="ERROR_LIMIT">ERROR_LIMIT</pt>
                      <pd>If the input rows have format errors they will be discarded provided that
                        the error limit count is not reached on any Greenplum segment instance
                        during input processing. If the error limit is not reached, all good rows
                        will be processed and any error rows discarded.</pd>
                    </plentry>
                  </parml><parml>
                    <plentry>
                      <pt id="ENCODING">ENCODING</pt>
                      <pd>Character set encoding to use for the data. Specify a string constant
                        (such as <codeph>'SQL_ASCII'</codeph>), an integer encoding number, or
                          <codeph>DEFAULT</codeph> to use the default client encoding. See <xref
                          href="character_sets.xml#topic1" type="topic" format="dita"/> for more
                        information.</pd>
                    </plentry>
                  </parml></pd>
              </plentry>
              <plentry>
                <pt id="OUTPUT">OUTPUT</pt>
                <pd>Optional. Defines where to output the formatted data of this MapReduce job. If
                  output is not defined, the default is <codeph>STDOUT</codeph> (standard output of
                  the client). You can send output to a file on the client host or to an existing
                  table in the database.<parml>
                    <plentry>
                      <pt id="OUTPUTNAME">NAME</pt>
                      <pd>A name for this output. The default output name is
                        <codeph>STDOUT</codeph>. Names must be unique with regards to the names of
                        other objects in this MapReduce job (such as map function, task, reduce
                        function and input names). Also, names cannot conflict with existing objects
                        in the database (such as tables, functions or views).</pd>
                    </plentry>
                    <plentry>
                      <pt id="OUTPUTFILE">FILE</pt>
                      <pd>Specifies a file location on the MapReduce client machine to output data
                        in the format: <codeph>/path/to/filename</codeph>.</pd>
                    </plentry>
                    <plentry>
                      <pt id="OUTPUTTABLE">TABLE</pt>
                      <pd>Specifies the name of a table in the database to output data. If this
                        table does not exist prior to running the MapReduce job, it will be created
                        using the distribution policy specified with <xref href="#topic3/KEYS"
                          format="dita">KEYS</xref>.</pd>
                    </plentry>
                    <plentry>
                      <pt id="KEYS">KEYS</pt>
                      <pd>Optional for <xref href="#topic3/OUTPUTTABLE" format="dita">TABLE</xref>
                        output. Specifies the column(s) to use as the Greenplum Database
                        distribution key. If the <xref href="#topic3/EXECUTE" format="dita"
                          >EXECUTE</xref> task contains a <xref href="#topic3/REDUCE" type="topic"
                          format="dita">REDUCE</xref> definition, then the <codeph>REDUCE</codeph>
                        keys will be used as the table distribution key by default. Otherwise, the
                        first column of the table will be used as the distribution key.</pd>
                    </plentry>
                    <plentry>
                      <pt id="MODE">MODE</pt>
                      <pd>Optional for <xref href="#topic3/OUTPUTTABLE" format="dita">TABLE</xref>
                        output. If not specified, the default is to create the table if it does not
                        already exist, but error out if it does exist. Declaring
                          <codeph>APPEND</codeph> adds output data to an existing table (provided
                        the table schema matches the output format) without removing any existing
                        data. Declaring <codeph>REPLACE</codeph> will drop the table if it exists
                        and then recreate it. Both <codeph>APPEND</codeph> and
                          <codeph>REPLACE</codeph> will create a new table if one does not
                        exist.</pd>
                    </plentry>
                  </parml></pd>
              </plentry>
              <plentry>
                <pt id="MAP">MAP</pt>
                <pd>Required. Each <codeph>MAP</codeph> function takes data structured in
                    (<codeph>key</codeph>, <codeph>value</codeph>) pairs, processes each pair, and
                  generates zero or more output (<codeph>key</codeph>, <codeph>value</codeph>)
                  pairs. The Greenplum MapReduce framework then collects all pairs with the same key
                  from all output lists and groups them together. This output is then passed to the
                    <xref href="#topic3/TASKREDUCE" format="dita">REDUCE</xref> task, which is
                  comprised of <xref href="#topic3/TCF" format="dita">TRANSITION | CONSOLIDATE |
                    FINALIZE</xref> functions. There is one predefined <codeph>MAP</codeph> function
                  named <codeph>IDENTITY</codeph> that returns (<codeph>key</codeph>,
                    <codeph>value</codeph>) pairs unchanged. Although (<codeph>key</codeph>,
                    <codeph>value</codeph>) are the default parameters, you can specify other
                  prototypes as needed.</pd>
              </plentry>
              <plentry>
                <pt id="TCF">TRANSITION | CONSOLIDATE | FINALIZE</pt>
                <pd><codeph>TRANSITION</codeph>, <codeph>CONSOLIDATE</codeph> and
                    <codeph>FINALIZE</codeph> are all component pieces of <xref
                    href="#topic3/REDUCE" format="dita">REDUCE</xref>. A <codeph>TRANSITION</codeph>
                  function is required. <codeph>CONSOLIDATE</codeph> and <codeph>FINALIZE</codeph>
                  functions are optional. By default, all take <codeph>state</codeph> as the first
                  of their input <xref href="#topic3/PARAMETERS" format="dita">PARAMETERS</xref>,
                  but other prototypes can be defined as well.</pd>
                <pd>
                  <p>A <codeph>TRANSITION</codeph> function iterates through each value of a given
                    key and accumulates values in a <codeph>state</codeph> variable. When the
                    transition function is called on the first value of a key, the
                      <codeph>state</codeph> is set to the value specified by <xref
                      href="#topic3/INITIALIZE" format="dita">INITALIZE</xref> of a <xref
                      href="#topic3/REDUCE" format="dita">REDUCE</xref> job (or the default state
                    value for the data type). A transition takes two arguments as input; the current
                    state of the key reduction, and the next value, which then produces a new
                      <codeph>state</codeph>.</p>
                </pd>
                <pd>If a <codeph>CONSOLIDATE</codeph> function is specified,
                    <codeph>TRANSITION</codeph> processing is performed at the segment-level before
                  redistributing the keys across the Greenplum interconnect for final aggregation
                  (two-phase aggregation). Only the resulting <codeph>state</codeph> value for a
                  given key is redistributed, resulting in lower interconnect traffic and greater
                  parallelism. <codeph>CONSOLIDATE</codeph> is handled like a
                    <codeph>TRANSITION</codeph>, except that instead of <codeph>(state + value)
                    =&gt; state</codeph>, it is <codeph>(state + state) =&gt; state</codeph>.</pd>
                <pd>If a <codeph>FINALIZE</codeph> function is specified, it takes the final
                    <codeph>state</codeph> produced by <codeph>CONSOLIDATE</codeph> (if present) or
                    <codeph>TRANSITION</codeph> and does any final processing before emitting the
                  final result. <codeph>TRANSITION</codeph> and <codeph>CONSOLIDATE
                  </codeph>functions cannot return a set of values. If you need a
                    <codeph>REDUCE</codeph> job to return a set, then a <codeph>FINALIZE</codeph> is
                  necessary to transform the final state into a set of output values.<parml>
                    <plentry>
                      <pt id="TCFNAME">NAME</pt>
                      <pd>Required. A name for the function. Names must be unique with regards to
                        the names of other objects in this MapReduce job (such as function, task,
                        input and output names). You can also specify the name of a function
                        built-in to Greenplum Database. If using a built-in function, do not supply
                          <xref href="#topic3/LANGUAGE" format="dita">LANGUAGE</xref> or a <xref
                          href="#topic3/FUNCTION" format="dita">FUNCTION</xref> body.</pd>
                    </plentry>
                    <plentry>
                      <pt id="FUNCTION">FUNCTION</pt>
                      <pd>Optional. Specifies the full body of the function using the specified
                          <xref href="#topic3/LANGUAGE" format="dita">LANGUAGE</xref>. If
                          <codeph>FUNCTION</codeph> is not specified, then a built-in database
                        function corresponding to <xref href="#topic3/TCFNAME" format="dita"
                          >NAME</xref> is used.</pd>
                    </plentry>
                    <plentry>
                      <pt id="LANGUAGE">LANGUAGE</pt>
                      <pd>Required when <xref href="#topic3/FUNCTION" format="dita">FUNCTION</xref>
                        is used. Specifies the implementation language used to interpret the
                        function. This release has language support for <codeph>perl</codeph>,
                          <codeph>python</codeph>, and <codeph>C</codeph>. If calling a built-in
                        database function, <codeph>LANGUAGE</codeph> should not be specified.</pd>
                    </plentry>
                    <plentry>
                      <pt id="LIBRARY">LIBRARY</pt>
                      <pd>Required when <xref href="#topic3/LANGUAGE" format="dita">LANGUAGE</xref>
                        is C (not allowed for other language functions). To use this attribute,
                          <xref href="#topic3/VERSION" format="dita">VERSION</xref> must be 1.0.0.2.
                        The specified library file must be installed prior to running the MapReduce
                        job, and it must exist in the same file system location on all Greenplum
                        hosts (master and segments).</pd>
                    </plentry>
                    <plentry>
                      <pt id="PARAMETERS">PARAMETERS</pt>
                      <pd>Optional. Function input parameters. The default type is
                          <codeph>text</codeph>. <p><codeph>MAP</codeph> default - <codeph>key
                            text</codeph>, <codeph>value
                            text</codeph></p><p><codeph>TRANSITION</codeph> default - <codeph>state
                            text</codeph>, <codeph>value
                            text</codeph></p><p><codeph>CONSOLIDATE</codeph> default -
                            <codeph>state1 text</codeph>, <codeph>state2 text</codeph> (must have
                          exactly two input parameters of the same data
                            type)</p><p><codeph>FINALIZE</codeph> default - <codeph>state
                            text</codeph> (single parameter only)</p></pd>
                    </plentry>
                    <plentry>
                      <pt id="RETURNS">RETURNS</pt>
                      <pd>Optional. The default return type is
                            <codeph>text</codeph>.<p><codeph>MAP</codeph> default - <codeph>key
                            text</codeph>, <codeph>value
                            text</codeph></p><p><codeph>TRANSITION</codeph> default - <codeph>state
                            text</codeph> (single return value
                            only)</p><p><codeph>CONSOLIDATE</codeph> default - <codeph>state
                            text</codeph> (single return value only)</p><p><codeph>FINALIZE</codeph>
                          default - <codeph>value text</codeph></p></pd>
                    </plentry>
                    <plentry>
                      <pt id="OPTIMIZE">OPTIMIZE</pt>
                      <pd>Optional optimization parameters for the
                            function:<p><codeph>STRICT</codeph> - function is not affected by
                            <codeph>NULL</codeph> values </p><p><codeph>IMMUTABLE</codeph> -
                          function will always return the same value for a given input</p></pd>
                    </plentry>
                    <plentry>
                      <pt id="TCFMODE">MODE</pt>
                      <pd>Optional. Specifies the number of rows returned by the function.
                            <p><codeph>MULTI</codeph> - returns 0 or more rows per input record. The
                          return value of the function must be an array of rows to return, or the
                          function must be written as an iterator using <codeph>yield</codeph> in
                          Python or <codeph>return_next</codeph> in Perl. <codeph>MULTI</codeph> is
                          the default mode for <codeph>MAP</codeph> and <codeph>FINALIZE</codeph>
                          functions.</p><p><codeph>SINGLE</codeph> - returns exactly one row per
                          input record. <codeph>SINGLE</codeph> is the only mode supported for
                            <codeph>TRANSITION</codeph> and <codeph>CONSOLIDATE</codeph> functions.
                          When used with <codeph>MAP</codeph> and <codeph>FINALIZE</codeph>
                          functions, <codeph>SINGLE</codeph> mode can provide modest performance
                          improvement.</p></pd>
                    </plentry>
                  </parml></pd>
              </plentry>
              <plentry>
                <pt id="REDUCE">REDUCE</pt>
                <pd>Required. A <codeph>REDUCE</codeph> definition names the <xref
                    href="#topic3/TCF" format="dita">TRANSITION | CONSOLIDATE | FINALIZE</xref>
                  functions that comprise the reduction of (<codeph>key</codeph>,
                    <codeph>value</codeph>) pairs to the final result set. There are also several
                  predefined <codeph>REDUCE</codeph> jobs you can execute, which all operate over a
                  column named <codeph>value</codeph>:<p><codeph>IDENTITY</codeph> - returns (key,
                    value) pairs unchanged</p><p><codeph>SUM</codeph> - calculates the sum of
                    numeric data </p><p><codeph>AVG</codeph> - calculates the average of numeric
                    data</p><p><codeph>COUNT</codeph> - calculates the count of input
                      data</p><p><codeph>MIN</codeph> - calculates minimum value of numeric data
                      </p><p><codeph>MAX</codeph> - calculates maximum value of numeric data</p><parml>
                    <plentry>
                      <pt id="REDUCENAME">NAME</pt>
                      <pd>Required. The name of this <codeph>REDUCE</codeph> job. Names must be
                        unique with regards to the names of other objects in this MapReduce job
                        (function, task, input and output names). Also, names cannot conflict with
                        existing objects in the database (such as tables, functions or views).</pd>
                    </plentry>
                    <plentry>
                      <pt id="TRANSITION">TRANSITION</pt>
                      <pd>Required. The name of the <codeph>TRANSITION</codeph> function.</pd>
                    </plentry>
                    <plentry>
                      <pt id="CONSOLIDATE">CONSOLIDATE</pt>
                      <pd>Optional. The name of the <codeph>CONSOLIDATE</codeph> function.</pd>
                    </plentry>
                    <plentry>
                      <pt id="FINALIZE">FINALIZE</pt>
                      <pd>Optional. The name of the <codeph>FINALIZE</codeph> function.</pd>
                    </plentry>
                    <plentry>
                      <pt id="INITIALIZE">INITIALIZE</pt>
                      <pd>Optional for <codeph>text</codeph> and <codeph>float</codeph> data types.
                        Required for all other data types. The default value for text is
                          <codeph>''</codeph> . The default value for float is <codeph>0.0</codeph>
                        . Sets the initial <codeph>state</codeph> value of the
                          <codeph>TRANSITION</codeph> function.</pd>
                    </plentry>
                    <plentry>
                      <pt id="REDUCEKEYS">KEYS</pt>
                      <pd>Optional. Defaults to <codeph>[key, *]</codeph>. When using a multi-column
                        reduce it may be necessary to specify which columns are key columns and
                        which columns are value columns. By default, any input columns that are not
                        passed to the <codeph>TRANSITION</codeph> function are key columns, and a
                        column named <codeph>key</codeph> is always a key column even if it is
                        passed to the <codeph>TRANSITION</codeph> function. The special indicator
                          <codeph>*</codeph> indicates all columns not passed to the
                          <codeph>TRANSITION</codeph> function. If this indicator is not present in
                        the list of keys then any unmatched columns are discarded.</pd>
                    </plentry>
                  </parml></pd>
              </plentry>
              <plentry>
                <pt id="TASK">TASK</pt>
                <pd>Optional. A <codeph>TASK</codeph> defines a complete end-to-end
                    <codeph>INPUT</codeph>/<codeph>MAP</codeph>/<codeph>REDUCE</codeph> stage within
                  a Greenplum MapReduce job pipeline. It is similar to <xref href="#topic3/EXECUTE"
                    format="dita">EXECUTE</xref> except it is not immediately executed. A task
                  object can be called as <xref href="#topic3/INPUT" format="dita">INPUT</xref> to
                  further processing stages.<parml>
                    <plentry>
                      <pt id="TASKNAME">NAME</pt>
                      <pd>Required. The name of this task. Names must be unique with regards to the
                        names of other objects in this MapReduce job (such as map function, reduce
                        function, input and output names). Also, names cannot conflict with existing
                        objects in the database (such as tables, functions or views).</pd>
                    </plentry>
                    <plentry>
                      <pt id="SOURCE">SOURCE</pt>
                      <pd>The name of an <xref href="#topic3/INPUT" format="dita">INPUT</xref> or
                        another <codeph>TASK</codeph>.</pd>
                    </plentry>
                    <plentry>
                      <pt id="TASKMAP">MAP</pt>
                      <pd>
                        <p>Optional. The name of a <xref href="#topic3/MAP" format="dita">MAP</xref>
                          function. If not specified, defaults to <codeph>IDENTITY</codeph>.</p>
                      </pd>
                    </plentry>
                    <plentry>
                      <pt id="TASKREDUCE">REDUCE</pt>
                      <pd>
                        <p>Optional. The name of a <xref href="#topic3/REDUCE" format="dita"
                            >REDUCE</xref> function. If not specified, defaults to
                            <codeph>IDENTITY</codeph>.</p>
                      </pd>
                    </plentry>
                  </parml></pd>
              </plentry>
            </parml></pd>
        </plentry>
        <plentry>
          <pt id="EXECUTE">EXECUTE</pt>
          <pd>Required. <codeph>EXECUTE</codeph> defines the final
              <codeph>INPUT</codeph>/<codeph>MAP</codeph>/<codeph>REDUCE</codeph> stage within a
            Greenplum MapReduce job pipeline.<parml>
              <plentry>
                <pt id="RUN">RUN</pt>
                <pd>
                  <parml>
                    <plentry>
                      <pt id="EXECUTESOURCE">SOURCE</pt>
                      <pd>Required. The name of an <xref href="#topic3/INPUT" format="dita"
                          >INPUT</xref> or <xref href="#topic3/TASK" format="dita">TASK</xref>.</pd>
                    </plentry>
                    <plentry>
                      <pt id="TARGET">TARGET</pt>
                      <pd>
                        <p>Optional. The name of an <xref href="#topic3/OUTPUT" format="dita"
                            >OUTPUT</xref>. The default output is <codeph>STDOUT</codeph>.</p>
                      </pd>
                    </plentry>
                    <plentry>
                      <pt id="EXECUTEMAP">MAP</pt>
                      <pd>Optional. The name of a <xref href="#topic3/MAP" format="dita">MAP</xref>
                        function. If not specified, defaults to <codeph>IDENTITY</codeph>.</pd>
                    </plentry>
                    <plentry>
                      <pt id="EXECUTEREDUCE">REDUCE</pt>
                      <pd>Optional. The name of a <xref href="#topic3/REDUCE" format="dita"
                          >REDUCE</xref> function. Defaults to <codeph>IDENTITY</codeph>.</pd>
                    </plentry>
                  </parml>
                </pd>
              </plentry>
            </parml></pd>
        </plentry>
      </parml>
    </body>
  </topic>
  <topic id="topic59" xml:lang="en">
    <title id="ii140655">Example Greenplum MapReduce Document</title>
    <body>
      <codeblock># This example MapReduce job processes documents and looks for keywords in them.
# It takes two database tables as input:
#   - documents (doc_id integer, url text, data text)
# - keywords (keyword_id integer, keyword text)#
# The documents data is searched for occurences of keywords and returns results of
# url, data and keyword (a keyword can be multiple words, such as "high performance # computing")
%YAML 1.1
---
VERSION:1.0.0.1

# Connect to Greenplum Database using this database and role
DATABASE:webdata
USER:jsmith

# Begin definition section
DEFINE:

 # Declare the input, which selects all columns and rows from the
 # 'documents' and 'keywords' tables.
- INPUT:
NAME:doc
TABLE:documents
- INPUT:
NAME:kw
TABLE:keywords
# Define the map functions to extract terms from documents and keyword
# This example simply splits on white space, but it would be possible
# to make use of a python library like nltk (the natural language toolkit)
# to perform more complex tokenization and word stemming.
 - MAP:
NAME:doc_map
 LANGUAGE:python
 FUNCTION:|
        i = 0            # the index of a word within the document
terms = {}# a hash of terms and their indexes within the document

# Lower-case and split the text string on space
for term in data.lower().split():
i = i + 1# increment i (the index)

        # Check for the term in the terms list:
        # if stem word already exists, append the i value to the array entry 
        # corresponding to the term. This counts multiple occurrences of the word.
        # If stem word does not exist, add it to the dictionary with position i.
        # For example:
 # data: "a computer is a machine that manipulates data" 
 # "a" [1, 4]
 # "computer" [2]
 # "machine" [3]
 # …
          if term in terms:
            terms[term] += ','+str(i)
          else:
            terms[term] = str(i)

# Return multiple lines for each document. Each line consists of 
# the doc_id, a term and the positions in the data where the term appeared.
        # For example: 
        #   (doc_id => 100, term => "a", [1,4]
        #   (doc_id => 100, term => "computer", [2]
        #    …
for term in terms:
yield([doc_id, term, terms[term]])
      OPTIMIZE:STRICT IMMUTABLE
      PARAMETERS:
- doc_id integer
        - data text
RETURNS:
- doc_id integer
        - term text
        - positions text

  # The map function for keywords is almost identical to the one for documents 
 # but it also counts of the number of terms in the keyword.
 - MAP:
NAME:kw_map
LANGUAGE:python
FUNCTION:|
        i = 0
        terms = {}
        for term in keyword.lower().split():
          i = i + 1
          if term in terms:
            terms[term] += ','+str(i)
          else:
            terms[term] = str(i)

 # output 4 values including i (the total count for term in terms):
yield([keyword_id, i, term, terms[term]])
      OPTIMIZE:STRICT IMMUTABLE
PARAMETERS:
- keyword_id integer
        - keyword text
RETURNS:
- keyword_id integer
        - nterms integer
        - term text
        - positions text

# A TASK is an object that defines an entire INPUT/MAP/REDUCE stage
# within a Greenplum MapReduce pipeline. It is like EXECUTION, but it is
# executed only when called as input to other processing stages.
# Identify a task called 'doc_prep' which takes in the 'doc' INPUT defined earlier
# and runs the 'doc_map' MAP function which returns doc_id, term, [term_position]
- TASK:
NAME:doc_prep
SOURCE:doc
MAP:doc_map

# Identify a task called 'kw_prep' which takes in the 'kw' INPUT defined earlier
# and runs the kw_map MAP function which returns kw_id, term, [term_position]
- TASK:
NAME:kw_prep
SOURCE:kw
MAP:kw_map

# One advantage of Greenplum MapReduce is that MapReduce tasks can be
# used as input to SQL operations and SQL can be used to process a MapReduce task.
# This INPUT defines a SQL query that joins the output of the 'doc_prep' 
# TASK to that of the 'kw_prep' TASK. Matching terms are output to the 'candidate' 
# list (any keyword that shares at least one term with the document).
- INPUT:
NAME: term_join
QUERY: |
        SELECT doc.doc_id, kw.keyword_id, kw.term, kw.nterms,
               doc.positions as doc_positions,
               kw.positions as kw_positions
          FROM doc_prep doc INNER JOIN kw_prep kw ON (doc.term = kw.term)

# In Greenplum MapReduce, a REDUCE function is comprised of one or more functions.
# A REDUCE has an initial 'state' variable defined for each grouping key. that is 
# A TRANSITION function adjusts the state for every value in a key grouping.
# If present, an optional CONSOLIDATE function combines multiple 
# 'state' variables. This allows the TRANSITION function to be executed locally at
# the segment-level and only redistribute the accumulated 'state' over
# the network. If present, an optional FINALIZE function can be used to perform 
# final computation on a state and emit one or more rows of output from the state.
#
# This REDUCE function is called 'term_reducer' with a TRANSITION function 
# called 'term_transition' and a FINALIZE function called 'term_finalizer'
- REDUCE:
NAME:term_reducer
TRANSITION:term_transition
FINALIZE:term_finalizer

- TRANSITION:
NAME:term_transition
 LANGUAGE:python
 PARAMETERS:
- state text
        - term text
        - nterms integer
        - doc_positions text
        - kw_positions text
FUNCTION: |

 # 'state' has an initial value of '' and is a colon delimited set 
        # of keyword positions. keyword positions are comma delimited sets of 
        # integers. For example, '1,3,2:4:' 
        # If there is an existing state, split it into the set of keyword positions
 # otherwise construct a set of 'nterms' keyword positions - all empty
if state:
          kw_split = state.split(':')
        else:
          kw_split = []
          for i in range(0,nterms):
            kw_split.append('')

        # 'kw_positions' is a comma delimited field of integers indicating what
 # position a single term occurs within a given keyword. 
        # Splitting based on ',' converts the string into a python list.
 # add doc_positions for the current term
for kw_p in kw_positions.split(','):
          kw_split[int(kw_p)-1] = doc_positions

        # This section takes each element in the 'kw_split' array and strings 
        # them together placing a ':' in between each element from the array.
 # For example: for the keyword "computer software computer hardware", 
        # the 'kw_split' array matched up to the document data of 
        # "in the business of computer software software engineers" 
        # would look like: ['5', '6,7', '5', ''] 
 # and the outstate would look like: 5:6,7:5:
outstate = kw_split[0]
        for s in kw_split[1:]:
          outstate = outstate + ':' + s
        return outstate

   - FINALIZE:
NAME: term_finalizer
LANGUAGE: python
RETURNS:
        - count integer
      MODE:MULTI
      FUNCTION:|
        if not state:
          return 0
        kw_split = state.split(':')

        # This function does the following:
 # 1) Splits 'kw_split' on ':'
        #    for example, 1,5,7:2,8 creates '1,5,7' and '2,8'
 # 2) For each group of positions in 'kw_split', splits the set on ',' 
        #    to create ['1','5','7'] from Set 0: 1,5,7 and 
        #    eventually ['2', '8'] from Set 1: 2,8
 # 3) Checks for empty strings
 # 4) Adjusts the split sets by subtracting the position of the set 
        #      in the 'kw_split' array
 # ['1','5','7'] - 0 from each element = ['1','5','7']
 # ['2', '8'] - 1 from each element = ['1', '7']
        # 5) Resulting arrays after subtracting the offset in step 4 are
 #    intersected and their overlaping values kept: 
        #    ['1','5','7'].intersect['1', '7'] = [1,7]
        # 6) Determines the length of the intersection, which is the number of 
 # times that an entire keyword (with all its pieces) matches in the 
 #    document data.
previous = None
        for i in range(0,len(kw_split)):
          isplit = kw_split[i].split(',')
          if any(map(lambda(x): x == '', isplit)):
            return 0
          adjusted = set(map(lambda(x): int(x)-i, isplit))
          if (previous):
            previous = adjusted.intersection(previous)
          else:
            previous = adjusted

        # return the final count
if previous:
          return len(previous)

   # Define the 'term_match' task which is then executed as part 
   # of the 'final_output' query. It takes the INPUT 'term_join' defined
   # earlier and uses the REDUCE function 'term_reducer' defined earlier
 - TASK:
NAME:term_match
SOURCE:term_join
REDUCE:term_reducer
- INPUT:
NAME:final_output
QUERY:|
        SELECT doc.*, kw.*, tm.count
        FROM documents doc, keywords kw, term_match tm
        WHERE doc.doc_id = tm.doc_id
          AND kw.keyword_id = tm.keyword_id
          AND tm.count > 0

# Execute this MapReduce job and send output to STDOUT
EXECUTE:
 - RUN:
SOURCE:final_output
TARGET:STDOUT</codeblock>
    </body>
    <topic id="topic60" xml:lang="en">
      <title>Flow Diagram for MapReduce Example</title>
      <body>
        <p>The following diagram shows the job flow of the MapReduce job defined in the example:</p>
        <image href="./graphics/mapreducejob.jpg" placement="break" width="487px" height="634px"/>
      </body>
    </topic>
  </topic>
</topic>
