<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="topic1" xml:lang="en">
  <title id="kh138244">Migrating Data from Greenplum 4.3 or 5</title>
  <shortdesc>You can migrate data from Greenplum Database 4.3 or 5 to Greenplum 6 using the standard
    backup and restore procedures, <codeph>gpbackup</codeph> and
    <codeph>gprestore</codeph>.</shortdesc>
  <body>
    <p>
      <note otherprops="oss-only">Open source Greenplum Database is available only for Greenplum
        Database 5 and later. </note>
      <note>Currently, you cannot upgrade a Greenplum Database 4.3 or 5 system directly to Greenplum
        Database 6.</note>
    </p>
    <p>This topic identifies known issues you may encounter when moving data from Greenplum 4.3 to
      Greenplum 6. You can work around these problems by making needed changes to your Greenplum 4.3
      databases so that you can create backups that can be restored successfully to Greenplum 6. </p>
    <ul id="ul_uw1_gnb_z3b">
      <li><xref href="#prep-gpdb6" format="dita"/></li>
      <li><xref href="#prep-gpdb4" format="dita"/></li>
      <li><xref href="#backup-and-restore" format="dita"/></li>
      <li><xref href="#after-migration" format="dita"/></li>
    </ul>
  </body>
  <topic id="prep-gpdb6">
    <title>Preparing the Greenplum 6 Cluster</title>
    <body>
      <ul id="ol_rzd_xfb_z3b">
        <li>Install and initialize a new Greenplum Database 6 cluster using the version 6
            <codeph>gpinitsystem</codeph> utility.
          <note><codeph>gprestore</codeph> only supports restoring data to a cluster that has an
            identical number of hosts and an identical number of segments per host, with each
            segment having the same <codeph>content_id</codeph> as the segment in the original
            cluster.</note>
          <note> Set the Greenplum Database 6 timezone to a value that is compatible with your host
            systems. Setting the Greenplum Database timezone prevents Greenplum Database from
            selecting a timezone each time the cluster is restarted. See <xref
              href="localization.xml" format="dita" scope="local">Configuring Timezone and
              Localization Settings</xref> for more information.</note></li>
        <li>Install the latest release of the Greenplum Backup and Restore utilities, available to
          download from <ph otherprops="pivotal"><xref
              href="https://network.pivotal.io/products/pivotal-gpdb-backup-restore" format="html"
              scope="external">Pivotal Network</xref></ph><ph otherprops="oss-only"><xref
              href="https://github.com/greenplum-db/gpbackup/releases" format="html"
              scope="external">github</xref></ph>.</li>
        <li>If you intend to install Greenplum Database 6 on the same hardware as your 4.3 system,
          you will need enough disk space to accommodate over five times the original data set (two
          full copies of the primary and mirror data sets, plus the original backup data in ASCII
          format) in order to migrate data with <codeph>gpbackup</codeph> and
            <codeph>gprestore</codeph>. Keep in mind that the ASCII backup data will require more
          disk space than the original data, which may be stored in compressed binary format.
          Offline backup solutions such as Dell EMC Data Domain can reduce the required disk space
          on each host.</li>
        <li>Install any external modules used in your Greenplum 4.3 system in the Greenplum 6 system
          before you restore the backup, for example MADlib or PostGIS. If versions of the external
          modules are not compatible, you may need to exclude tables that reference them when
          restoring the Greenplum 4.3 backup to Greenplum 6.</li>
        <li>When restoring language-based user-defined functions, the shared object file must be in
          the location specified in the <codeph>CREATE FUNCTION</codeph> SQL command and must have
          been recompiled on the Greenplum 6 system. This applies to user-defined functions,
          user-defined types, and any other objects that use custom functions, such as aggregates
          created with the <codeph>CREATE AGGREGATE</codeph> command.</li>
        <li>Greenplum 6 provides <i>resource groups</i>, an alternative to managing resources using
          resource queues. Setting the <codeph>gp_resource_manager</codeph> server configuration
          parameter to <codeph>queue</codeph> or <codeph>group</codeph> selects the resource
          management scheme the Greenplum Database system will use. The default is
            <codeph>queue</codeph>, so no action is required when you move from Greenplum version
          4.3 to version 6. To more easily transition from resource queues to resource groups, you
          can set resource groups to allocate and manage memory in a way that is similar to resource
          queue memory management. To select this feature, set the <codeph>MEMORY_LIMIT</codeph> and
            <codeph>MEMORY_SPILL_RATIO</codeph> attributes of your resource groups to 0. See <xref
            href="../admin_guide/workload_mgmt_resgroups.xml" format="dita" scope="peer">Using
            Resource Groups</xref> for information about enabling and configuring resource
          groups.</li>
      </ul>
    </body>
  </topic>
  <topic id="prep-gpdb4">
    <title>Preparing Greenplum 4.3 and 5 Databases for Backup</title>
    <body>
      <note>A Greenplum 4 system must be at least version 4.3.22 to use the
          <codeph>gpbackup</codeph> and <codeph>gprestore</codeph> utilities. A Greenplum 5 system
        must be at least version 5.5. Be sure to use the latest release of the backup and restore
        utilities, available for download from <ph otherprops="pivotal"><xref
            href="https://network.pivotal.io/products/pivotal-gpdb-backup-restore" format="html"
            scope="external">Pivotal Network</xref></ph><ph otherprops="oss-only"><xref
            href="https://github.com/greenplum-db/gpbackup/releases" format="html" scope="external"
            >github</xref></ph>.</note>
      <note type="important">Make sure that you have a complete backup of all data in the Greenplum
        Database 4.3 or 5 cluster, and that you can successfully restore the Greenplum Database
        cluster if necessary.</note>
      <p>Following are some issues that are known to cause errors when restoring a Greenplum 4.3 or
        5 backup to Greenplum 6. Keep a list of any changes you make to the Greenplum 4.3 or 5
        database to enable migration so that you can fix them in Greenplum 6 after restoring the
        backup.</p>
      <ul>
        <li>If you have configured PXF in your Greenplum Database 5 installation, review <xref
            scope="peer" href="../pxf/migrate_5to6.html" type="topic" format="html">Migrating PXF
            from Greenplum 5</xref> to plan for the PXF migration.</li>
        <li>Greenplum Database version 6 removes support for the <codeph>gphdfs</codeph> protocol.
          If you have created external tables that use gphdfs, remove the external table definitions
          and (optionally) recreate them to use Pivotal Extension Framework (PXF) before you migrate
          the data to Greenplum 6. Refer to <xref scope="peer" href="../pxf/gphdfs-pxf-migrate.html"
            >Migrating gphdfs External Tables to PXF</xref> in the PXF documentation for the
          migration procedure.</li>
        <li>References to catalog tables or their attributes can cause a restore to fail due to
          catalog changes from Greenplum 4.3 or 5 to Greenplum 6. Here are some catalog issues to be
          aware of when migrating to Greenplum 6:<ul id="ul_i54_jjj_s3b">
            <li>In the <codeph>pg_class</codeph> system table, the <codeph>reltoastidxid</codeph>
              column has been removed.</li>
            <li>In the <codeph>pg_stat_replication</codeph> system table, the
                <codeph>procpid</codeph> column is renamed to <codeph>pid</codeph>.</li>
            <li>In the <codeph>pg_stat_activity</codeph> system table, the <codeph>procpid</codeph>
              column is renamed to <codeph>pid</codeph>. The <codeph>current_query</codeph> column
              is removed. </li>
            <li>In the <codeph>gp_distribution_policy</codeph> system table, the
                <codeph>attrnums</codeph> column is renamed to <codeph>distkey</codeph> and its data
              type is changed to <codeph>int2vector</codeph>. A backend function
                <codeph>pg_get_table_distributedby()</codeph> was added to get the distribution
              policy for a table as a string.</li>
            <li>In the <codeph>session_level_memory_consumption</codeph> system view, the
                <codeph>__gp_localid</codeph> and <codeph>__gp_masterid</codeph> columns are
              removed.</li>
            <li>Filespaces are removed in Greenplum 6. The <codeph>pg_filespace</codeph> and
                <codeph>pg_filespace_entry</codeph> system tables are removed. Any reference to
                <codeph>pg_filespace</codeph> or <codeph>pg_filespace_entry</codeph> will fail in
              Greenplum 6.</li>
            <li>Restoring a Greenplum 4 backup can fail due to lack of dependency checking in
              Greenplum 4 catalog tables. For example, restoring a UDF can fail if it references a
              custom data type that is created later in the backup file.</li>
          </ul></li>
        <li>The <codeph>int4_avg_accum()</codeph> function signature changed in Greenplum 6 from
            <codeph>int4_avg_accum(bytea, integer)</codeph> to <codeph>int4_avg_accum(bigint[],
            integer)</codeph>. This function is the state transition function (<i>sfunc</i>) called
          when calculating the average of a series of 4-byte integers. If you have created a custom
          aggregate in a previous Greenplum release that called the built-in
            <codeph>int4_avg_accum()</codeph> function, you will need to revise your aggregate for
          the new signature. </li>
        <li><codeph>gpbackup</codeph> saves the distribution policy and distribution key for each
          table in the backup so that data can be restored to the same segment. If a table's
          distribution key in the Greenplum 4.3 or 5 database is incompatible with Greenplum 6,
            <codeph>gprestore</codeph> cannot restore the table to the correct segment in the
          Greenplum 6 database. This can happen if the distribution key in the older Greenplum
          release has columns with data types not allowed in Greenplum 6 distribution keys, or if
          the data representation for data types has changed or is insufficient for Greenplum 6 to
          generate the same hash value for a distribution key. You should correct these kinds of
          problems by altering distribution keys in the tables before you back up the Greenplum
          database. </li>
        <li>Greenplum 6 requires primary keys and unique index keys to match a table's distribution
          key. The leaf partitions of partitioned tables must have the same distribution policy as
          the root partition. These known issues should be corrected in the source Greenplum
          database before you back up the database:<ul id="ul_tpr_kzz_y3b">
            <li>If the primary key is different than the distribution key for a table, alter the
              table to either remove the primary key or change the primary key to match the
              distribution key.</li>
            <li>If the key columns for a unique index are not a subset of the distribution key
              columns, before you back up the source database, drop the index and, optionally,
              recreate it with a compatible key.</li>
            <li>If a partitioned table in the source database has a <codeph>DISTRIBUTED BY</codeph>
              distribution policy, but has leaf partitions that are <codeph>DISTRIBUTED
                RANDOMLY</codeph>, alter the leaf tables to match the root table distribution policy
              before you back up the source database. </li>
          </ul></li>
        <li>Columns of type <codeph>abstime</codeph>, <codeph>reltime</codeph>,
            <codeph>tinterval</codeph>, <codeph>money</codeph>, or <codeph>anyarray</codeph> are not
          supported as distribution keys in Greenplum 6.<p>If you have tables distributed on columns
            of type <codeph>abstime</codeph>, <codeph>reltime</codeph>, <codeph>tinterval</codeph>,
              <codeph>money</codeph>, or <codeph>anyarray</codeph>, use the <codeph>ALTER
              TABLE</codeph> command to set the distribution to <codeph>RANDOM</codeph> before you
            back up the database. After the data is restored, you can set a new distribution
            policy.</p></li>
        <li>Greenplum 6 no longer automatically converts from the deprecated timestamp format
          YYYYMMDDHH24MISS. The format could not be parsed unambiguously in previous Greenplum
          Database releases. You can still specify the YYYYMMDDHH24MISS format in conversion
          functions such as <codeph>to_timestamp</codeph> and <codeph>to_char</codeph> for
          compatibility with other database systems. You can use input formats for converting text
          to date or timestamp values to avoid unexpected results or query execution failures.</li>
        <li>Creating a table using the <codeph>CREATE TABLE AS</codeph> command in Greenplum 4.3 or
          5 could create a table with a duplicate distribution key. The <codeph>gpbackup</codeph>
          utility saves the table to the backup using a <codeph>CREATE TABLE</codeph> command that
          lists the duplicate keys in the <codeph>DISTRIBUTED BY</codeph> clause. Restoring this
          backup will cause a duplicate distribution key error. The <codeph>CREATE TABLE AS</codeph>
          command was fixed in Greenplum 5.10 to disallow duplicate distribution keys.</li>
        <li>Only Boolean operators can use Boolean negators. In Greenplum Database 4.3 and 5 it was
          possible to create a non-Boolean operator that specifies a Boolean negator function. For
          example, this <codeph>CREATE OPERATOR</codeph> command creates an integer
            <codeph>@@</codeph> operator with a Boolean
            negator:<codeblock>CREATE OPERATOR public.@@ (
        PROCEDURE = int4pl,
        LEFTARG = integer,
        RIGHTARG = integer,
        NEGATOR = OPERATOR(public.!!)
);</codeblock><p>If
            you restore a backup containing an operator like this to a Greenplum 6 system,
              <codeph>gprestore</codeph> produces an error: <codeph>ERROR: only boolean operators
              can have negators (SQLSTATE 42P13)</codeph>.</p></li>
        <li>In Greenplum Database 4.3 and 5, the undocumented server configuration parameter
            <codeph>allow_system_table_mods</codeph> could have a value of <codeph>none</codeph>,
            <codeph>ddl</codeph>, <codeph>dml</codeph>, or <codeph>all</codeph>. In Greenplum 6,
          this parameter has changed to a Boolean value, with a default value of
            <codeph>false</codeph>. If there are any references to this parameter in the source
          database, remove them to prevent errors during the restore.</li>
      </ul>
    </body>
  </topic>
  <topic id="backup-and-restore">
    <title>Backing Up and Restoring a Database</title>
    <body>
      <p>Create a <codeph>--metadata-only</codeph> backup from the source Greenplum database and
        restore it to the Greenplum 6 system to help find any additional problems that are not
        identified in <xref href="#prep-gpdb4" format="dita"/>. Refer to the <xref
          href="https://gpdb.docs.pivotal.io/backup-restore/latest/" format="html" scope="external"
          >Greenplum Backup and Restore documentation</xref> for syntax and examples for the
          <codeph>gpbackup</codeph> and <codeph>gprestore</codeph> utilities.</p>
      <p>Review the <codeph>gprestore</codeph> log file for error messages and correct any remaining
        problems in the source Greenplum database.</p>
      <p>When you are able to restore a metadata backup successfully, create the full backup and
        then restore it to the Greenplum 6 system. If needed, use the <codeph>gpbackup</codeph> or
          <codeph>gprestore</codeph> filter options to omit schemas or tables that cannot be
        restored without error. </p>
      <note type="important">When you restore a backup taken from a Greenplum Database 4.3 or 5
        system, <codeph>gprestore</codeph> warns that the restore will use legacy hash operators
        when loading the data. This is because Greenplum 6 has new hash algorithms that map
        distribution keys to segments, but the data in the backup set must be restored to the same
        segments as the cluster from which the backup was taken. The <codeph>gprestore</codeph>
        utility sets the <codeph>gp_use_legacy_hashops</codeph> server configuration parameter to
          <codeph>on</codeph> when restoring to Greenplum 6 from an earlier version so that the
        restored tables are created using the legacy operator classes instead of the new default
        operator classes. <p>After restoring, you can redistribute these tables with the
            <codeph>gp_use_legacy_hashops</codeph> parameter set to <codeph>off</codeph> so that the
          tables use the new Greenplum 6 hash operators. See <xref href="#redistribute"
            format="dita"/> for more information and examples.</p></note>
    </body>
  </topic>
  <topic id="after-migration">
    <title>Completing the Migration</title>
    <body>
      <p>Migrate any tables you skipped during the restore using other methods, for example using
        the <codeph>COPY TO</codeph> command to create an external file and then loading the data
        from the external file into Greenplum 6 with the <codeph>COPY FROM</codeph> command.</p>
      <p>Recreate any objects you dropped in the Greenplum 4.3 or 5 database to enable migration,
        such as external tables, indexes, user-defined functions, or user-defined aggregates.</p>
      <p>Here are some additional items to consider to complete your migration to Greenplum 6.</p>
      <ul id="ul_spw_q2b_z3b">
        <li>If you are migrating from Greenplum Database 4.3.27 or an earlier 4.3.x release and have
          configured PgBouncer in your Greenplum Database installation, you must migrate to the new
          PgBouncer when you upgrade Greenplum Database. Refer to <xref
            href="../admin_guide/access_db/topics/pgbouncer.xml#pgb_migrate" format="dita"
            scope="peer">Migrating PgBouncer</xref> for specific migration instructions.</li>
        <li>Greenplum Database 5 and 6 remove automatic casts between the text type and other data
          types. After you migrate from Greenplum Database version 4.3 to version 6, this changed
          behavior may impact existing applications and queries. Refer to <xref href="43x_to_5x.xml"
            format="dita" scope="local">About Implicit Text Casting in Greenplum Database</xref> for
          information, including a discussion about<ph otherprops="pivotal"> supported and
            unsupported</ph><ph otherprops="oss-only"> recommended</ph> workarounds.</li>
        <li>After migrating data you may need to modify SQL scripts, administration scripts, and
          user-defined functions as necessary to account for changes in Greenplum Database version
          6. Review the <xref
            href="https://gpdb.docs.pivotal.io/latest/relnotes/GPDB_600_README.html" format="html"
            scope="external">Pivotal Greenplum 6.0.0 Release Notes</xref> for features and changes
          that may necessitate post-migration tasks.</li>
        <li>To use the new Greenplum 6 default hash operator classes, use the command <codeph>ALTER
            TABLE &lt;table> SET DISTRIBUTED BY (&lt;key>)</codeph> to redistribute tables restored
          from Greenplum 4.3 or 5 backups. The <codeph>gp_use_legacy_hashops</codeph> parameter must
          be set to <codeph>off</codeph> when your execute the command. See <xref
            href="#redistribute" format="dita"/> for more information about hash operator classes.
        </li>
      </ul>
    </body>
    <topic id="redistribute">
      <title>Working With Hash Operator Classes in Greenplum 6</title>
      <body>
        <p>Greenplum 6 has new <i>jump consistent</i> hash operators that map distribution keys for
          distributed tables to the segments. The new hash operators enable faster database
          expansion because they don't require redistributing rows unless they map to a different
          segment. The hash operators used in Greenplum 4.3 and 5 are present in Greenplum 6 as
          non-default legacy hash operator classes. For example, for integer columns, the new hash
          operator class is named <codeph>int_ops</codeph> and the legacy operator class is named
            <codeph>cdbhash_int_ops</codeph>.</p>
        <p>This example creates a table using the legacy hash operator class
            <codeph>cdbhash_int_ops</codeph>.</p>
        <codeblock>test=# <b>SET gp_use_legacy_hashops=on;</b>
SET            
test=# <b>CREATE TABLE t1 (
    c1 integer,
    c2 integer,
    p integer
) DISTRIBUTED BY (c1);</b>
CREATE TABLE
test=# <b>\d+ t1</b>
                          Table "public.t1"
 Column |  Type   | Modifiers | Storage | Stats target | Description
--------+---------+-----------+---------+--------------+-------------
 c1     | integer |           | plain   |              |
 c2     | integer |           | plain   |              |
 p      | integer |           | plain   |              |
Distributed by: (c1)</codeblock>
        <p>Notice that the distribution key is <codeph>c1</codeph>. If the
            <codeph>gp_use_legacy_hashops</codeph> parameter is <codeph>on</codeph> and the operator
          class is a legacy operator class, the operator class name is not shown. However, if
            <codeph>gp_use_legacy_hashops</codeph> is <codeph>off</codeph>, the legacy operator
          class name is reported with the distribution key.</p>
        <codeblock>test=# <b>SET gp_use_legacy_hashops=off;</b>
SET
test=# <b>\d+ t1
</b>                          Table "public.t1"
 Column |  Type   | Modifiers | Storage | Stats target | Description
--------+---------+-----------+---------+--------------+-------------
 c1     | integer |           | plain   |              |
 c2     | integer |           | plain   |              |
 p      | integer |           | plain   |              |
Distributed by: (c1 cdbhash_int4_ops)</codeblock>
        <p>The operator class name is reported only when it does not match the setting of the
            <codeph>gp_use_legacy_hashops</codeph> parameter.</p>
        <p>To change the table to use the new jump consistent operator class, use the <codeph>ALTER
            TABLE</codeph> command to redistribute the table with the
            <codeph>gp_use_legacy_hashops</codeph> parameter set to <codeph>off</codeph>.
          <note>Redistributing tables with a large amount of data can take a long time. </note></p>
        <codeblock>test=# <b>SHOW gp_use_legacy_hashops;</b>
 gp_use_legacy_hashops
-----------------------
 off
(1 row)

test=# <b>ALTER TABLE t1 SET DISTRIBUTED BY (c1);</b>
ALTER TABLE
test=# <b>\d+ t1</b>
                          Table "public.t1"
 Column |  Type   | Modifiers | Storage | Stats target | Description
--------+---------+-----------+---------+--------------+-------------
 c1     | integer |           | plain   |              |
 c2     | integer |           | plain   |              |
 p      | integer |           | plain   |              |
Distributed by: (c1)</codeblock>
        <p>To verify the default jump consistent operator class has been used, set
            <codeph>gp_use_legacy_hashops</codeph> to on before you show the table definition.</p>
        <codeblock>test=# <b>SET gp_use_legacy_hashops=on;</b>
SET
test=# <b>\d+ t1</b>
                          Table "public.t1"
 Column |  Type   | Modifiers | Storage | Stats target | Description
--------+---------+-----------+---------+--------------+-------------
 c1     | integer |           | plain   |              |
 c2     | integer |           | plain   |              |
 p      | integer |           | plain   |              |
Distributed by: (c1 int4_ops)</codeblock>
      </body>
    </topic>
  </topic>
</topic>
