<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="topic1" xml:lang="en">
  <title id="kk159835">Backing Up and Restoring Databases</title>
  <shortdesc>The topic describes Greenplum backup and restore features.</shortdesc>
  <body>
    <p>Taking regular backups ensures that you can restore your data or rebuild your Greenplum
      Database system if data corruption or system failure occur. You can also use backups to
      migrate data from one Greenplum Database system to another.</p>
  </body>
  <topic id="topic2" xml:lang="en">
    <title id="kk155065">Backup and Restore Operations</title>
    <body>
      <p>Greenplum Database supports parallel and non-parallel backup and restore. Parallel backup
        and restore operations scale regardless of the number of segments in your system. Greenplum
        Database also supports non-parallel backup and restore utilities to enable migration from
        PostgreSQL to Greenplum. See <xref href="#topic4" type="topic" format="dita"/>.</p>
    </body>
    <topic id="topic3" xml:lang="en">
      <title>Parallel Backup Overview</title>
      <body>
        <p>The Greenplum Database parallel dump utility <cmdname>gpcrondump</cmdname> backs up the
          Greenplum master instance and each active segment instance at the same time. </p>
        <p>By default, <cmdname>gpcrondump</cmdname> creates dump files in the
            <codeph>gp_dump</codeph> subdirectory. </p>
        <p>Several dump files are created for the master, containing database information such as
          DDL statements, the Greenplum system catalog tables, and metadata files.
            <cmdname>gpcrondump</cmdname> creates one dump file for each segment, which contains
          commands to recreate the data on that segment.</p>
        <p>You can perform full or incremental backups. To restore a database to its state when an
          incremental backup was made, you will need to restore the previous full backup and all
          subsequent incremental backups. </p>
        <p>Each file created for a backup begins with a 14-digit timestamp key that identifies the
          backup set the file belongs to. </p>
        <fig id="kk155499">
          <title>Parallel Backups in Greenplum Database</title>
          <image href="../../graphics/gp_dump.jpg" placement="break" width="402px" height="226px"/>
        </fig>
        <p id="kk155233"><cmdname>gpcrondump</cmdname> can be run directly in a terminal on the
          master host, or you can add it to <codeph>crontab</codeph> on the master host to schedule
          regular backups. See <xref href="#topic29" type="topic" format="dita"/>.</p>
      </body>
    </topic>
    <topic id="topic4" xml:lang="en">
      <title id="kk155276">Non-Parallel Backup</title>
      <body>
        <p>You can use the PostgreSQL non-parallel backup utilities <codeph>pg_dump</codeph> and
            <codeph>pg_dumpall</codeph> to migrate PostgreSQL databases to Greenplum Database. These
          utilities create a single dump file on the master host that contains all data from all
          active segments. In most cases, the master host does not have enough disk space for a
          single backup file of a distributed database.</p>
        <p>Another way to backup Greenplum Database data is to use the <codeph>COPY TO</codeph> SQL
          command to copy all or a portion of a table out of the database to a delimited text file
          on the master host. </p>
      </body>
    </topic>
    <topic id="topic5" xml:lang="en">
      <title id="kk156306">Parallel Restores</title>
      <body>
        <p>The Greenplum Database parallel restore utility <codeph>gpdbrestore</codeph> takes the
          timestamp key generated by <cmdname>gpcrondump</cmdname>, validates the backup set, and
          restores the database objects and data into a distributed database in parallel. Parallel
          restore operations require a complete backup set created by <cmdname>gpcrondump</cmdname>,
          a full backup and any required incremental backups.</p>
        <fig id="kk157614">
          <title>Parallel Restores in Greenplum Database</title>
          <image href="../../graphics/gp_restore.jpg" placement="break" width="437px" height="241px"
          />
        </fig>
        <p id="kk156487">The Greenplum Database <codeph>gpdbrestore</codeph> utility provides
          flexibility and verification options for use with the automated backup files produced by
            <cmdname>gpcrondump</cmdname> or with backup files moved from the Greenplum array to an
          alternate location. See <xref href="#topic32" type="topic" format="dita"/>.</p>
        <note type="note"><codeph>gpdbrestore</codeph> can also be used to copy files to the
          alternate location.</note>
      </body>
    </topic>
    <topic id="topic6" xml:lang="en">
      <title>Non-Parallel Restores</title>
      <body>
        <p>Greenplum supports the non-parallel PostgreSQL restore utility
            <codeph>pg_restore</codeph> to enable:</p>
        <ul>
          <li id="kk168986">Migration from PostgreSQL to Greenplum Database.</li>
          <li id="kk169050">Migration between Greenplum Database systems with different
            configurations, such as a source system with four segments to a target system with give
            segments. <cmdname>gpdbrestore</cmdname> cannot distribute the source system's backup
            files evenly across an expanded system.</li>
        </ul>
        <p><codeph>pg_restore</codeph> requires compressed dump files created by
            <cmdname>pg_dump</cmdname> or <cmdname>pg_dumpall</cmdname>. Before starting the
          restore, modify the <codeph>CREATE TABLE</codeph> statements in the dump files to include
          the Greenplum <codeph>DISTRIBUTED</codeph> clause. </p>
        <note type="note">If you do not include the <codeph>DISTRIBUTED</codeph> clause, Greenplum
          Database assigns a default value. For details, see <codeph>CREATE TABLE</codeph> in the
            <cite>Greenplum Database Reference Guide</cite>.</note>
        <p>To perform a non-parallel restore using parallel backup files, collect each backup file
          from the segment hosts, copy them to the master host, and load them through the master.
          See <xref href="#topic33" type="topic" format="dita"/>.</p>
        <fig id="kk156418">
          <title>Non-parallel Restore Using Parallel Backup Files</title>
          <image href="../../graphics/nonpar_restore.jpg" placement="break" width="390px"
            height="231px"/>
        </fig>
      </body>
    </topic>
  </topic>
  <topic id="topic7" xml:lang="en">
    <title id="kk155429">Backing Up a Database</title>
    <body>
      <p>The options for database backup are as follows.</p>
      <ul>
        <li id="kk155624"><b>Schedule or run routine dumps with</b>
          <cmdname>gpcrondump</cmdname><b>.</b>
          <cmdname>gpcrondump</cmdname> allows you to schedule routine backups, including
          incremental backups, using the UNIX scheduling utility, <cmdname>cron</cmdname>. Schedule
            <cmdname>cron</cmdname> jobs that call <cmdname>gpcrondump</cmdname> on the Greenplum
          master host. <cmdname>gpcrondump</cmdname> backs up databases, data, and objects such as
          database roles and server configuration files.<p>Full backup jobs scheduled or run with
              <cmdname>gpcrondump</cmdname> can use Data Domain Boost. See <xref href="#topic17"
              type="topic" format="dita"/>. </p></li>
        <li id="kk171425"><b>Create a single dump file with</b>
          <cmdname>pg_dump</cmdname><b> or </b><cmdname>pg_dumpall</cmdname><b>.</b> Use this option
          to migrate your data to another database vendor's system. If restoring to a PostgreSQL or
          Greenplum database and the dump file is in archive format, you can use
            <cmdname>pg_restore</cmdname>. If the dump file is in plain text format, you can use a
          client such as <cmdname>psql</cmdname>. To restore to another Greenplum Database system,
          do a parallel dump using <cmdname>gpcrondump</cmdname> then do a non-parallel
          restore.</li>
      </ul>
      <p>When backing up a database, Greenplum Database locks the following tables:</p>
      <ol id="ol_vtk_x2n_1p">
        <li>When a backup starts, an <codeph>EXCLUSIVE</codeph> lock is acquired on the catalog
          table <i>pg_class</i> that contains database relation information. The
            <codeph>EXCLUSIVE</codeph> lock permits only concurrent read operations. Relations such
          as tables, indexes, and views cannot be created or dropped in the database.<p>While
              <i>pg_class</i> is locked, schema information is collected on database tables that
            will be backed up.</p><p>The <codeph>EXCLUSIVE</codeph> lock on <i>pg_class</i> is
            released after an <codeph>ACCESS SHARE</codeph> lock is acquired on all the tables that
            are to be backed up. See the next item. </p></li>
        <li> When backing up the table data, the backup operation acquires an <codeph>ACCESS
            SHARE</codeph> lock on the tables to be backed up after the schema information is
          collected. The <codeph>ACCESS SHARE</codeph> lock is acquired at the segment instance
          level as a parallel operation. After the data has been backed up for the tables in a
          segment, the lock on the tables in the segment is released. <p>An <codeph>ACCESS
              SHARE</codeph> lock is a lock that is acquired by queries that only read from a table.
            An <codeph>ACCESS SHARE</codeph> lock only conflicts with <codeph>ACCESS
              EXCLUSIVE</codeph> lock. The SQL command which acquire the <codeph>ACCESS
              EXCLUSIVE</codeph> lock:<ul id="ul_lj2_hpn_1p">
              <li><codeph>ALTER TABLE</codeph></li>
              <li><codeph>CLUSTER</codeph></li>
              <li><codeph>DROP TABLE</codeph></li>
              <li><codeph>REINDEX</codeph></li>
              <li><codeph>TRUNCATE</codeph></li>
              <li><codeph>VACUUM FULL</codeph></li>
            </ul></p></li>
      </ol>
      <p>For information about the catalog table <i>pg_class</i>, see the <i>Greenplum Database
          Reference Guide</i>. For information about Greenplum Database lock modes see <xref
          href="../dml.xml#topic2">About Concurrency Control</xref>.</p>
      <note>For external tables, the table definition is backed up, however the data is not backed
        up. For leaf child partition of a partitioned table that is a readable external table, the
        leaf child partition data is not backed up. </note>
    </body>
    <topic id="topic8" xml:lang="en">
      <title>Incremental Backup Support</title>
      <body>
        <p>For Greenplum Database Version 4.2.5 and later, the utilities
            <cmdname>gpcrondump</cmdname> and <cmdname>gpdbrestore</cmdname> support incremental
          backups and restores of append-optimized tables, including column-oriented tables. Use the
            <cmdname>gpcrondump</cmdname> option <cmdname>--incremental</cmdname> to create an
          incremental backup. </p>
        <p>An incremental backup is similar to a full backup. An incremental backup creates backup
          files for the master and segments. A unique 14-digit timestamp key identifies the files
          that comprise an incremental backup set. Similar to a full backup, an incremental backup
          backs up regular, heap-storage tables. The difference between an incremental backup and a
          full backup is that append-optimized tables are backed up only if one of the following
          operations was performed on the table after the last backup:</p>
        <ul id="ul_cvy_xhp_34">
          <li>
            <codeph>ALTER TABLE</codeph>
          </li>
          <li>
            <codeph>DELETE</codeph>
          </li>
          <li>
            <codeph>INSERT</codeph>
          </li>
          <li>
            <codeph>TRUNCATE</codeph>
          </li>
          <li>
            <codeph>UPDATE</codeph>
          </li>
          <li><codeph>DROP</codeph> and then re-create the table</li>
        </ul>
        <p>For partitioned append-optimized tables, only the changed table partitions are backed
          up.</p>
        <note type="important">For incremental back up sets, a full backup and associated
          incremental backups, the backup set must be on a single device. For example, a backup set
          must all be on a Data Domain system. The backup set cannot have some backups on a Data
          Domain system and others on the local file system or a NetBackup system. </note>
        <note>You can use a Data Domain server as an NFS file system (without Data Domain Boost) to
          perform incremental backups. </note>
        <p>To create an incremental backup or restore data from an incremental backup, you need the
          complete backup set. A <i>complete backup set</i> consists of a full backup and any
          incremental backups that were created after the last full backup. See <xref href="#topic9"
            type="topic" format="dita"/>.</p>
        <p>Because incremental backups are table-based for append-optimized tables, incremental
          backups are efficient when the updates are made to data in tables that are separate from
          tables that contain unchanged data, and the total amount of data in tables that have
          changed is small compared to the data in the tables that contain unchanged data.
          Incremental backups are also efficient with append-optimized partition tables if only a
          few table partitions are being updated. An incremental backup only backs up the partitions
          that have changed.</p>
        <p>When you archive incremental backups, all incremental backups between the last full
          backup and the target incremental backup must be archived. You must archive all the files
          that are created to back up the master and all segments. </p>
        <p>Changes to the Greenplum Database segment configuration invalidate incremental backups.
          After you change the segment configuration you must create a full backup before you can
          create an incremental backup.</p>
      </body>
      <topic id="topic9" xml:lang="en">
        <title id="kk175790">Example using full and incremental backups</title>
        <body>
          <p>Each backup set has a key, which is a timestamp taken when the backup is created. For
            example, if you created a backup that starts on May 14, 2016, the backup set file names
            would contain <codeph>20160514<varname>hhmmss</varname></codeph>. The
              <varname>hhmmss</varname> represents the time, hour, minute, and second.</p>
          <p>Assume you created both full and incremental backups of the database <i>mytest</i>. You
            used the following command to create full backups: </p>
          <codeblock>gpcrondump –x mytest –u /backupdir</codeblock>
          <p>You used this command to create incremental backups.</p>
          <codeblock>gpcrondump –x mytest –u /backupdir --incremental</codeblock>
          <p>When you specify the <codeph>-u</codeph> option, the backups are created in the
            directory <codeph>\backupdir</codeph> on the Greenplum Database hosts with the following
            timestamp keys. The full backups have the timestamp key 20160514054532 and
            20161114064330. The other backups are incremental backups.</p>
          <p id="p_ryy_33p_34">
            <ul id="ul_cwq_j3p_34">
              <li><codeph>20160514054532</codeph> (full backup)</li>
              <li>
                <codeph>20160714095512</codeph>
              </li>
              <li>
                <codeph>20160914081205</codeph>
              </li>
              <li><codeph>20161114064330</codeph> (full backup) </li>
              <li>
                <codeph>20170114051246</codeph>
              </li>
            </ul>
          </p>
          <p>If you create an incremental backup that starts on March 14, 2017, the timestamp for
            the backup would be <codeph>20170314<varname>hhmmss</varname></codeph>. To create the
            incremental backup, you need both the incremental backup <codeph>20170114051246</codeph>
            and the full backup <codeph>20161114064330</codeph>. Also, you must specify the same
              <codeph>-u</codeph> option for any incremental backups that are part of the backup
            set. </p>
          <p>To restore a database with the incremental backup <codeph>20160914081205</codeph>, you
            need the incremental backups <codeph>20160914081205</codeph> and
              <codeph>20160714095512</codeph>, and the full backup
            <codeph>20160514054532</codeph>.</p>
          <p>To restore the <i>mytest</i> database with the incremental backup
              <codeph>20170114051246</codeph>, you need only the incremental backup and the full
            backup <codeph>20161114064330</codeph>. The restore command would be similar to this
            command.</p>
          <codeblock>gpdbrestore –t 20170114051246 -u /backupdir
</codeblock>
        </body>
      </topic>
      <topic id="topic10" xml:lang="en">
        <title>Backing Up a Set of Tables</title>
        <body>
          <p>You can perform an incremental backup on a set of database tables by specifying the
              <cmdname>gpcrondump</cmdname> option <codeph>--prefix</codeph> to identify the backup
            set when you specify the tables to include or exclude when you create the full backup. </p>
          <p>First, create a full backup of a set of tables. When you create the full backup,
            specify the <codeph>--prefix</codeph> option to identify the backup set. To include a
            set of tables, use the <cmdname>gpcrondump</cmdname> option <codeph>-t</codeph> or
              <codeph>--table-file</codeph>. To exclude a set of tables, use the
              <cmdname>gpcrondump</cmdname> option <codeph>-T</codeph> or
              <codeph>--exclude-table-file</codeph>. </p>
          <p>First, create a full backup of a set of tables. When you create the full backup,
            specify the <codeph>--prefix</codeph> option to identify the backup set. To include a
            set of tables, use the <cmdname>gpcrondump</cmdname> option <codeph>-t</codeph> or
              <codeph>--table-file</codeph>. To exclude a set of tables, use the
              <cmdname>gpcrondump</cmdname> option <codeph>-T</codeph> or
              <codeph>--exclude-table-file</codeph>. You can also specify the option
              <codeph>-s</codeph> or <codeph>--schema-file</codeph> to include all tables that are
            qualified by a schema or set of schemas. To exclude a set tables that are qualified by a
            schema or a set of schemas, use the <cmdname>gpcrondump</cmdname> option
              <codeph>-S</codeph> or <codeph>--exclude-schema-file</codeph>. Only a set of tables or
            set of schemas can be specified. For example, the <codeph>-s</codeph> option cannot be
            specified with the <codeph>-t</codeph> option.</p>
          <p>To create an incremental backup based on the full backup of the set of tables, specify
            the <cmdname>gpcrondump</cmdname> option <codeph>--incremental</codeph> and the
              <codeph>--prefix</codeph> option with the string specified when creating the full
            backup. The incremental backup is limited to only the tables in the full backup.</p>
          <p>The following example uses the <codeph>--table-file</codeph> option to create a full
            backup for the set of tables listed in the file <codeph>user-tables</codeph>. The prefix
              <codeph>user_backup</codeph> identifies the backup set. </p>
          <p>
            <codeblock>gpcrondump -x mydatabase --table-file=user-tables --prefix <i>user_backup</i></codeblock>
          </p>
          <p>To create an incremental backup for the backup identified by the prefix
              <codeph>user_backup</codeph>, specify the <codeph>--incremental</codeph> option and
            the option <codeph>--prefix user_backup</codeph> to identify backup set. This example
            creates an incremental backup.</p>
          <p>
            <codeblock>gpcrondump -x mydatabase --incremental --prefix <i>user_backup</i></codeblock>
          </p>
          <p>This command lists the tables that were included or exclued for the full backup.</p>
          <p>
            <codeblock><codeph>gp</codeph>crondump -x mydatabase --incremental --prefix user_backup --list-filter-tables
</codeblock>
          </p>
        </body>
      </topic>
      <topic id="topic11" xml:lang="en">
        <title>Restoring From an Incremental Backup</title>
        <body>
          <p>When restoring a backup with <cmdname>gpdbrestore</cmdname>, command line output
            displays whether the backup is an incremental or a full backup. If the
              <cmdname>gpdbrestore</cmdname> option <codeph>-q</codeph> is specified, the backup
            type information is written to the log file.</p>
          <p>With the <cmdname>gpdbrestore</cmdname> option <codeph>--noplan</codeph>, you can
            restore only the data contained in an incremental backup.</p>
          <p>With the <codeph>--list-backup</codeph> option you can display the full and incremental
            backup sets required to perform a restore. </p>
        </body>
      </topic>
    </topic>
    <topic id="topic12" xml:lang="en">
      <title>Using Direct I/O</title>
      <body>
        <p>Direct I/O allows you to bypass the buffering of memory within the file system cache.
          When Direct I/O is used for a file, data is transferred directly from the disk to the
          application buffer, without the use of the file buffer cache. Direct I/O benefits
          applications by reducing CPU consumption and eliminating the overhead of copying data
          twice: first between the disk and the file buffer cache, and then from the file.</p>
        <note type="note">Direct I/O is supported only on RHEL, CentOS and SUSE.</note>
      </body>
      <topic id="topic13" xml:lang="en">
        <title>Turn on Direct I/O</title>
        <body>
          <codeblock>$ gpconfig -c gp_backup_directIO -v on
</codeblock>
        </body>
      </topic>
      <topic id="topic14" xml:lang="en">
        <title>Decrease network data chunks sent to dump when the database is busy</title>
        <body>
          <codeblock>$ gpconfig -c gp_backup_directIO_read_chunk_mb -v 10
</codeblock>
          <p>The above command sets the chunk size to 10MB; the default chunk size is 20MB. The
            default value has been tested to be the optimal setting. Decreasing it will increase the
            backup time and increasing it will result in little change to backup time.</p>
        </body>
      </topic>
      <topic id="topic15" xml:lang="en">
        <title>Verify the current data chunk size</title>
        <body>
          <codeblock>$ gpconfig –s gp_backup_directIO_read_chunk_mb
</codeblock>
        </body>
      </topic>
      <topic id="topic16" xml:lang="en">
        <title>Verify whether Direct I/O is turned on</title>
        <body>
          <codeblock>$ gpconfig –s gp_backup_directIO
</codeblock>
        </body>
      </topic>
    </topic>
    <topic id="topic17" xml:lang="en">
      <title id="kk171433">Using Data Domain Boost</title>
      <body>
        <p>Data Domain Boost is a <cmdname>gpcrondump</cmdname> and <cmdname>gpdbrestore</cmdname>
          option that provides faster backups after the initial backup operation, and provides
          deduplication at the source to decrease network traffic. When you restore files from the
          Data Domain system with Data Domain Boost, some files are copied to the master local disk
          and are restored from there, and others are restored directly.</p>
        <p>With Data Domain Boost managed file replication, you can replicate Greenplum Database
          backup images that are stored on a Data Domain system for disaster recover purposes. The
            <cmdname>gpmfr</cmdname> utility manages the Greenplum Database backup sets that are on
          the primary and a remote Data Domain system. For information about
            <cmdname>gpmfr</cmdname>, see the <i>Greenplum Database Utility Guide</i>.</p>
        <p>Managed file replication requires network configuration when a replication network is
          being used between two Data Domain systems:</p>
        <ul>
          <li id="kk178323">The Greenplum Database system requires the Data Domain login credentials
            to be configured with <cmdname>gpcrondump</cmdname>. These credentials are created for
            the local and remote Data Domain systems. </li>
          <li id="kk178334">When the non-management network interface is used for replication on the
            Data Domain systems, static routes must be configured on the systems to pass the
            replication data traffic to the correct interfaces.</li>
        </ul>
        <p>Do not use Data Domain Boost with <cmdname>pg_dump</cmdname>, or
            <cmdname>pg_dumpall</cmdname>. </p>
        <p>Refer to Data Domain Boost documentation for detailed information.</p>
        <note type="important">For incremental back up sets, a full backup and associated
          incremental backups, the backup set must be on a single device. For example, a backup set
          must all be on a file system. The backup set cannot have some backups on the local file
          system and others on a Data Domain system or a NetBackup system. </note>
        <note>You can use a Data Domain server as an NFS file system (without Data Domain Boost) to
          perform incremental backups. </note>
      </body>
      <topic id="topic18" xml:lang="en">
        <title>Data Domain Boost Requirements</title>
        <body>
          <p>Using Data Domain Boost requires the following.</p>
          <ul>
            <li id="kk169399">Purchase and install a Data Domain Boost license on the Data
              Domain.</li>
            <li id="kk169390">Obtain sizing recommendations for Data Domain Boost. </li>
          </ul>
          <p>Contact your Dell EMC Data Domain account representative for assistance.</p>
        </body>
      </topic>
      <topic id="topic19" xml:lang="en">
        <title id="kk161655">One-Time Data Domain Boost Credential Setup</title>
        <body>
          <p>There is a one-time process to set up credentials to use Data Domain Boost. Credential
            setup connects one Greenplum Database instance to one Data Domain instance. If you are
            using Data Domain Boost managed file replication capabilities for disaster recovery
            purposes, you must set up credentials for both the primary and remote Data Domain
            systems. </p>
          <p>To perform the credential setup, run <cmdname>gpcrondump</cmdname> with the following
            options:</p>
          <codeblock>--ddboost-host ddboost_hostname --ddboost-user ddboost_user 
--ddboost-backupdir backup_directory
</codeblock>
          <p>To remove credentials, run <cmdname>gpcrondump</cmdname> with the
              <codeph>--ddboost-config-remove</codeph> option.</p>
          <p>To manage credentials for the remote Data Domain system that is used for backup
            replication, specify the --ddboost-remote option with the other
              <cmdname>gpcrondump</cmdname> options. For example, these options set up credentials
            for a Data Domain system that is used for backup replication. The system IP address is
              <codeph>192.0.2.230</codeph>, the user ID is <codeph>ddboostmyuser</codeph>, and the
            location for the backups on the system is <codeph>GPDB/gp_production</codeph>:</p>
          <codeblock>--ddboost-host 192.0.2.230 --ddboost-user ddboostmyuser 
--ddboost-backupdir gp_production --ddboost-remote
</codeblock>
          <p>For details, see <cmdname>gpcrondump</cmdname> in the <i>Greenplum Database Utility
              Guide</i>.</p>
          <p>If you use two or more network connections to connect to the Data Domain system, use
              <cmdname>gpcrondump</cmdname> to set up the login credentials for the Data Domain
            hostnames associated with the network connections. To perform this setup for two network
            connections, run <cmdname>gpcrondump</cmdname> with the following options:</p>
          <codeblock>--ddboost-host ddboost_hostname1 
--ddboost-host ddboost_hostname2 --ddboost-user ddboost_user 
--ddboost-backupdir backup_directory
</codeblock>
        </body>
      </topic>
      <topic id="topic20" xml:lang="en">
        <title>Configuring Data Domain Boost for the Greenplum Database</title>
        <body>
          <p>After you set up credentials for Data Domain Boost on the Greenplum Database, perform
            the following tasks in Data Domain to allow Data Domain Boost to work with the Greenplum
            Database:</p>
          <ul>
            <li id="kk164115">
              <xref href="#topic21" type="topic" format="dita"/>
            </li>
            <li id="kk164176">
              <xref href="#topic22" type="topic" format="dita"/>
            </li>
            <li id="kk171548">
              <xref href="#topic23" type="topic" format="dita"/>
            </li>
          </ul>
        </body>
        <topic id="topic21" xml:lang="en">
          <title id="kk171637">Configuring Distributed Segment Processing in Data Domain</title>
          <body>
            <p>
              <ph>Configure the distributed segment processing option on the Data Domain system. The
                configuration applies to all the DCA servers and the Data Domain Boost plug-in
                installed on them. This option is enabled by default, but verify that it is enabled
                before using Data Domain Boost backups:</ph>
            </p>
            <codeblock># ddboost option show
</codeblock>
            <p>To enable <ph>distributed segment processing:</ph></p>
            <p># ddboost option set distributed-segment-processing {enabled | disabled}</p>
          </body>
        </topic>
        <topic id="topic22" xml:lang="en">
          <title id="kk163244">Configuring Advanced Load Balancing and Link Failover in Data
            Domain</title>
          <body>
            <p>If you have multiple network connections on a network subnet, you can create an
              interface group to provide load balancing and higher network throughput on your Data
              Domain system. When a Data Domain system on an interface group receives data from the
              media server clients, the data transfer is load balanced and distributed as separate
              jobs on the private network. You can achieve optimal throughput with multiple 1 GbE
              connections. </p>
            <note type="note">To ensure that interface groups function properly, use <ph>interface
                groups only when using multiple network connections on the same networking
                subnet.</ph></note>
            <p>To create an interface group on the Data Domain system, create interfaces with the
                <codeph>net</codeph> command (if interfaces do not already exist), add the
              interfaces to the group, and register the Data Domain system with the backup
              application. </p>
            <ol>
              <li id="kk163338">Add the interfaces to the group:<p>
                  <codeblock># ddboost ifgroup add interface 192.0.2.1
# ddboost ifgroup add interface 192.0.2.2
# ddboost ifgroup add interface 192.0.2.3
# ddboost ifgroup add interface 192.0.2.4
</codeblock>
                </p><note type="note">You can create only one interface group and this group cannot
                  be named.</note></li>
              <li id="kk163346">Select one interface on the Data Domain system to register with the
                backup application. Create a failover aggregated interface and register that
                interface with the backup application.<note type="note">You do not have to register
                  one of the <codeph>ifgroup </codeph>interfaces with the backup application. You
                  can use an interface that is not part of the <codeph>ifgroup</codeph> to register
                  with the backup application.</note></li>
              <li id="kk163367">Enable DD Boost on the Data Domain system:<p>
                  <codeblock># ddboost ifgroup enable</codeblock>
                </p></li>
              <li id="kk163369">Verify the Data Domain system configuration as follows:<p>
                  <codeblock># ddboost ifgroup show config</codeblock>
                </p><p>Results similar to the following appear.</p><p>
                  <codeblock>Interface
-------------
192.0.2.1
192.0.2.2
192.0.2.3
192.0.2.4
-------------
</codeblock>
                </p></li>
            </ol>
            <p>You can add or delete interfaces from the group at any time.</p>
            <note type="note">Manage Advanced Load Balancing and Link Failover (an interface group)
              using the <codeph>ddboost ifgroup</codeph> command or from the Enterprise Manager Data
              Management &gt; DD Boost view.</note>
          </body>
        </topic>
      </topic>
      <topic id="topic23" xml:lang="en">
        <title id="kk166437">Export the Data Domain Path to the DCA Network</title>
        <body>
          <p>The commands and options in this topic apply to DDOS 5.0.x and 5.1.x. See the Data
            Domain documentation for details.</p>
          <p> Use the following Data Domain commands to export the <codeph>/backup/ost </codeph>
            directory to the DCA for Data Domain Boost backups.
              <codeblock># nfs add /backup/ost 192.0.2.0/24, 198.51.100.0/24 (insecure)</codeblock><note
              type="note">The IP addresses refer to the Greenplum system working with the Data
              Domain Boost system.</note></p>
        </body>
        <topic id="topic24" xml:lang="en">
          <title>
            <ph>Create the Data Domain Login Credentials for the DCA</ph>
          </title>
          <body>
            <p>
              <ph>Create a username and password for the DCA to access the DD Boost Storage Unit
                (SU) at the time of backup and restore:</ph>
            </p>
            <codeblock># user add &lt;user&gt; [password &lt;password&gt;] [priv {admin | security | user}]</codeblock>
          </body>
        </topic>
      </topic>
      <topic id="topic25" xml:lang="en">
        <title>Backup Options for Data Domain Boost</title>
        <body>
          <p>Specify the <cmdname>gpcrondump</cmdname> options to match the setup. </p>
          <p>Data Domain Boost backs up files to the Data Domain system. Status and report files
            remain on the local disk. </p>
          <p>To configure Data Domain Boost to remove old backup directories before starting a
            backup operation, specify a <codeph>gpcrondump </codeph>backup expiration option. </p>
          <ul>
            <li id="kk169801">The <codeph>-c</codeph> option clears all backup directories.</li>
            <li id="kk169821">The <codeph>-o</codeph> option clears the oldest backup
              directory.</li>
          </ul>
          <p>To remove the oldest dump directory, specify <codeph>gpcrondump --ddboost</codeph> with
            the <codeph>-o</codeph> option. For example, if your retention period is 30 days, use
              <codeph>gpcrondump --ddboost</codeph> with the <codeph>-o</codeph> option on day
            31.</p>
          <p>Use <codeph>gpcrondump --ddboost</codeph> with the <codeph>-c</codeph> option to clear
            out all the old dump directories in <codeph>db_dumps</codeph>. The <codeph>-c</codeph>
            option deletes all dump directories that are at least one day old.</p>
        </body>
      </topic>
    </topic>
    <topic id="topic_o3l_xly_3p">
      <title>Using Veritas NetBackup</title>
      <body id="body_ebt_kzy_3p">
        <p>For Greenplum Database running Red Hat Enterprise Linux, you can configure Greenplum
          Database to perform backup and restore operations with Veritas NetBackup. To perform a
          back up or restore with NetBackup, you configure Greenplum Database and NetBackup and then
          run a Greenplum Database gpcrondump or gpdbrestore command. These are NetBackup
          topics.</p>
        <ul id="ul_zyz_fyy_3p">
          <li>
            <xref href="#topic_lpr_bcz_3p" format="dita"/>
          </li>
          <li>
            <xref href="#topic_t1d_qbz_3p" format="dita"/>
          </li>
          <li>
            <xref href="#topic_myv_lbz_3p" format="dita"/>
          </li>
          <li>
            <xref href="#topic_mlb_1bz_3p" format="dita"/>
          </li>
          <li>
            <xref href="#topic_vwm_v1z_3p" format="dita"/>
          </li>
          <li>
            <xref href="#topic_lk5_s1z_3p" format="dita"/>
          </li>
          <li>
            <xref href="#topic_nq4_m1z_3p" format="dita"/>
          </li>
        </ul>
      </body>
      <topic id="topic_lpr_bcz_3p">
        <title>About NetBackup Software</title>
        <body>
          <p>NetBackup includes the following server and the client software:</p>
          <ul id="ul_pm1_dcz_3p">
            <li>The NetBackup master server manages NetBackup backups, archives, and restores. The
              master server is responsible for media and device selection for NetBackup. </li>
            <li>NetBackup Media servers are the NetBackup device hosts that provide additional
              storage by allowing NetBackup to use the storage devices that are attached to
              them.</li>
            <li>NetBackup client software that resides on the Greenplum Database hosts that contain
              data to back up. </li>
          </ul>
          <p>See the <cite>Veritas NetBackup Getting Started Guide</cite> for information about
            NetBackup.</p>
        </body>
      </topic>
      <topic id="topic_t1d_qbz_3p">
        <title>Performing a Back Up or Restore with NetBackup </title>
        <body>
          <p>The Greenplum Database <codeph>gpcrondump</codeph> and <codeph>gpdbrestore</codeph>
            utilities support options to back up or restore data to a NetBackup storage unit. When
            performing a back up, Greenplum Database transfers data files directly to the NetBackup
            storage unit. No backup data files are created on the Greenplum Database hosts. The
            backup metadata files are both stored on the hosts and the backed up to the NetBackup
            storage unit. </p>
          <p>When performing a restore, the files are retrieved from the NetBackup server, and then
            restored. </p>
          <p>The gpcrondump utility options for NetBackup:</p>
          <pre>--netbackup-service-host netbackup_master_server
--netbackup-policy policy_name
--netbackup-schedule schedule_name
--netbackup-block-size size (optional)
--netbackup-keyword keword (optional) </pre>
          <p>The gpdbrestore utility options for NetBackup:</p>
          <pre>--netbackup-service-host netbackup_master_server
--netbackup-block-size size (optional)</pre>
          <note>When performing a restore operation from NetBackup, you must specify the backup
            timestamp with the gpdbrestore utility -t option.</note>
          <p>The policy name and schedule name are defined on the NetBackup master server. See <xref
              href="#topic_vwm_v1z_3p" format="dita"/> for information about policy name and
            schedule name. See the <cite>Greenplum Database Utility Guide</cite> for information
            about the Greenplum Database utilities. </p>
          <note>You must run the <codeph>gpcrondump</codeph> or <codeph>gpdbrestore</codeph> command
            during a time window defined for the NetBackup schedule. </note>
          <p>During a back up or restore operation, a separate NetBackup job is created for the
            following types of Greenplum Database data: </p>
          <ul id="ul_s5b_ybz_3p">
            <li>Segment data for each segment instance</li>
            <li>C database data</li>
            <li> Metadata</li>
            <li> Post data for the master </li>
            <li>State files Global objects (gpcrondump <codeph>-G</codeph> option)</li>
            <li>Configuration files for master and segments (gpcrondump <codeph>-g</codeph>
              option)</li>
            <li>Report files (gpcrondump <codeph>-h</codeph> option)</li>
          </ul>
          <p>In the NetBackup Administration Console, the Activity Monitor lists NetBackup jobs. For
            each job, the job detail displays Greenplum Database backup information. </p>
        </body>
      </topic>
      <topic id="topic_myv_lbz_3p">
        <title>Example Back Up and Restore Commands</title>
        <body>
          <p>This <codeph>gpcrondump</codeph> command backs up the database <i>customer</i> and
            specifies a NetBackup policy and schedule that are defined on the NetBackup master
            server <codeph>nbu_server1</codeph>. A block size of 1024 bytes is used to transfer data
            to the NetBackup server.
            <codeblock>gpcrondump -x customer --netbackup-service-host=nbu_server1
   --netbackup-policy=gpdb_cust --netbackup-schedule=gpdb_backup
   --netbackup-block-size=1024</codeblock></p>
          <p>This <codeph>gpdbrestore</codeph> command restores Greenplum Database data from the
            data managed by NetBackup master server <codeph>nbu_server1</codeph>. The option
              <codeph>-t 20170530090000</codeph> specifies the timestamp generated by
              <codeph>gpcrondump</codeph> when the backup was created. The <codeph>-e</codeph>
            option specifies that the target database is dropped before it is restored.</p>
          <codeblock>gpdbrestore -t 20170530090000 -e   --netbackup-service-host=nbu_server1</codeblock>
        </body>
      </topic>
      <topic id="topic_mlb_1bz_3p">
        <title>Configuring Greenplum Database Hosts for NetBackup</title>
        <body>
          <section>
            <p>You install and configure NetBackup client software on the Greenplum Database master
              host and all segment hosts. </p>
            <ol id="ol_ojl_gbz_3p">
              <li>Install the NetBackup client software on Greenplum Database hosts. See the
                NetBackup installation documentation for information on installing NetBackup clients
                on UNIX systems.</li>
              <li>Set parameters in the NetBackup configuration file
                  <codeph>/usr/openv/netbackup/bp.conf</codeph> on the Greenplum Database master and
                segment hosts. Set the following parameters on each Greenplum Database host. <table
                  frame="all" id="table_sll_gbz_3p">
                  <title>NetBackup bp.conf parameters for Greenplum Database</title>
                  <tgroup cols="2">
                    <colspec colname="c1" colnum="1" colwidth="1*"/>
                    <colspec colname="c2" colnum="2" colwidth="2.15*"/>
                    <thead>
                      <row>
                        <entry>Parameter</entry>
                        <entry>Description</entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry><codeph>SERVER</codeph></entry>
                        <entry>Host name of the NetBackup Master Server</entry>
                      </row>
                      <row>
                        <entry><codeph>MEDIA_SERVER</codeph></entry>
                        <entry>Host name of the NetBackup Media Server</entry>
                      </row>
                      <row>
                        <entry><codeph>CLIENT_NAME</codeph></entry>
                        <entry>Host name of the Greenplum Database Host</entry>
                      </row>
                    </tbody>
                  </tgroup>
                </table><p>See the <cite>Veritas NetBackup Administrator's Guide</cite> for
                  information about the bp.conf file. </p></li>
              <li>Set the <codeph>LD_LIBRARY_PATH</codeph> environment variable for Greenplum
                Database hosts to use NetBackup client 7.1 or 7.5. Greenplum Database ships with
                NetBackup SDK library files compatible with NetBackup 7.1 and 7.5 clients. To use
                the NetBackup 7.1 or 7.5 client that ships with Greenplum Database, add the
                following line to the file
                  <codeph>$GPHOME/greenplum_path.sh</codeph>:<codeblock>LD_LIBRARY_PATH=$GPHOME/lib/nbu<varname>NN</varname>/lib:$LD_LIBRARY_PATH </codeblock><p>Replace
                  the <varname>NN</varname> with 75 or 71 depending on the NetBackup client you need
                  to use. </p><p>The <codeph>LD_LIBRARY_PATH</codeph> line should be added before
                  this line in
                <codeph>$GPHOME/greenplum_path.sh</codeph></p><codeblock>export LD_LIBRARY_PATH</codeblock></li>
              <li>Run this command to remove the current <codeph>LD_LIBRARY_PATH</codeph>
                value:<codeblock>unset LD_LIBRARY_PATH</codeblock></li>
              <li>Run this command to update the environment variables for Greenplum
                Database:<codeblock>source $GPHOME/greenplum_path.sh</codeblock></li>
            </ol>
          </section>
          <section>
            <p>See the <cite>Veritas NetBackup Administrator's Guide</cite> for information about
              configuring NetBackup servers.</p>
            <ol id="ol_yml_gbz_3p">
              <li>Ensure that the Greenplum Database hosts are listed as NetBackup clients for the
                NetBackup server. <p>In the NetBackup Administration Console, the information for
                  the NetBackup clients, Media Server, and Master Server is in <uicontrol>NetBackup
                    Management</uicontrol> node within the <uicontrol>Host Properties</uicontrol>
                  node. </p></li>
              <li>Configure a NetBackup storage unit. The storage unit must be configured to point
                to a writable disk location.<p>In the NetBackup Administration Console, the
                  information for NetBackup storage units is in <uicontrol>NetBackup
                    Management</uicontrol> node within the <uicontrol>Storage</uicontrol> node.
                </p></li>
              <li>Configure a NetBackup backup policy and schedule within the policy. <p>In the
                  NetBackup Administration Console, the <uicontrol>Policy</uicontrol> node below the
                    <uicontrol>Master Server</uicontrol> node is where you create a policy and a
                  schedule for the policy. </p><ul id="ul_w4l_gbz_3p">
                  <li>In the <uicontrol>Policy Attributes</uicontrol> tab, these values are required
                    for Greenplum Database:<p>The value in the <uicontrol>Policy type</uicontrol>
                      field must be DataStore</p><p>The value in the <uicontrol>Policy
                        storage</uicontrol> field is the storage unit created in the previous
                      step.</p><p>The value in <uicontrol>Limit jobs per policy</uicontrol> field
                      must be at least 3. </p></li>
                  <li>In the <uicontrol>Policy Schedules</uicontrol> tab, create a NetBackup
                    schedule for the policy. </li>
                </ul></li>
            </ol>
          </section>
        </body>
      </topic>
      <topic id="topic_vwm_v1z_3p">
        <title>Configuring NetBackup for Greenplum Database</title>
        <body>
          <p>See the <cite>Veritas NetBackup Administrator's Guide</cite> for information about
            configuring NetBackup servers.</p>
          <ol id="ol_xqz_x1z_3p">
            <li>Ensure that the Greenplum Database hosts are listed as NetBackup clients for the
              NetBackup server. <p>In the NetBackup Administration Console, the information for the
                NetBackup clients, Media Server, and Master Server is in <uicontrol>NetBackup
                  Management</uicontrol> node within the <uicontrol>Host Properties</uicontrol>
                node. </p></li>
            <li>Configure a NetBackup storage unit. The storage unit must be configured to point to
              a writable disk location.<p>In the NetBackup Administration Console, the information
                for NetBackup storage units is in <uicontrol>NetBackup Management</uicontrol> node
                within the <uicontrol>Storage</uicontrol> node. </p></li>
            <li>Configure a NetBackup backup policy and schedule within the policy. <p>In the
                NetBackup Administration Console, the <uicontrol>Policy</uicontrol> node below the
                  <uicontrol>Master Server</uicontrol> node is where you create a policy and a
                schedule for the policy. </p><ul id="ul_xrz_x1z_3p">
                <li>In the <uicontrol>Policy Attributes</uicontrol> tab, these values are required
                  for Greenplum Database:<p>The value in the <uicontrol>Policy type</uicontrol>
                    field must be DataStore</p><p>The value in the <uicontrol>Policy
                      storage</uicontrol> field is the storage unit created in the previous
                    step.</p><p>The value in <uicontrol>Limit jobs per policy</uicontrol> field must
                    be at least 3. </p></li>
                <li>In the <uicontrol>Policy Schedules</uicontrol> tab, create a NetBackup schedule
                  for the policy. </li>
              </ul></li>
          </ol>
        </body>
      </topic>
      <topic id="topic_lk5_s1z_3p">
        <title>System Requirements</title>
        <body>
          <ul id="ul_xw4_51z_3p">
            <li>Greenplum Database 4.3.3 or later. <note>Greenplum Database uses the NetBackup API
                (XBSA) to communicate with the NetBackup. Greenplum Database uses SDK version XBSA
                1.1.0.</note></li>
            <li>NetBackup client software installed and configured on the Greenplum Database master
              host and all segment hosts.<p>The NetBackup client software must be able to
                communicate with the NetBackup server software.</p></li>
            <li>NetBackup Master Server Version 7.5 and NetBackup Media Server Version 7.5</li>
            <li>NetBackup Client version 7.1 or later.</li>
          </ul>
        </body>
      </topic>
      <topic id="topic_nq4_m1z_3p">
        <title>Limitations</title>
        <body>
          <ul id="ul_fdn_q1z_3p">
            <li>NetBackup is not compatible with DDBoost. Both NetBackup and DDBoost cannot be used
              in a single back up or restore operation. </li>
            <li>For incremental back up sets, a full backup and associated incremental backups, the
              backup set must be on a single device. For example, a backup set must all be on a
              NetBackup system. The backup set cannot have some backups on a NetBackup system and
              others on the local file system or a Data Domain system. </li>
          </ul>
        </body>
      </topic>
    </topic>
    <topic id="topic26" xml:lang="en">
      <title>Using Named Pipes</title>
      <body>
        <p>Greenplum Database supports using named pipes with <cmdname>gpcrondump</cmdname> and
            <codeph>gpdbrestore</codeph> to back up and restore a Greenplum database. When backing
          up a database with regular files, the files that contain the backup information are placed
          in directories on the Greenplum Database segments. When you use named pipes, you can
          configure named pipes on Greenplum Database segments to connect to another process, such
          as input process to a backup device. With named pipes you can back up data without the
          need for regular files to temporarily store the backup files. </p>
        <p>Backing up with named pipes is not supported if the option <codeph>--ddboost</codeph> is
          specified.</p>
        <section id="kk180388">
          <title>To back up a Greenplum database using named pipes:</title>
          <ol>
            <li id="kk180389">Generate the names of the named pipes with the
                <cmdname>gpcrondump</cmdname> options <codeph>-K </codeph><codeph>timestamp</codeph>
              and <codeph>--list-backup-files</codeph>. <p>The file names use the
                  <codeph>timestamp</codeph> specified by the
                  <codeph>-K </codeph><codeph>timestamp</codeph> option and have the suffix
                  <codeph>_pipes</codeph> and <codeph>_regular_files</codeph>. For example:
                </p><p><codeblock>gp_dump_20170514093000_pipes 
gp_dump_20170514093000_regular_files</codeblock>The
                file names listed in the <codeph>_pipes</codeph> file can be created as named pipes.
                The file names in the <codeph>_regular_files</codeph> file cannot be created as
                named pipes. <cmdname>gpcrondump</cmdname> and <codeph>gpdbrestore</codeph> use the
                information in these files during backup and restore operations.</p></li>
            <li id="kk180226">Create the named pipes as writable on all Greenplum Database
              segments.</li>
            <li id="kk180227">Back up the database using the named pipes.<p>To back up a complete
                set of Greenplum Database backup files, the files listed in the
                  <codeph>_regular_files</codeph> file must also be backed up.</p></li>
          </ol>
          <title>To restore a database that used named pipes during backup:</title>
          <ol>
            <li id="kk180233">Configure the named pipes as readable. </li>
            <li id="kk180234">Restore the database using the named pipes and the backup files.</li>
          </ol>
        </section>
      </body>
      <topic id="topic27" xml:lang="en">
        <title>Example</title>
        <body>
          <p>This <cmdname>gpcrondump</cmdname> command creates two text files that contain the file
            names that will be used to back up the database <codeph>testdb</codeph>. The files are
            created in the directory <codeph>/backups</codeph>.</p>
          <p>
            <codeblock>gpcrondump -x testdb -K 20170530090000 --list-backup-files -u /backups
</codeblock>
          </p>
          <p>After you create the writable named pipes on all the Greenplum Database segments, you
            run <cmdname>gpcrondump</cmdname> to back up the database.</p>
          <p>
            <codeblock>gpcrondump -x testdb -K 20170530090000 -u /backups
</codeblock>
          </p>
          <p>To restore the database with <codeph>gpdbrestore</codeph>, you configure the named
            pipes as readable and run this command:</p>
          <p>
            <codeblock>gpdbrestore -x testdb -t 20170530090000 -u /backups
</codeblock>
          </p>
        </body>
      </topic>
    </topic>
    <topic id="topic29" xml:lang="en">
      <title id="kk155437">Automating Parallel Backups with gpcrondump</title>
      <body>
        <p>You can call <cmdname>gpcrondump</cmdname> directly or from a <codeph>crontab</codeph>
          entry. Use <cmdname>gpcrondump</cmdname> to backup databases, data, and objects such as
          database roles and server configuration files.</p>
        <p>As the default, <cmdname>gpcrondump</cmdname> creates the dump files in the master and
          each segment instance's data directory in
            <codeph>&lt;data_directory&gt;/db_dumps/YYYYMMDD</codeph>. The segment data dump files
          are compressed using <codeph>gzip</codeph>.</p>
        <section id="kk156876">
          <title>To schedule a dump operation using CRON</title>
          <ol>
            <li id="kk156820">On the master, log in as the Greenplum superuser
                (<codeph>gpadmin</codeph>).</li>
            <li id="kk156881">Define a crontab entry that calls <cmdname>gpcrondump</cmdname>. For
              example, if your shell is <codeph>bin/bash</codeph> and the <codeph>PATH</codeph>
              includes the location of the Greenplum Database management utilities, schedule a
              nightly dump of the <i>sales</i> database at one minute past midnight as
                  follows:<p><b>Linux Example:</b></p><p>
                <codeblock>SHELL=/bin/bash
GPHOME=/usr/local/greenplum-db-4.3.0.0
MASTER_DATA_DIRECTORY=/data/gpdb_p1/gp-1
01 0 * * * gpadmin source $GPHOME/greenplum_path.sh;

gpcrondump -x sales -c -g -G -a -q &gt;&gt; gp_salesdump.log
</codeblock>
              </p></li>
            <li id="kk156938">Create a file named <codeph>mail_contacts</codeph> in either the
              Greenplum superuser's home directory or in <codeph>$GPHOME/bin</codeph>. For example:<p>
                <codeblock>$ vi /home/gpadmin/mail_contacts
$ vi /export/home/gpadmin/mail_contacts
</codeblock>
              </p></li>
            <li id="kk156853">In this file, type one email address per line. For example:<p>
                <codeblock>dba@example.com
jjones@example.com
</codeblock>
              </p></li>
            <li id="kk156856">Save and close the <codeph>mail_contacts</codeph> file.
                <cmdname>gpcrondump</cmdname> will send email notifications to the email addresses
              listed in this file.</li>
          </ol>
          <title>To schedule a dump operation using <codeph>CRON</codeph> with Data Domain
            Boost</title>
          <ol>
            <li id="kk160664">Ensure the <xref href="#topic19" type="topic" format="dita"/> is
              complete.</li>
            <li id="kk159968">Add the option <codeph>--ddboost</codeph> to the gpcrondump option:<p>
                <codeblock>gpcrondump -x mydatabase -z -v --ddboost 
</codeblock>
              </p></li>
          </ol>
          <note type="important">Do not use compression with Data Domain Boost backups. The
              <codeph>-z </codeph> option turns backup compression off.<p>Some of the options
              available in <cmdname>gpcrondump</cmdname> have different implications when using Data
              Domain Boost. For details, see <cmdname>gpcrondump</cmdname> in the <i>Greenplum
                Database Utility Guide</i>.</p></note>
        </section>
      </body>
    </topic>
  </topic>
  <topic id="topic30" xml:lang="en">
    <title id="kk159972">Restoring From Parallel Backup Files</title>
    <body>
      <p>How you restore a database from parallel backup files depends on how you answer the
        following questions.</p>
      <ol>
        <li id="kk156235"><b>Where are your backup files?</b> If your backup files are on the
          segment hosts where <cmdname>gpcrondump</cmdname> created them, you can restore the
          database with <codeph>gpdbrestore</codeph>. If you moved your backup files off the
          Greenplum array, for example to an archive server with <cmdname>gpcrondump</cmdname>, use
            <codeph>gpdbrestore</codeph>.</li>
        <li id="kk156245"><b>Are you recreating the Greenplum Database system, or just restoring
            your data? </b>If Greenplum Database is running and you are restoring your data, use
            <codeph>gpdbrestore</codeph>. If you lost your entire array and need to rebuild the
          entire system from backup, use <codeph>gpinitsystem</codeph>.</li>
        <li id="kk156252"><b>Are you restoring to a system with the same number of segment instances
            as your backup set?</b> If you are restoring to an array with the same number of segment
          hosts and segment instances per host, use <codeph>gpdbrestore</codeph>. If you are
          migrating to a different array configuration, you must do a non-parallel restore. See
            <xref href="#topic33" type="topic" format="dita"/>.</li>
      </ol>
    </body>
    <topic id="topic32" xml:lang="en">
      <title id="kk156079">Restoring a Database Using gpdbrestore</title>
      <body>
        <p>The <codeph>gpdbrestore</codeph> utility provides convenience and flexibility in
          restoring from a set of backup files created by <cmdname>gpcrondump</cmdname>. To restore
          using <codeph>gpdbrestore</codeph>, ensure that you have:</p>
        <ul>
          <li id="kk157813">A complete set of backup files created by a
              <cmdname>gpcrondump</cmdname> operation. A full backup and any required incremental
            backups. </li>
          <li id="kk157821">A running Greenplum Database system.</li>
          <li id="kk157825">A Greenplum Database system with the same number of primary segment
            instances as the system that was backed up.</li>
          <li id="kk166943">The database you are restoring to is created in the system.</li>
        </ul>
        <section id="kk167020">
          <title>To restore from an archive host using gpdbrestore</title>
          <p>This procedure assumes that the backup set was moved off the Greenplum array to another
            host in the network.</p>
          <ol>
            <li id="kk167022">Ensure that the archive host is reachable from the Greenplum master host:<p>
                <codeblock>$ ping archive_host</codeblock>
              </p></li>
            <li id="kk170774">Ensure that the restore's target database exists. For example:<p>
                <codeblock>$ createdb database_name</codeblock>
              </p></li>
            <li id="kk167026">From the master, run the <cmdname>gpdbrestore</cmdname> utility.
                <codeph>-R</codeph> specifies the host name and path to a complete backup set:<p>
                <codeblock>$ gpdbrestore -R <i>archive_host</i>:/gpdb/backups/archive/20160714</codeblock>
              </p></li>
          </ol>
          <title>To restore from a Data Domain system using gpdbrestore with Data Domain
            Boost</title>
          <ol>
            <li id="kk167029">Ensure the <xref href="#topic19" type="topic" format="dita"/> is
              complete.</li>
            <li id="kk167030">Add the option <codeph>--ddboost</codeph> to the
                <codeph>gpdbrestore</codeph> option:<p>
                <codeblock>$ gpdbrestore -t <i>backup_timestamp</i> -v -ddboost</codeblock>
              </p></li>
          </ol>
          <note type="note">Some of the options available in <cmdname>gpdbrestore</cmdname> have
            different implications when using Data Domain. For details, see
              <cmdname>gpdbrestore</cmdname>.</note>
        </section>
      </body>
    </topic>
    <topic id="topic33" xml:lang="en">
      <title id="kk174264">Restoring to a Different Greenplum System Configuration</title>
      <body>
        <p>To perform a parallel restore operation using <cmdname>gpdbrestore</cmdname>, the system
          you are restoring to must have the same configuration as the system that was backed up. To
          restore your database objects and data into a different system configuration, for example,
          to expand into a system with more segments, restore your parallel backup files by loading
          them through the Greenplum master. To perform a non-parallel restore, you must have:</p>
        <ul>
          <li id="kk157975">A full backup set created by a <cmdname>gpcrondump</cmdname> operation.
            The backup file of the master contains the DDL to recreate your database objects. The
            backup files of the segments contain the data. </li>
          <li id="kk157979">A running Greenplum Database system.</li>
          <li id="kk157983">The database you are restoring to exists in the system.</li>
        </ul>
        <p>Segment dump files contain a <cmdname>COPY</cmdname> command for each table followed by
          the data in delimited text format. Collect all of the dump files for all of the segment
          instances and run them through the master to restore your data and redistribute it across
          the new system configuration.</p>
        <section id="kk159158">
          <title>To restore a database to a different system configuration</title>
          <ol>
            <li id="kk159159">Ensure that you have a complete backup set, including dump files of
              the master (<codeph>gp_dump_1_1_<varname>timestamp</varname></codeph>,
                  <codeph>gp_dump_1_1_<varname>timestamp</varname>_post_data</codeph>) and one for
              each segment instance (<codeph>gp_dump_0_2_<varname>timestamp</varname></codeph>,
                  <codeph>gp_dump_0_3_<varname>timestamp</varname></codeph>,
                  <codeph>gp_dump_0_4_<varname>timestamp</varname></codeph>, and so on). Each dump
              file must have the same timestamp key. <cmdname>gpcrondump</cmdname> creates the dump
              files in each segment instance's data directory. You must collect all the dump files
              and move them to one location on the master host. You can copy each segment dump file
              to the master, load it, and then delete it after it loads successfully.</li>
            <li id="kk158088">Ensure that the database you are restoring to is created in the
              system. For
              example:<codeblock>$ createdb <varname>database_name</varname></codeblock></li>
            <li id="kk158025">Load the master dump file to restore the database objects. For
              example:<codeblock>$ psql <varname>database_name</varname> -f /gpdb/backups/gp_dump_1_1_20160714</codeblock></li>
            <li id="kk158097">Load each segment dump file to restore the data. For
              example:<codeblock>$ psql <varname>database_name</varname> -f /gpdb/backups/gp_dump_0_2_20160714
$ psql <varname>database_name</varname> -f /gpdb/backups/gp_dump_0_3_20160714
$ psql <varname>database_name</varname> -f /gpdb/backups/gp_dump_0_4_20160714
$ psql <varname>database_name</varname> -f /gpdb/backups/gp_dump_0_5_20160714
...</codeblock></li>
            <li id="kk159206">Load the post data file to restore database objects such as indexes,
              triggers, primary key constraints,
              etc.<codeblock>$ psql database_name -f /gpdb/backups/gp_dump_0_5_20160714_post_data</codeblock></li>
            <li>Update the database sequences based on the values from the original database. <p>You
                can use the system utilities <cmdname>gunzip</cmdname> and <cmdname>egrep</cmdname>
                to extract the sequence value information from the original file Greenplum Database
                master dump file <codeph>gp_dump_1_1_<varname>timestamp</varname>.gz</codeph> into a
                text file. This command extracts the information into the file
                  <codeph>schema_path_and_seq_next_val</codeph>.
                <codeblock>gunzip -c <varname>path_to_master_dump_directory</varname>/gp_dump_1_1_<varname>timestamp</varname>.gz | egrep "SET search_path|SELECT pg_catalog.setval"  
   > schema_path_and_seq_next_val</codeblock></p><p>This
                example command assumes the original Greenplum Database master dump file is in
                  <codeph>/data/gpdb/master/gpseg-1/db_dumps/20150112</codeph>.
                </p><codeblock>gunzip -c /data/gpdb/master/gpseg-1/db_dumps/20150112/gp_dump_1_1_20150112140316.gz 
  | egrep "SET search_path|SELECT pg_catalog.setval" > schema_path_and_seq_next_val</codeblock><p>After
                extracting the information, use the Greenplum Database <codeph>psql</codeph> utility
                to update the sequences in the database. This example command updates the sequence
                information in the database
              <i>test_restore</i>:</p><codeblock>psql test_restore -f schema_path_and_seq_next_val</codeblock></li>
          </ol>
        </section>
      </body>
    </topic>
  </topic>
</topic>
