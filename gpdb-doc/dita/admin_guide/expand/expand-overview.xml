<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="topic3" xml:lang="en">
  <title>System Expansion Overview</title>
  <body>
    <p>Data warehouses typically grow over time, often at a continuous pace, as additional data is
      gathered and the retention period increases for existing data. At times, it is necessary to
      increase database capacity to consolidate disparate data warehouses into a single database.
      The data warehouse may also require additional computing capacity (CPU) to accommodate added
      analytics projects. It is good to provide capacity for growth when a system is initially
      specified, but even if you anticipate high rates of growth, it is generally unwise to invest
      in capacity long before it is required. Database expansion, therefore, is a project that you
      should expect to have to execute periodically. </p>
    <p>When you expand your database, you should expect the following qualities:<ul
        id="ul_tgf_gpg_mr">
        <li>Scalable capacity and performance. When you add resources to a Greenplum Database, the
          capacity and performance are the same as if the system had been originally implemented
          with the added resources. </li>
        <li>Uninterrupted service during expansion. Regular workloads, both scheduled and ad-hoc,
          are not interrupted. A short, scheduled downtime period is required to initialize the new
          servers, similar to downtime required to restart the system. The length of downtime is
          unrelated to the size of the system before or after expansion. </li>
        <li>Transactional consistency. </li>
        <li>Fault tolerance. During the expansion, standard fault-tolerance mechanisms—such as
          segment mirroring—remain active, consistent, and effective. </li>
        <li>Replication and disaster recovery. Any existing replication mechanisms continue to
          function during expansion. Restore mechanisms needed in case of a failure or catastrophic
          event remain effective. </li>
        <li>Transparency of process. The expansion process employs standard Greenplum Database
          mechanisms, so administrators can diagnose and troubleshoot any problems.</li>
        <li>Configurable process. Expansion can be a long running process, but it can be fit into a
          schedule of ongoing operations. The expansion schema's tables allow administrators to
          prioritize the order in which tables are redistributed, and the expansion activity can be
          paused and resumed.</li>
      </ul></p>
    <p>The planning and physical aspects of an expansion project are a greater share of the work
      than expanding the database itself. It will take a multi-discipline team to plan and execute
      the project. For on-premise installations, space must be acquired and prepared for the new
      servers. The servers must be specified, acquired, installed, cabled, configured, and tested.
      For cloud deployments, similar plans should also be made. <xref
        href="expand-planning.xml#topic5" type="topic" format="dita"/> describes general
      considerations for deploying new hardware. </p>
    <p>After you provision the new hardware platforms and set up their networks, configure the
      operating systems and run performance tests using Greenplum utilities. The Greenplum Database
      software distribution includes utilities that are helpful to test and burn-in the new servers
      before beginning the software phase of the expansion. See <xref href="expand-nodes.xml#topic1"
      /> for steps to prepare the new hosts for Greenplum Database.</p>
    <p>Once the new servers are installed and tested, the software phase of the Greenplum Database
      expansion process begins. The software phase is designed to be minimally disruptive,
      transactionally consistent, reliable, and flexible. <ul id="ul_zwt_png_tt">
        <li>There is a brief period of downtime while the new segment hosts are initialized and the
          system is prepared for the expansion process. This downtime can be scheduled to occur
          during a period of low activity to avoid disrupting ongoing business operations. During
          the initialization process, the following tasks are performed:<ul id="ul_s3v_vfh_mr">
            <li>Greenplum Database software is installed. </li>
            <li>Databases and database objects are created on the new segment hosts. </li>
            <li>An expansion schema is created in the master database to control the expansion
              process.</li>
            <li>The distribution policy for each table is changed to <codeph>DISTRIBUTED
                RANDOMLY</codeph>.</li>
          </ul></li>
        <li>The system is restarted and applications resume. <ul id="ul_tmw_b3h_mr">
            <li>New segments are immediately available and participate in new queries and data
              loads. The existing data, however, is skewed. It is concentrated on the original
              segments and must be redistributed across the new total number of primary segments. </li>
            <li>Because tables now have a random distribution policy, the optimizer creates query
              plans that are not dependent on distribution keys. Some queries will be less efficient
              because more data motion operators are needed. </li>
          </ul></li>
        <li>Using the expansion control tables as a guide, tables and partitions are redistributed.
          For each table:<ul id="ul_uxt_png_tt">
            <li> An <codeph>ALTER TABLE</codeph> statement is issued to change the distribution
              policy back to the original policy. This causes an automatic data redistribution
              operation, which spreads data across all of the servers, old and new, according to the
              original distribution policy.</li>
            <li>The table's status is updated in the expansion control tables.</li>
            <li>The query optimizer creates more efficient execution plans by including the
              distribution key in the planning.</li>
          </ul></li>
        <li>When all tables have been redistributed, the expansion is complete.</li>
      </ul></p>
    <p>Redistributing data is a long-running process that creates a large volume of network and disk
      activity. It can take days to redistribute some very large databases. To minimize the effects
      of the increased activity on business operations, system administrators can pause and resume
      expansion activity on an ad hoc basis, or according to a predetermined schedule. Datasets can
      be prioritized so that critical applications benefit first from the expansion. </p>
    <p>In a typical operation, you run the <codeph>gpexpand</codeph> utility four times with
      different options during the complete expansion process. </p>
    <ol id="ol_jjf_pmg_tt">
      <li id="no165977">To <xref href="expand-initialize.xml#topic23">create an expansion input
          file</xref>:<codeblock>gpexpand -f <i>hosts_file</i></codeblock></li>
      <li id="no166063">To <xref href="expand-initialize.xml#expand-initialize">initialize segments
          and create the expansion
            schema</xref>:<codeblock>gpexpand -i <i>input_file</i> -D <i>database_name</i></codeblock><p><codeph>gpexpand</codeph>
          creates a data directory, copies user tables from all existing databases on the new
          segments, and captures metadata for each table in an expansion schema for status tracking.
          After this process completes, the expansion operation is committed and
        irrevocable.</p></li>
      <li id="no176239"><xref href="expand-redistribute.xml#topic28">To redistribute
          tables</xref>:<codeblock>gpexpand -d <i>duration</i></codeblock><p>At initialization,
            <codeph>gpexpand</codeph> nullifies hash distribution policies on tables in all existing
          databases, except for parent tables of a partitioned table, and sets the distribution
          policy for all tables to random distribution.</p><p>To complete system expansion, you must
          run <codeph>gpexpand</codeph> to redistribute data tables across the newly added segments.
          Depending on the size and scale of your system, redistribution can be accomplished in a
          single session during low-use hours, or you can divide the process into batches over an
          extended period. Each table or partition is unavailable for read or write operations
          during redistribution. As each table is redistributed across the new segments, database
          performance should incrementally improve until it exceeds pre-expansion performance
          levels.</p><p>You may need to run <codeph>gpexpand</codeph> several times to complete the
          expansion in large-scale systems that require multiple redistribution sessions.
            <codeph>gpexpand</codeph> can benefit from explicit table redistribution ranking; see
            <xref href="expand-planning.xml#topic10" type="topic" format="dita"/>.</p><p>Users can
          access Greenplum Database after initialization completes and the system is back online,
          but they may experience performance degradation on systems that rely heavily on hash
          distribution of tables. Normal operations such as ETL jobs, user queries, and reporting
          can continue, though users might experience slower response times.</p><p>When a table has
          a random distribution policy, Greenplum Database cannot enforce unique constraints (such
          as <codeph>PRIMARY KEY</codeph>). This can affect your ETL and loading processes until
          table redistribution completes because duplicate rows do not issue a constraint violation
          error. </p></li>
      <li id="no165990">To remove the expansion schema:<codeblock>gpexpand -c</codeblock></li>
    </ol>
    <p>For information about the <codeph>gpexpand</codeph> utility and the other utilities that are
      used for system expansion, see the <i>Greenplum Database Utility Guide</i>. </p>
  </body>
</topic>
