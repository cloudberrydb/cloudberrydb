<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="using_plcontainer">
    <title>Using PL/Container</title>
    <body>
        <p>This topic covers further details on: </p>
        <p>
            <ul id="ul_ozt_t1j_blb">
                <li><xref href="#topic_resmgmt">PL/Container Resource Management</xref></li>
                <li><xref href="#topic_rh3_p3q_dw">PL/Container Functions</xref></li>
            </ul>
        </p>
    </body>
    <topic id="topic_resmgmt">
        <title>PL/Container Resource Management</title>
        <body>
            <section id="intro_resmgmt">
                <p>The Docker containers and the Greenplum Database servers share CPU and memory
                    resources on the same hosts. In the default case, Greenplum Database is unaware
                    of the resources consumed by running PL/Container instances. You can use
                    Greenplum Database resource groups to control overall CPU and memory resource
                    usage for running PL/Container instances.</p>
                <p>PL/Container manages resource usage at two levels - the container level and the
                    runtime level. You can control container-level CPU and memory resources with the
                        <codeph>memory_mb</codeph> and <codeph>cpu_share</codeph> settings that you
                    configure for the PL/Container runtime. <codeph>memory_mb</codeph> governs the
                    memory resources available to each container instance. The
                        <codeph>cpu_share</codeph> setting identifies the relative weighting of a
                    container's CPU usage compared to other containers. See <xref
                        href="../utility_guide/ref/plcontainer-configuration.xml" format="dita"/> for further
                    details.</p>
                <p>You cannot, by default, restrict the number of executing PL/Container container
                    instances, nor can you restrict the total amount of memory or CPU resources that
                    they consume.</p>
            </section>
            <section id="topic_resgroup">
                <title>Using Resource Groups to Manage PL/Container Resources</title>
                <p>With PL/Container 1.2.0 and later, you can use Greenplum Database resource groups
                    to manage and limit the total CPU and memory resources of containers in
                    PL/Container runtimes. For more information about enabling, configuring, and
                    using Greenplum Database resource groups, refer to <xref
                        href="../admin_guide/workload_mgmt_resgroups.xml" format="dita" scope="peer"
                        >Using Resource Groups</xref> in the <cite>Greenplum Database Administrator
                        Guide</cite>.</p>
                <note>If you do not explicitly configure resource groups for a PL/Container runtime,
                    its container instances are limited only by system resources. The containers may
                    consume resources at the expense of the Greenplum Database server.</note>
                <p>Resource groups for external components such as PL/Container use Linux control
                    groups (cgroups) to manage component-level use of memory and CPU resources. When
                    you manage PL/Container resources with resource groups, you configure both a
                    memory limit and a CPU limit that Greenplum Database applies to all container
                    instances that share the same PL/Container runtime configuration.</p>
                <p>When you create a resource group to manage the resources of a PL/Container
                    runtime, you must specify <codeph>MEMORY_AUDITOR=cgroup</codeph> and
                        <codeph>CONCURRENCY=0</codeph> in addition to the required CPU and memory
                    limits. For example, the following command creates a resource group named
                        <codeph>plpy_run1_rg</codeph> for a PL/Container runtime:
                    <codeblock>CREATE RESOURCE GROUP plpy_run1_rg WITH (MEMORY_AUDITOR=cgroup, CONCURRENCY=0,
                                                  CPU_RATE_LIMIT=10, MEMORY_LIMIT=10);</codeblock></p>
                <p>PL/Container does not use the <codeph>MEMORY_SHARED_QUOTA</codeph> and
                        <codeph>MEMORY_SPILL_RATIO</codeph> resource group memory limits. Refer to
                    the <codeph><xref href="../ref_guide/sql_commands/CREATE_RESOURCE_GROUP.xml"
                            format="dita" scope="peer">CREATE RESOURCE GROUP</xref></codeph>
                    reference page for detailed information about this SQL command.</p>
                <p>You can create one or more resource groups to manage your running PL/Container
                    instances. After you create a resource group for PL/Container, you assign the
                    resource group to one or more PL/Container runtimes. You make this assignment
                    using the <codeph>groupid</codeph> of the resource group. You can determine the
                        <codeph>groupid</codeph> for a given resource group name from the
                        <codeph>gp_resgroup_config</codeph>
                    <codeph>gp_toolkit</codeph> view. For example, the following query displays the
                        <codeph>groupid</codeph> of a resource group named
                        <codeph>plpy_run1_rg</codeph>:<codeblock>SELECT groupname, groupid FROM gp_toolkit.gp_resgroup_config
 WHERE groupname='plpy_run1_rg';
                            
 groupname   |  groupid
 --------------+----------
 plpy_run1_rg |   16391
 (1 row)</codeblock></p>
                <p>You assign a resource group to a PL/Container runtime configuration by specifying
                    the <codeph>-s resource_group_id=<varname>rg_groupid</varname></codeph> option
                    to the <codeph>plcontainer runtime-add</codeph> (new runtime) or
                        <codeph>plcontainer runtime-replace</codeph> (existing runtime) commands.
                    For example, to assign the <codeph>plpy_run1_rg</codeph> resource group to a new
                    PL/Container runtime named <codeph>python_run1</codeph>:
                    <codeblock>plcontainer runtime-add -r python_run1 -i pivotaldata/plcontainer_python_shared:devel -l python -s resource_group_id=16391</codeblock></p>
                <p>You can also assign a resource group to a PL/Container runtime using the
                        <codeph>plcontainer runtime-edit</codeph> command. For information about the
                        <codeph>plcontainer</codeph> command, see <xref href="../utility_guide/ref/plcontainer.xml"
                        format="dita"/> reference page.</p>
                <p>After you assign a resource group to a PL/Container runtime, all container
                    instances that share the same runtime configuration are subject to the memory
                    limit and the CPU limit that you configured for the group. If you decrease the
                    memory limit of a PL/Container resource group, queries executing in running
                    containers in the group may fail with an out of memory error. If you drop a
                    PL/Container resource group while there are running container instances,
                    Greenplum Database kills the running containers.</p>
            </section>
            <section id="topic_resgroupcfg">
                <title>Configuring Resource Groups for PL/Container</title>
                <p>To use Greenplum Database resource groups to manage PL/Container resources, you
                    must explicitly configure both resource groups and PL/Container.</p>
            
            <p>Perform the following procedure to configure PL/Container to use Greenplum Database
                resource groups for CPU and memory resource management:</p>
            <ol id="ol_qdq_vmh_blb">
                <li>If you have not already configured and enabled resource groups in your Greenplum
                    Database deployment, configure cgroups and enable Greenplum Database resource
                    groups as described in <xref
                        href="../admin_guide/workload_mgmt_resgroups.xml#topic71717999"
                        format="dita" scope="peer">Using Resource Groups</xref> in the
                        <cite>Greenplum Database Administrator Guide</cite>. <note>If you have
                        previously configured and enabled resource groups in your deployment, ensure
                        that the Greenplum Database resource group <codeph>gpdb.conf</codeph>
                        cgroups configuration file includes a <codeph>memory { }</codeph> block as
                        described in the previous link.</note></li>
                <li>Analyze the resource usage of your Greenplum Database deployment. Determine the
                    percentage of resource group CPU and memory resources that you want to allocate
                    to PL/Container Docker containers.</li>
                <li>Determine how you want to distribute the total PL/Container CPU and memory
                    resources that you identified in the step above among the PL/Container runtimes.
                        Identify:<ul id="ul_rdq_vmh_blb">
                        <li>The number of PL/Container resource group(s) that you require.</li>
                        <li>The percentage of memory and CPU resources to allocate to each resource
                            group.</li>
                        <li>The resource-group-to-PL/Container-runtime assignment(s).</li>
                    </ul></li>
                <li>Create the PL/Container resource groups that you identified in the step above.
                        For example, suppose that you choose to allocate 25% of both memory and CPU
                        Greenplum Database resources to PL/Container. If you further split these
                        resources among 2 resource groups 60/40, the following SQL commands create
                        the resource
                        groups:<codeblock>CREATE RESOURCE GROUP plr_run1_rg WITH (MEMORY_AUDITOR=cgroup, CONCURRENCY=0,
                                           CPU_RATE_LIMIT=15, MEMORY_LIMIT=15);
 CREATE RESOURCE GROUP plpy_run1_rg WITH (MEMORY_AUDITOR=cgroup, CONCURRENCY=0,
                                          CPU_RATE_LIMIT=10, MEMORY_LIMIT=10);</codeblock></li>
                <li>Find and note the <codeph>groupid</codeph> associated with each resource group
                        that you created. For
                        example:<codeblock>SELECT groupname, groupid FROM gp_toolkit.gp_resgroup_config
WHERE groupname IN ('plpy_run1_rg', 'plr_run1_rg');
                                    
groupname   |  groupid
--------------+----------
plpy_run1_rg |   16391
plr_run1_rg  |   16393
(1 row)</codeblock></li>
                <li>Assign each resource group that you created to the desired PL/Container runtime
                        configuration. If you have not yet created the runtime configuration, use
                        the <codeph>plcontainer runtime-add</codeph> command. If the runtime already
                        exists, use the <codeph>plcontainer runtime-replace</codeph> or
                            <codeph>plcontainer runtime-edit</codeph> command to add the resource
                        group assignment to the runtime configuration. For example:
                            <codeblock>plcontainer runtime-add -r python_run1 -i pivotaldata/plcontainer_python_shared:devel -l python -s resource_group_id=16391
plcontainer runtime-replace -r r_run1 -i pivotaldata/plcontainer_r_shared:devel -l r -s resource_group_id=16393</codeblock><p>For
                            information about the <codeph>plcontainer</codeph> command, see <xref
                                href="../utility_guide/ref/plcontainer.xml" format="dita"/>
                            reference page.</p></li>
            </ol></section>
            
            <section id="plc_notes">
                <title>Notes</title>
                <p id="pl_logging_notes"><b>PL/Container logging</b></p>
                <p>When PL/Container logging is enabled, you can set the log level with the
                    Greenplum Database server configuration parameter <codeph><xref
                            href="../ref_guide/config_params/guc-list.xml#log_min_messages"
                            scope="peer">log_min_messages</xref></codeph>. The default log level is
                        <codeph>warning</codeph>. The parameter controls the PL/Container log level
                    and also controls the Greenplum Database log level.</p>
                <ul id="ul_knd_jhl_mcb">
                    <li>PL/Container logging is enabled or disabled for each runtime ID with the
                            <codeph>setting</codeph> attribute
                            <codeph>use_container_logging</codeph>. The default is no logging. </li>
                    <li>The PL/Container log information is the information from the UDF that is run
                        in the Docker container. By default, the PL/Container log information is
                        sent to a system service. On Red Hat 7 or CentOS 7 systems, the log
                        information is sent to the <codeph>journald</codeph> service. </li>
                    <li>The Greenplum Database log information is sent to log file on the Greenplum
                        Database master.</li>
                    <li>When testing or troubleshooting a PL/Container UDF, you can change the
                        Greenplum Database log level with the <codeph>SET</codeph> command. You can
                        set the parameter in the session before you run your PL/Container UDF. This
                        example sets the log level to <codeph>debug1</codeph>. <codeblock>SET log_min_messages='debug1' ;</codeblock>
                        <note>The parameter <codeph>log_min_messages</codeph> controls both the
                            Greenplum Database and PL/Container logging, increasing the log level
                            might affect Greenplum Database performance even if a PL/Container UDF
                            is not running.</note></li>
                </ul>
            </section>
        </body>
    </topic>
        
    <topic id="topic_rh3_p3q_dw">
        <title>PL/Container Functions</title>
        <body>
            <p>When you enable PL/Container in a database of a Greenplum Database system, the
                language <codeph>plcontainer</codeph> is registered in that database. Specify
                    <codeph>plcontainer</codeph> as a language in a UDF definition to create and run
                user-defined functions in the procedural languages supported by the PL/Container
                Docker images.</p>
            
            <section id="topic_c3v_clg_wkb">
                <title>Limitations</title>
                <p>Review the following limitations when creating and using PL/Container PL/Python
                    and PL/R functions: </p>
                <ul id="ul_d3v_clg_wkb">
                    <li>Greenplum Database domains are not supported.</li>
                    <li>Multi-dimensional arrays are not supported.</li>
                    <li>Python and R call stack information is not displayed when debugging a
                        UDF.</li>
                    <li>The <codeph>plpy.execute()</codeph> methods <codeph>nrows()</codeph> and
                            <codeph>status()</codeph> are not supported.</li>
                    <li>The PL/Python function <codeph>plpy.SPIError()</codeph> is not
                        supported.</li>
                    <li>Executing the <codeph>SAVEPOINT</codeph> command with
                        <codeph>plpy.execute()</codeph> is not supported.</li>
                    <li>The <codeph>DO</codeph> command (anonymous code block) is supported only
                        with PL/Container 3 (currently a Beta feature). </li>
                    <li>Container flow control is not supported.</li>
                    <li>Triggers are not supported.</li>
                    <li><codeph>OUT</codeph> parameters are not supported.</li>
                    <li>The Python <codeph>dict</codeph> type cannot be returned from
                      a PL/Python UDF.  When returning the Python <codeph>dict</codeph>
                      type from a UDF, you can convert the <codeph>dict</codeph> type
                      to a Greenplum Database user-defined data type (UDT).</li>
                </ul>
            </section>
            
            <section id="using_functions">
                <title>Using PL/Container functions</title>
                <p>A UDF definition that uses PL/Container must have the these items.</p>
                <ul id="ul_z2m_1kj_kw">
                    <li>The first line of the UDF must be <codeph># container:
                            <varname>ID</varname></codeph></li>
                    <li>The <codeph>LANGUAGE</codeph> attribute must be
                        <codeph>plcontainer</codeph></li>
                </ul>
                <p>The <varname>ID</varname> is the name that PL/Container uses to identify a Docker
                    image. When Greenplum Database executes a UDF on a host, the Docker image on the
                    host is used to start a Docker container that runs the UDF. In the XML
                    configuration file <codeph>plcontainer_configuration.xml</codeph>, there is a
                        <codeph>runtime</codeph> XML element that contains a corresponding
                        <codeph>id</codeph> XML element that specifies the Docker container startup
                    information. See <xref
                        href="../utility_guide/ref/plcontainer-configuration.xml#topic_sk1_gdq_dw"
                        format="dita"/> for information about how PL/Container maps the
                        <varname>ID</varname> to a Docker image.</p>
                <p>The PL/Container configuration file is read only on the first invocation of a
                    PL/Container function in each Greenplum Database session that runs PL/Container
                    functions. You can force the configuration file to be re-read by performing a
                        <codeph>SELECT</codeph> command on the view
                        <codeph>plcontainer_refresh_config</codeph> during the session. For example,
                    this <codeph>SELECT</codeph> command forces the configuration file to be
                    read.</p>
                <codeblock>SELECT * FROM plcontainer_refresh_config;</codeblock>
                <p>Running the command executes a PL/Container function that updates the
                    configuration on the master and segment instances and returns the status of the
                    refresh.<codeblock> gp_segment_id | plcontainer_refresh_local_config
 ---------------+----------------------------------
 1 | ok
 0 | ok
-1 | ok
(3 rows)</codeblock></p>
                <p>Also, you can show all the configurations in the session by performing a
                        <codeph>SELECT</codeph> command on the view
                        <codeph>plcontainer_show_config</codeph>. For example, this
                        <codeph>SELECT</codeph> command returns the PL/Container configurations. </p>
                <codeblock>SELECT * FROM plcontainer_show_config;</codeblock>
                <p>Running the command executes a PL/Container function that displays configuration
                    information from the master and segment instances. This is an example of the
                    start and end of the view
                    output.<codeblock>INFO:  plcontainer: Container 'plc_py_test' configuration
 INFO:  plcontainer:     image = 'pivotaldata/plcontainer_python_shared:devel'
 INFO:  plcontainer:     memory_mb = '1024'
 INFO:  plcontainer:     use container network = 'no'
 INFO:  plcontainer:     use container logging  = 'no'
 INFO:  plcontainer:     shared directory from host '/usr/local/greenplum-db/./bin/plcontainer_clients' to container '/clientdir'
 INFO:  plcontainer:     access = readonly
                
 ...
                
 INFO:  plcontainer: Container 'plc_r_example' configuration  (seg0 slice3 192.168.180.45:40000 pid=3304)
 INFO:  plcontainer:     image = 'pivotaldata/plcontainer_r_without_clients:0.2'  (seg0 slice3 192.168.180.45:40000 pid=3304)
 INFO:  plcontainer:     memory_mb = '1024'  (seg0 slice3 192.168.180.45:40000 pid=3304)
 INFO:  plcontainer:     use container network = 'no'  (seg0 slice3 192.168.180.45:40000 pid=3304)
 INFO:  plcontainer:     use container logging  = 'yes'  (seg0 slice3 192.168.180.45:40000 pid=3304)
 INFO:  plcontainer:     shared directory from host '/usr/local/greenplum-db/bin/plcontainer_clients' to container '/clientdir'  (seg0 slice3 192.168.180.45:40000 pid=3304)
 INFO:  plcontainer:         access = readonly  (seg0 slice3 192.168.180.45:40000 pid=3304)
 gp_segment_id | plcontainer_show_local_config
 ---------------+-------------------------------
  0 | ok
 -1 | ok
  1 | ok</codeblock></p>
                <p>The PL/Container function <codeph>plcontainer_containers_summary()</codeph>
                    displays information about the currently running Docker
                    containers.<codeblock>SELECT * FROM plcontainer_containers_summary();</codeblock></p>
                <p>If a normal (non-superuser) Greenplum Database user runs the function, the
                    function displays information only for containers created by the user. If a
                    Greenplum Database superuser runs the function, information for all containers
                    created by Greenplum Database users is displayed. This is sample output when 2
                    containers are running.</p>
                <codeblock> SEGMENT_ID |                           CONTAINER_ID                           |   UP_TIME    |  OWNER  | MEMORY_USAGE(KB)
 ------------+------------------------------------------------------------------+--------------+---------+------------------
 1          | 693a6cb691f1d2881ec0160a44dae2547a0d5b799875d4ec106c09c97da422ea | Up 8 seconds | gpadmin | 12940
 1          | bc9a0c04019c266f6d8269ffe35769d118bfb96ec634549b2b1bd2401ea20158 | Up 2 minutes | gpadmin | 13628
 (2 rows)</codeblock>
                <p>When Greenplum Database executes a PL/Container UDF, Query Executer (QE)
                    processes start Docker containers and reuse them as needed. After a certain
                    amount of idle time, a QE process quits and destroys its Docker containers. You
                    can control the amount of idle time with the Greenplum Database server
                    configuration parameter <codeph><xref
                            href="../ref_guide/config_params/guc-list.xml#gp_vmem_idle_resource_timeout"
                            scope="peer">gp_vmem_idle_resource_timeout</xref></codeph>. Controlling
                    the idle time might help with Docker container reuse and avoid the overhead of
                    creating and starting a Docker container. </p>
                <note type="warning">Changing <codeph>gp_vmem_idle_resource_timeout</codeph> value,
                    might affect performance due to resource issues. The parameter also controls the
                    freeing of Greenplum Database resources other than Docker containers.</note>
            </section>
        
        <section xml:lang="en" id="function_examples">
                <title>Examples</title>
                <p>The values in the <codeph># container</codeph> lines of the examples,
                        <codeph>plc_python_shared</codeph> and <codeph>plc_r_shared</codeph>, are
                    the <codeph>id</codeph> XML elements defined in the
                        <codeph>plcontainer_config.xml</codeph> file. The <codeph>id</codeph>
                    element is mapped to the <codeph>image</codeph> element that specifies the
                    Docker image to be started. If you configured PL/Container with a different ID,
                    change the value of the <codeph># container</codeph> line. For information about
                    configuring PL/Container and viewing the configuration settings, see <xref
                        href="../utility_guide/ref/plcontainer-configuration.xml" format="dita"/>.</p>
                <p>This is an example of PL/Python function that runs using the
                        <codeph>plc_python_shared</codeph> container that contains Python
                    2:<codeblock>CREATE OR REPLACE FUNCTION pylog100() RETURNS double precision AS $$
 # container: plc_python_shared
 import math
 return math.log10(100)
 $$ LANGUAGE plcontainer;</codeblock></p>
                <p>This is an example of a similar function using the <codeph>plc_r_shared</codeph>
                    container:<codeblock>CREATE OR REPLACE FUNCTION rlog100() RETURNS text AS $$
# container: plc_r_shared
return(log10(100))
$$ LANGUAGE plcontainer;</codeblock></p>
                <p>If the <codeph># container</codeph> line in a UDF specifies an ID that is not in
                    the PL/Container configuration file, Greenplum Database returns an error when
                    you try to execute the UDF.</p>
            </section>
        
    
        <section id="topic_ctk_xjg_wkb">
            <title>About PL/Container Running PL/Python </title>
            
                <p>In the Python language container, the module <codeph>plpy</codeph> is
                    implemented. The module contains these methods:</p>
                <ul id="ul_dtk_xjg_wkb">
                    <li><codeph>plpy.execute(stmt)</codeph> - Executes the query string
                            <codeph>stmt</codeph> and returns query result in a list of dictionary
                        objects. To be able to access the result fields ensure your query returns
                        named fields.</li>
                    <li><codeph>plpy.prepare(stmt[, argtypes])</codeph> - Prepares the execution
                        plan for a query. It is called with a query string and a list of parameter
                        types, if you have parameter references in the query.</li>
                    <li><codeph>plpy.execute(plan[, argtypes])</codeph> - Executes a prepared
                        plan.</li>
                    <li><codeph>plpy.debug(msg)</codeph> - Sends a DEBUG2 message to the Greenplum
                        Database log.</li>
                    <li><codeph>plpy.log(msg)</codeph> - Sends a LOG message to the Greenplum
                        Database log.</li>
                    <li><codeph>plpy.info(msg)</codeph> - Sends an INFO message to the Greenplum
                        Database log.</li>
                    <li><codeph>plpy.notice(msg)</codeph> - Sends a NOTICE message to the Greenplum
                        Database log.</li>
                    <li><codeph>plpy.warning(msg)</codeph> - Sends a WARNING message to the
                        Greenplum Database log.</li>
                    <li><codeph>plpy.error(msg)</codeph> - Sends an ERROR message to the Greenplum
                        Database log. An ERROR message raised in Greenplum Database causes the query
                        execution process to stop and the transaction to rollback.</li>
                    <li><codeph>plpy.fatal(msg)</codeph> - Sends a FATAL message to the Greenplum
                        Database log. A FATAL message causes Greenplum Database session to be closed
                        and transaction to be rolled back.</li>
                    <li><codeph>plpy.subtransaction()</codeph> - Manages
                            <codeph>plpy.execute</codeph> calls in an explicit subtransaction. See
                            <xref
                            href="https://www.postgresql.org/docs/9.4/plpython-subtransaction.html"
                            format="html" scope="external">Explicit Subtransactions</xref> in the
                        PostgreSQL documentation for additional information about
                            <codeph>plpy.subtransaction()</codeph>.</li>
                </ul>
                <p>If an error of level <codeph>ERROR</codeph> or <codeph>FATAL</codeph> is raised
                    in a nested Python function call, the message includes the list of enclosing
                    functions.</p>
                <p>The Python language container supports these string quoting functions that are
                    useful when constructing ad-hoc queries. <ul id="ul_etk_xjg_wkb">
                        <li><codeph>plpy.quote_literal(string)</codeph> - Returns the string quoted
                            to be used as a string literal in an SQL statement string. Embedded
                            single-quotes and backslashes are properly doubled.
                                <codeph>quote_literal()</codeph> returns null on null input (empty
                            input). If the argument might be null, <codeph>quote_nullable()</codeph>
                            might be more appropriate.</li>
                        <li><codeph>plpy.quote_nullable(string)</codeph> - Returns the string quoted
                            to be used as a string literal in an SQL statement string. If the
                            argument is null, returns <codeph>NULL</codeph>. Embedded single-quotes
                            and backslashes are properly doubled.</li>
                        <li>
                            <codeph>plpy.quote_ident(string)</codeph> - Returns the string quoted to
                            be used as an identifier in an SQL statement string. Quotes are added
                            only if necessary (for example, if the string contains non-identifier
                            characters or would be case-folded). Embedded quotes are properly
                            doubled. </li>
                    </ul></p>
                <p>When returning text from a PL/Python function, PL/Container converts a Python
                    unicode object to text in the database encoding. If the conversion cannot be
                    performed, an error is returned.</p>
                <p>PL/Container does not support this Greenplum Database PL/Python feature:<ul
                        id="ul_ftk_xjg_wkb">
                        <li> Multi-dimensional arrays.</li>
                    </ul></p>
                <p>Also, the Python module has two global dictionary objects that retain the data
                    between function calls. They are named GD and SD. GD is used to share the data
                    between all the function running within the same container, while SD is used for
                    sharing the data between multiple calls of each separate function. Be aware that
                    accessing the data is possible only within the same session, when the container
                    process lives on a segment or master. Be aware that for idle sessions Greenplum
                    Database terminates segment processes, which means the related containers would
                    be shut down and the data from GD and SD lost.</p>
                <p>For information about PL/Python, see <xref href="pl_python.xml#topic1"/>. </p>
                <p>For information about the <codeph>plpy</codeph> methods, see <xref
                        href="https://www.postgresql.org/docs/9.4/plpython-database.html"
                        format="html" scope="external"
                        >https://www.postgresql.org/docs/9.4/plpython-database.htm</xref>. </p>
            
        </section>
        <section id="topic_plc_py3">
          <title>About PL/Container Running PL/Python with Python 3 </title>
          <p>PL/Container for Greenplum Database 5 supports Python version 3.6+.
            PL/Container for Greenplum Database 6 supports Python 3.7+.</p>
          <p>If you want to use PL/Container to run the same function body in
            both Python2 and Python3, you must create 2 different user-defined
            functions.</p>
          <p>Keep in mind that UDFs that you created for Python 2 may not run in
            PL/Container with Python 3. The following Python references may be
            useful:</p>
          <ul>
            <li>Changes to Python - <xref href="https://docs.python.org/3/whatsnew/3.0.html"
              format="html" scope="external">Whatâ€™s New in Python 3</xref></li>
            <li>Porting from Python 2 to 3 - <xref href="https://docs.python.org/3/howto/pyporting.html"
              format="html" scope="external">Porting Python 2 Code to Python 3</xref></li>
            </ul>
        </section>
            
        <section id="topic_lqz_t3q_dw">
            <title>About PL/Container Running PL/R</title>
            
                <p>In the R language container, the module <codeph>pg.spi</codeph> is implemented.
                    The module contains these methods:</p>
                <ul id="ul_mqz_t3q_dw">
                    <li><codeph>pg.spi.exec(stmt)</codeph> - Executes the query string
                            <codeph>stmt</codeph> and returns query result in R
                            <codeph>data.frame</codeph>. To be able to access the result fields make
                        sure your query returns named fields.</li>
                    <li><codeph>pg.spi.prepare(stmt[, argtypes])</codeph> - Prepares the execution
                        plan for a query. It is called with a query string and a list of parameter
                        types if you have parameter references in the query.</li>
                    <li><codeph>pg.spi.execp(plan[, argtypes])</codeph> - Execute a prepared
                        plan.</li>
                    <li><codeph>pg.spi.debug(msg)</codeph> - Sends a DEBUG2 message to the Greenplum
                        Database log.</li>
                    <li><codeph>pg.spi.log(msg)</codeph> - Sends a LOG message to the Greenplum
                        Database log.</li>
                    <li><codeph>pg.spi.info(msg)</codeph> - Sends an INFO message to the Greenplum
                        Database log.</li>
                    <li><codeph>pg.spi.notice(msg)</codeph> - Sends a NOTICE message to the
                        Greenplum Database log.</li>
                    <li><codeph>pg.spi.warning(msg)</codeph> - Sends a WARNING message to the
                        Greenplum Database log.</li>
                    <li><codeph>pg.spi.error(msg)</codeph> - Sends an ERROR message to the Greenplum
                        Database log. An ERROR message raised in Greenplum Database causes the query
                        execution process to stop and the transaction to rollback.</li>
                    <li><codeph>pg.spi.fatal(msg)</codeph> - Sends a FATAL message to the Greenplum
                        Database log. A FATAL message causes Greenplum Database session to be closed
                        and transaction to be rolled back.</li>
                </ul>
                <p>PL/Container does not support this PL/R feature:<ul id="ul_wjk_dgb_4cb">
                        <li> Multi-dimensional arrays.</li>
                    </ul></p>
                <p>For information about PL/R, see <xref href="pl_r.xml#topic1"/>.</p>
                <p>For information about the <codeph>pg.spi</codeph> methods, see <xref
                        href="http://www.joeconway.com/plr/doc/plr-spi-rsupport-funcs-normal.html"
                        format="html" scope="external"
                        >http://www.joeconway.com/plr/doc/plr-spi-rsupport-funcs-normal.html</xref></p>
            
        </section>
        </body>
    </topic>
</topic>
