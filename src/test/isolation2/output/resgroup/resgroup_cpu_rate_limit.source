-- start_ignore
DROP VIEW IF EXISTS cpu_status;
DROP
DROP VIEW IF EXISTS busy;
DROP
DROP VIEW IF EXISTS cancel_all;
DROP
DROP FUNCTION IF EXISTS round_percentage(text);
DROP
DROP TABLE IF EXISTS bigtable;
DROP
DROP ROLE IF EXISTS role1_cpu_test;
DROP
DROP ROLE IF EXISTS role2_cpu_test;
DROP
DROP RESOURCE GROUP rg1_cpu_test;
ERROR:  resource group "rg1_cpu_test" does not exist
DROP RESOURCE GROUP rg2_cpu_test;
ERROR:  resource group "rg2_cpu_test" does not exist
-- end_ignore

--
-- helper functions, tables and views
--

CREATE TABLE bigtable AS SELECT i AS c1, 'abc' AS c2 FROM generate_series(1,50000) i;
CREATE 50000

-- the cpu usage limitation has an error rate about +-7.5%,
-- and also we want to satisfy the 0.1:0.2 rate under 90% overall limitation
-- so we round the cpu rate by 15%
CREATE OR REPLACE FUNCTION round_percentage(float) RETURNS text AS $$ SELECT (round($1/15)*15)::text||'%' $$ LANGUAGE sql;
CREATE

CREATE OR REPLACE VIEW cpu_status AS SELECT b.rsgname, round_percentage(avg(b.cpu)) AS cpu FROM gp_segment_configuration gsc, ( SELECT btrim((cpu->'key')::text, '"')::int AS content, (cpu->'value') ::text::float AS cpu, rsgname FROM ( SELECT rsgname, row_to_json(json_each(cpu_usage::json)) AS cpu FROM gp_toolkit.gp_resgroup_status ) a ) b WHERE gsc.content = b.content AND gsc.role = 'p' AND gsc.content <> -1 GROUP BY b.rsgname ORDER BY b.rsgname;
CREATE

CREATE VIEW busy AS SELECT count(*) FROM bigtable t1, bigtable t2, bigtable t3, bigtable t4, bigtable t5 WHERE 0 = (t1.c1 % 2 + 10000)! AND 0 = (t2.c1 % 2 + 10000)! AND 0 = (t3.c1 % 2 + 10000)! AND 0 = (t4.c1 % 2 + 10000)! AND 0 = (t5.c1 % 2 + 10000)! ;
CREATE

CREATE VIEW cancel_all AS SELECT pg_cancel_backend(procpid) FROM pg_stat_activity WHERE current_query LIKE 'SELECT * FROM busy%';
CREATE

--
-- check gpdb cgroup configuration
--
-- cfs_quota_us := cfs_period_us * ncores * gp_resource_group_cpu_limit
-- shares := 1024 * gp_resource_group_cpu_priority
--

! python -c "print $(cat @cgroup_mnt_point@/cpu/gpdb/cpu.cfs_quota_us) == int($(cat @cgroup_mnt_point@/cpu/gpdb/cpu.cfs_period_us) * $(nproc) * $(psql -d isolation2resgrouptest -Aqtc "SHOW gp_resource_group_cpu_limit"))";
True


! python -c "print $(cat @cgroup_mnt_point@/cpu/gpdb/cpu.shares) == 1024 * $(psql -d isolation2resgrouptest -Aqtc "SHOW gp_resource_group_cpu_priority")";
True


--
-- check default groups configuration
--
-- SUB/shares := TOP/shares * cpu_rate_limit
--

! python -c "print $(cat @cgroup_mnt_point@/cpu/gpdb/$(psql -d isolation2resgrouptest -Aqtc "SELECT oid FROM pg_resgroup WHERE rsgname='default_group'")/cpu.shares) == int($(cat @cgroup_mnt_point@/cpu/gpdb/cpu.shares) * $(psql -d isolation2resgrouptest -Aqtc "SELECT value FROM pg_resgroupcapability c, pg_resgroup g WHERE c.resgroupid=g.oid AND reslimittype=2 AND g.rsgname='default_group'") / 100)";
True


! python -c "print $(cat @cgroup_mnt_point@/cpu/gpdb/$(psql -d isolation2resgrouptest -Aqtc "SELECT oid FROM pg_resgroup WHERE rsgname='admin_group'")/cpu.shares) == int($(cat @cgroup_mnt_point@/cpu/gpdb/cpu.shares) * $(psql -d isolation2resgrouptest -Aqtc "SELECT value FROM pg_resgroupcapability c, pg_resgroup g WHERE c.resgroupid=g.oid AND reslimittype=2 AND g.rsgname='admin_group'") / 100)";
True


-- create two resource groups
CREATE RESOURCE GROUP rg1_cpu_test WITH (concurrency=5, cpu_rate_limit=10, memory_limit=20);
CREATE
CREATE RESOURCE GROUP rg2_cpu_test WITH (concurrency=5, cpu_rate_limit=20, memory_limit=20);
CREATE

-- check rg1_cpu_test configuration
! python -c "print $(cat @cgroup_mnt_point@/cpu/gpdb/$(psql -d isolation2resgrouptest -Aqtc "SELECT oid FROM pg_resgroup WHERE rsgname='rg1_cpu_test'")/cpu.shares) == int($(cat @cgroup_mnt_point@/cpu/gpdb/cpu.shares) * 0.1)";
True


-- check rg2_cpu_test configuration
! python -c "print $(cat @cgroup_mnt_point@/cpu/gpdb/$(psql -d isolation2resgrouptest -Aqtc "SELECT oid FROM pg_resgroup WHERE rsgname='rg2_cpu_test'")/cpu.shares) == int($(cat @cgroup_mnt_point@/cpu/gpdb/cpu.shares) * 0.2)";
True


-- create two roles and assign them to above groups
CREATE ROLE role1_cpu_test RESOURCE GROUP rg1_cpu_test;
CREATE
CREATE ROLE role2_cpu_test RESOURCE GROUP rg2_cpu_test;
CREATE
GRANT ALL ON busy TO role1_cpu_test;
GRANT
GRANT ALL ON busy TO role2_cpu_test;
GRANT

-- prepare parallel queries in the two groups
10: SET ROLE TO role1_cpu_test;
SET
11: SET ROLE TO role1_cpu_test;
SET
12: SET ROLE TO role1_cpu_test;
SET
13: SET ROLE TO role1_cpu_test;
SET
14: SET ROLE TO role1_cpu_test;
SET

20: SET ROLE TO role2_cpu_test;
SET
21: SET ROLE TO role2_cpu_test;
SET
22: SET ROLE TO role2_cpu_test;
SET
23: SET ROLE TO role2_cpu_test;
SET
24: SET ROLE TO role2_cpu_test;
SET

--
-- now we get prepared.
--
-- on empty load the cpu usage shall be 0%
--

SELECT * FROM cpu_status;
rsgname      |cpu
-------------+---
admin_group  |0% 
default_group|0% 
rg1_cpu_test |0% 
rg2_cpu_test |0% 
(4 rows)

--
-- a group should burst to use all the cpu usage
-- when it's the only one with running queries.
--
-- however the overall cpu usage is controlled by a GUC
-- gp_resource_group_cpu_limit which is 90% by default.
--
-- so the cpu usage shall be 90%
--

10&: SELECT * FROM busy;  <waiting ...>
11&: SELECT * FROM busy;  <waiting ...>
12&: SELECT * FROM busy;  <waiting ...>
13&: SELECT * FROM busy;  <waiting ...>
14&: SELECT * FROM busy;  <waiting ...>

SELECT pg_sleep(20);
pg_sleep
--------
        
(1 row)
SELECT * FROM cpu_status;
rsgname      |cpu
-------------+---
admin_group  |0% 
default_group|0% 
rg1_cpu_test |90%
rg2_cpu_test |0% 
(4 rows)

-- start_ignore
SELECT * FROM cancel_all;
pg_cancel_backend
-----------------
t                
t                
t                
t                
t                
(5 rows)

10<:  <... completed>
ERROR:  canceling statement due to user request
11<:  <... completed>
ERROR:  canceling statement due to user request
12<:  <... completed>
ERROR:  canceling statement due to user request
13<:  <... completed>
ERROR:  canceling statement due to user request
14<:  <... completed>
ERROR:  canceling statement due to user request
-- end_ignore

--
-- when there are multiple groups with parallel queries,
-- they should share the cpu usage by their cpu_usage settings,
--
-- rg1_cpu_test:rg2_cpu_test is 0.1:0.2 => 1:2, so:
--
-- - rg1_cpu_test gets 90% * 1/3 => 30%;
-- - rg2_cpu_test gets 90% * 2/3 => 60%;
--

10&: SELECT * FROM busy;  <waiting ...>
11&: SELECT * FROM busy;  <waiting ...>
12&: SELECT * FROM busy;  <waiting ...>
13&: SELECT * FROM busy;  <waiting ...>
14&: SELECT * FROM busy;  <waiting ...>

20&: SELECT * FROM busy;  <waiting ...>
21&: SELECT * FROM busy;  <waiting ...>
22&: SELECT * FROM busy;  <waiting ...>
23&: SELECT * FROM busy;  <waiting ...>
24&: SELECT * FROM busy;  <waiting ...>

SELECT pg_sleep(20);
pg_sleep
--------
        
(1 row)
SELECT * FROM cpu_status;
rsgname      |cpu
-------------+---
admin_group  |0% 
default_group|0% 
rg1_cpu_test |30%
rg2_cpu_test |60%
(4 rows)

-- start_ignore
SELECT * FROM cancel_all;
pg_cancel_backend
-----------------
t                
t                
t                
t                
t                
t                
t                
t                
t                
t                
(10 rows)

10<:  <... completed>
ERROR:  canceling statement due to user request
11<:  <... completed>
ERROR:  canceling statement due to user request
12<:  <... completed>
ERROR:  canceling statement due to user request
13<:  <... completed>
ERROR:  canceling statement due to user request
14<:  <... completed>
ERROR:  canceling statement due to user request

20<:  <... completed>
ERROR:  canceling statement due to user request
21<:  <... completed>
ERROR:  canceling statement due to user request
22<:  <... completed>
ERROR:  canceling statement due to user request
23<:  <... completed>
ERROR:  canceling statement due to user request
24<:  <... completed>
ERROR:  canceling statement due to user request
-- end_ignore

-- cleanup
REVOKE ALL ON busy FROM role1_cpu_test;
REVOKE
REVOKE ALL ON busy FROM role2_cpu_test;
REVOKE
DROP ROLE role1_cpu_test;
DROP
DROP ROLE role2_cpu_test;
DROP
DROP RESOURCE GROUP rg1_cpu_test;
DROP
DROP RESOURCE GROUP rg2_cpu_test;
DROP
